[
{
	"uri": "/openshift/appcenter/nexus3/",
	"title": "Nexus 3.x私服使用",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-05-14 创建 开源方案 初始版本    私服架构 私服是指私有服务器，是架设在局域网的一种特殊的远程仓库，目的是代理远程仓库及部署第三方构建。 有了私服之后，当 Maven 需要下载构件时，直接请求私服，私服上存在则下载到本地仓库； 否则，私服请求外部的远程仓库，将构件下载到私服，再提供给本地仓库下载。\nNexus是一个强大的Maven仓库管理器，它极大地简化了本地内部仓库的维护和外部仓库的访问。 如果使用了公共的Maven仓库服务器，可以从Maven中央仓库下载所需要的构件（Artifact），但这通常不是一个好的做法。 正常做法是在本地架设一个本地Maven仓库服务器，利用Nexus私服可以只在一个地方就能够完全控制访问和部署在你所维护仓库中的每个Artifact。\nNexus优点 为什么要构建Nexus私服？好处随便列几点：\n Nexus在代理远程仓库的同时维护本地仓库，以降低中央仓库的负荷,节省外网带宽和时间，Nexus私服就可以满足这样的需要。 Nexus是一套“开箱即用”的系统不需要数据库，它使用文件系统加Lucene来组织数据。 Nexus使用ExtJS来开发界面，利用Restlet来提供完整的REST APIs，并能通过插件和各种IDE集成。 Nexus支持WebDAV与LDAP安全身份认证。 Nexus还提供了强大的仓库管理功能，构件搜索功能，它基于REST，提供友好的UI，占用较少的内存，基于简单文件系统而非数据库。  Nexus安装 我们这里直接使用 Nexus 模板进行安装 模板地址参考 https://github.com/hunterfu/openshift-template/tree/master/devops/nexus3\nNexus使用设置 访问对应的router域名, Nexus默认的用户名密码是 admin/admin123\n首页效果如下：\n使用默认的管理员,进入管理界面：\n仓库 最核心的是仓库管理\n默认的个仓库说明：\n maven-central：maven中央库，默认从https://repo1.maven.org/maven2/拉取jar maven-releases：私库发行版jar，初次安装请将Deployment policy设置为Allow redeploy maven-snapshots：私库快照（调试版本）jar maven-public：仓库分组，把上面三个仓库组合在一起对外提供服务，在本地maven基础配置settings.xml中使用。  Nexus默认的仓库类型有以下四种：\n group(仓库组类型)：又叫组仓库，用于方便开发人员自己设定的仓库； hosted(宿主类型)：内部项目的发布仓库（内部开发人员，发布上去存放的仓库）； proxy(代理类型)：从远程中央仓库中寻找数据的仓库（可以点击对应的仓库的Configuration页签下Remote Storage属性的值即被代理的远程仓库的路径）； virtual(虚拟类型)：虚拟仓库（这个基本用不到，重点关注上面三个仓库的使用）；  Policy(策略): 表示该仓库为发布(Release)版本仓库还是快照(Snapshot)版本仓库；\n由于访问中央仓库有时候会比较慢，这里我们添加一个阿里云的代理仓库，然后优先级放到默认中央库之前\n阿里云的maven仓库url为http://maven.aliyun.com/nexus/content/groups/public\n然后再public组里面将这个aliyun-proxy仓库加入，排在maven-central之前即可。\nNexus仓库分类的概念  Maven可直接从宿主仓库下载构件,也可以从代理仓库下载构件,而代理仓库间接的从远程仓库下载并缓存构件\n 为了方便,Maven可以从仓库组下载构件,而仓库组并没有时间的内容(下图中用虚线表示,它会转向包含的宿主仓库或者代理仓库获得实际构件的内容)\n  Nexus的调度任务 默认安装好之后是没有索引和jar文件的，因为你要自己定义任务去执行。\nNexus提供了一系列可配置的调度任务来方便用户管理系统。用户可以设定这些任务运行的方式，例如每天、每周等。调度任务会在适当的时候在后台运行。\n要建立一个调度任务，单击左边导航菜单中的Tasks，点击Create Task，然后选择一个任务类型。\n以下几种常用类型的调度任务：\n Execute script：执行自定义脚本 Purge开头：清理一些不使用的资源。 Rebuild repository index：为仓库重新编纂索引，从远仓库下载最新的索引。 Rebuild Maven repository metadata：基于仓库内容重新创建仓库元数据文件，同时重新创建每个文件的校验和md5与sha1。 Remove snapshots from Maven repository：把快照删了，这个是在稳定版发布后清除  比如我新建一个重构索引的任务，然后选择aliyun仓库，让它把远程索引取下来，手动执行。不过最好别这样做，因为需要很大的硬盘空间。\n最好是让它自己去维护，请求一个依赖的时候如果私服没有会自动去远仓库取的。\nNexus搜索页 这个不需要登录就可以访问，用来查询jar包。支持模糊查询\nBlob Stores 文件存储的地方，创建一个目录的话，对应文件系统的一个目录，可供仓库上传文件使用，如图所示：\n本地Maven使用私服 安装和配置好之后，在开发中如何使用呢。可在maven的默认配置settings.xml中修改如下：\n\u0026lt;servers\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;releases\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt;admin\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;admin123\u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;snapshots\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt;admin\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;admin123\u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;/servers\u0026gt; \u0026lt;mirrors\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;nexus\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;*\u0026lt;/mirrorOf\u0026gt; \u0026lt;url\u0026gt;http://nexus.ops.com/repository/maven-public/\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;/mirrors\u0026gt;  如果要发布自己的jar到私服，就需要修改工程的pom.xml，添加如下内容，否则什么都不用做：\n\u0026lt;distributionManagement\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;releases\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;Releases\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://nexus.ops.com/repository/maven-releases/\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;snapshotRepository\u0026gt; \u0026lt;id\u0026gt;snapshots\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;Snapshot\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://nexus.ops.com/repository/maven-snapshots/\u0026lt;/url\u0026gt; \u0026lt;/snapshotRepository\u0026gt; \u0026lt;/distributionManagement\u0026gt;  注意上面的repository的id值一定要跟settings.xml文件中配置的server一致。\n上传到Nexus上，使用 mvn deploy 即可，开发的时候请使用snapshot版本，也就是version的后缀必须是-SNAPSHOT。\n第三方Jar上传到Nexus\nmvn deploy:deploy-file \\ -DgroupId=\u0026lt;group-id\u0026gt; \\ -DartifactId=\u0026lt;artifact-id\u0026gt; \\ -Dversion=\u0026lt;version\u0026gt; \\ -Dpackaging=\u0026lt;type-of-packaging\u0026gt; \\ -Dfile=\u0026lt;path-to-file\u0026gt; \\ -DrepositoryId=\u0026lt;server-id-settings.xml\u0026gt; \\ -Durl=\u0026lt;url-of-the-repository-to-deploy\u0026gt;  -DrepositoryId的值即为在setttings.xml里面配置的server id。\n"
},
{
	"uri": "/openshift/appcenter/custom_category/",
	"title": "定制 Catalog 分类",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-27 创建 开源方案 初始版本    类目分类说明 类目分类是用来组织模板和builder image的分组方式,我们可以按照自己需要的方式，对应用模板进行归类. 主要匹配的是 模板或者ImageStream的 tag 标签项\n定制类目方法 创建下面配置js脚本,比如 my-catalog-categories.js // Add Go to the Languages category var category = _.find(window.OPENSHIFT_CONSTANTS.CATALOG_CATEGORIES, { id: 'languages' }); category.items.splice(2,0,{ // Insert at the third spot // Required. Must be unique id: \u0026quot;go\u0026quot;, // Required label: \u0026quot;Go\u0026quot;, // Optional. If specified, defines a unique icon for this item iconClass: \u0026quot;font-icon icon-go-gopher\u0026quot;, // Optional. If specified, enables matching other tag values to this category // item categoryAliases: [ \u0026quot;golang\u0026quot; ] }); // Add a Featured category section at the top of the catalog window.OPENSHIFT_CONSTANTS.CATALOG_CATEGORIES.unshift({ // Required. Must be unique id: \u0026quot;opssolution\u0026quot;, // Required label: \u0026quot;开源方案之解决方案\u0026quot;, // Optional. If specified, each item in the category will utilize this icon // as a default iconClassDefault: \u0026quot;fa fa-code\u0026quot;, items: [ { // Required. Must be unique id: \u0026quot;go\u0026quot;, // Required label: \u0026quot;Go\u0026quot;, // Optional. If specified, defines a unique icon for this item iconClass: \u0026quot;font-icon icon-go-gopher\u0026quot;, // Optional. If specified, enables matching other tag values to this // category item categoryAliases: [ \u0026quot;golang\u0026quot; ], // Optional. If specified, will display below the item label description: \u0026quot;An open source programming language developed at Google in \u0026quot; + \u0026quot;2007 by Robert Griesemer, Rob Pike, and Ken Thompson.\u0026quot; }, { // Required. Must be unique id: \u0026quot;httpd\u0026quot;, // Required label: \u0026quot;HttpdServer\u0026quot;, // Optional. If specified, defines a unique icon for this item iconClass: \u0026quot;font-icon icon-apache\u0026quot;, // Optional. If specified, enables matching other tag values to this // category item categoryAliases: [ \u0026quot;httpd\u0026quot; ], // Optional. If specified, will display below the item label description: \u0026quot;开源web server\u0026quot; + \u0026quot;最老牌的http server\u0026quot; }, { // Required. Must be unique 开源方案的ID id: \u0026quot;51know\u0026quot;, // Required label: \u0026quot;开源方案之解决方案\u0026quot;, // Optional. If specified, defines a unique icon for this item iconClass: \u0026quot;font-icon icon-load-balancer\u0026quot;, // Optional. If specified, enables matching other tag values to this // category item categoryAliases: [ \u0026quot;httpd\u0026quot; ], // Optional. If specified, will display below the item label description: \u0026quot;由开源方案维护支持的解决方案\u0026quot; + \u0026quot;涉及到常见的应用解决方案\u0026quot; }, { // Required. Must be unique id: \u0026quot;jenkins\u0026quot;, // Required label: \u0026quot;Jenkins\u0026quot;, // Optional. If specified, defines a unique icon for this item iconClass: \u0026quot;font-icon icon-jenkins\u0026quot;, // Optional. If specified, will display below the item label description: \u0026quot;An open source continuous integration tool written in Java.\u0026quot; } ] });  保存文件,并修改 master 配置文件 /etc/origin/master/master-config.yaml  assetConfig: ... extensionScripts: - /path/to/my-catalog-categories.js  重启master服务 # systemctl restart origin-master  定制后，效果如下 参考资料\nhttps://docs.openshift.org/3.6/install_config/web_console_customization.html#configuring-catalog-categories\nhttps://docs.openshift.org/3.6/dev_guide/templates.html#writing-description\n"
},
{
	"uri": "/openshift/application/import_template/",
	"title": "导入示例模板",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-27 创建 开源方案 初始版本    导入模板和镜像(导入了部分模板) 本节的所涉及到的操作都是在openshift master节点上操作\n导入基础镜像(S2I RUNTIME) oc project openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/image-streams/image-streams-centos7.json -n openshift imagestream \u0026quot;httpd\u0026quot; created imagestream \u0026quot;ruby\u0026quot; created imagestream \u0026quot;nodejs\u0026quot; created imagestream \u0026quot;perl\u0026quot; created imagestream \u0026quot;php\u0026quot; created imagestream \u0026quot;python\u0026quot; created imagestream \u0026quot;wildfly\u0026quot; created imagestream \u0026quot;mysql\u0026quot; created imagestream \u0026quot;mariadb\u0026quot; created imagestream \u0026quot;postgresql\u0026quot; created imagestream \u0026quot;mongodb\u0026quot; created imagestream \u0026quot;redis\u0026quot; created imagestream \u0026quot;jenkins\u0026quot; created  导入应用模板(openshift项目中) oc project openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/mariadb-ephemeral-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/mariadb-persistent-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/mongodb-ephemeral-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/mongodb-persistent-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/mysql-ephemeral-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/mysql-persistent-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/redis-ephemeral-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/redis-persistent-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/quickstart-templates/cakephp-mysql.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/quickstart-templates/cakephp-mysql-persistent.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/quickstart-templates/jenkins-ephemeral-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/quickstart-templates/jenkins-persistent-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/quickstart-templates/httpd.json -n openshift  查看导入的镜像列表 [root@openshift-master ~]# oc project openshift Now using project \u0026quot;openshift\u0026quot; on server \u0026quot;https://192.168.124.22:8443\u0026quot;. [root@openshift-master ~]# oc get is -n openshift NAME DOCKER REPO TAGS UPDATED httpd 172.30.111.126:5000/openshift/httpd latest,2.4 About an hour ago jenkins 172.30.111.126:5000/openshift/jenkins latest,1,2 About an hour ago mariadb 172.30.111.126:5000/openshift/mariadb 10.1,latest About an hour ago mongodb 172.30.111.126:5000/openshift/mongodb 2.4,latest,3.2 + 1 more... About an hour ago mysql 172.30.111.126:5000/openshift/mysql latest,5.7,5.6 + 1 more... About an hour ago nodejs 172.30.111.126:5000/openshift/nodejs latest,0.10,4 + 1 more... About an hour ago perl 172.30.111.126:5000/openshift/perl latest,5.24,5.20 + 1 more... About an hour ago php 172.30.111.126:5000/openshift/php 5.5,latest,7.0 + 1 more... About an hour ago postgresql 172.30.111.126:5000/openshift/postgresql latest,9.5,9.4 + 1 more... About an hour ago python 172.30.111.126:5000/openshift/python latest,3.5,3.4 + 2 more... About an hour ago redis 172.30.111.126:5000/openshift/redis latest,3.2 About an hour ago ruby 172.30.111.126:5000/openshift/ruby 2.3,2.2,2.0 + 1 more... About an hour ago wildfly 172.30.111.126:5000/openshift/wildfly latest,10.1,10.0 + 2 more... About an hour ago  查看导入的应用模板列表(相当于企业内部的APPSTORE 应用市场) [root@openshift-master ~]# oc get templates -n openshift NAME DESCRIPTION PARAMETERS OBJECTS cakephp-mysql-example An example CakePHP application with a MySQL database. For more information ab... 19 (4 blank) 8 cakephp-mysql-persistent An example CakePHP application with a MySQL database. For more information ab... 20 (4 blank) 9 httpd-example An example Httpd application that serves static content. For more information... 9 (3 blank) 5 jenkins-ephemeral Jenkins service, without persistent storage.... 7 (all set) 6 jenkins-persistent Jenkins service, with persistent storage.... 8 (all set) 7 mariadb-ephemeral MariaDB database service, without persistent storage. For more information ab... 7 (3 generated) 3 mariadb-persistent MariaDB database service, with persistent storage. For more information about... 8 (3 generated) 4 mongodb-ephemeral MongoDB database service, without persistent storage. For more information ab... 8 (3 generated) 3 mongodb-persistent MongoDB database service, with persistent storage. For more information about... 9 (3 generated) 4 mysql-ephemeral MySQL database service, without persistent storage. For more information abou... 8 (3 generated) 3 mysql-persistent MySQL database service, with persistent storage. For more information about u... 9 (3 generated) 4 redis-ephemeral Redis in-memory data structure store, without persistent storage. For more in... 5 (1 generated) 3 redis-persistent Redis in-memory data structure store, with persistent storage. For more infor... 6 (1 generated) 4  "
},
{
	"uri": "/openshift/install/arch_intro/",
	"title": "架构介绍",
	"tags": [],
	"description": "介绍 openshift origin 容器云的体系架构",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    Openshift Origin v3.6 架构图 OpenShift 是一款容器应用平台，它将 Docker 和 Kubernetes 技术带入企业。\n上图是openshift origin 总体架构图\n上图是 openshift 和 k8s 所在容器云平台的关系\n主服务和计算节点关系结构 主服务器(Masters)依赖于基于etcd的分布式目录， 主要用来提供配置共享和服务发现\n计算节点(Nodes) 主要用来作为PODS的宿主和运行容器\n整体应用概念介绍 上述应用架构图中， 概念来源于Kubernetes的概念， 需要明白以下主要的对象。\n 一个 POD 是一个Docker 容器的运行环境(如果需要共享本地的资源， 我们将在单独的POD中布署两种类别的容器) 一个 Service 服务是一个入口(VIP)，抽象出一个均衡访问负载到一组相同的容器，理论上， 最少是一个服务对应一个架构层 一个服务布署者(Service Deployer)或布署配置(Deployment Config)是一个对象， 用来描述基于触发器的容器的布署策略(比如，当docker仓库中有新版本的映象时， 重新布署) 一个复制控制器(Replication Controller)是一个技术组件， 主要负责POD 的弹性。 一个路由(Route)是用来暴露一个应用的入口(域名解析， 主机名或VIP)  "
},
{
	"uri": "/openshift/install/env_intro/",
	"title": "部署环境介绍",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    Openshift v3.6 高可用部署架构图  Masters Node 负载均衡使用 HAPROXY 做基于tcp模式的负载(SSL证书穿透) Data Store 是使用etcd作为信息的存储数据库 Infrastructure Node 基础设施节点,用于运行平台自身的管理服务(route,docker仓库,度量数据,日志数据等),也可以定义一些其他功能节点 比如ops Node 主要运维相关的服务(zabbix,cmdb,ticket等),dev Node 主要运行研发相关服务(maven镜像库,gitlab,go-cd等) Persistent Storage 持久卷这里使用的是 CEPH https://ceph.com/ 分布式文件系统 操作系统发行版本使用的是 CENTOS 7  部署主机角色说明    主机角色 IP地址 操作系统 摘要 域名     管理节点(Master) openshift-master1(192.168.124.22) CentOS Linux release 7.3.1611 (Core) x86_64 master + etcd + haproxy openshift.ops.com   管理节点(Master) openshift-master2(192.168.124.23) CentOS Linux release 7.3.1611 (Core) x86_64 master + etcd openshift.ops.com   管理节点(Master) openshift-master3(192.168.124.24) CentOS Linux release 7.3.1611 (Core) x86_64 master + etcd openshift.ops.com   基础设施节点(Node) openshift-node1（192.168.124.30） CentOS Linux release 7.3.1611 (Core) X86-64 router + registry lb.openshift.ops.com   计算节点(Node) openshift-node2（192.168.124.46） CentOS Linux release 7.3.1611 (Core) X86-64 计算节点 无对外域名    注：本环境中 master节点的负载均衡haproxy 示例没有配置2个节点,如需要,可以参考互联网上 haproxy + keeplived 方式实现\n域名设置说明    域名角色 通配域名(泛域名) CNAME地址     开发环境域名 *.dev.openshift.ops.com lb.openshift.ops.com   测试环境域名 *.test.openshift.ops.com lb.openshift.ops.com   生产环境域名 *.prod.openshift.ops.com lb.openshift.ops.com    注：这里为每个环境都配置范域名解析,都指向 router 所在的计算节点上,如果没有配置DNS,做hosts绑定域名也可以\n但是 master的高可用负载域名 openshift.ops.com(指向haproxy) 在master节点所配置的dns必须能够解析,hosts绑定是无效的, 原因是计算节点运行的容器内部 /etc/resolve.conf dns是指向master节点的,如果容器内需要访问 openshift.ops.com 这个域名就会forward 主服务节点配置的dns上\n "
},
{
	"uri": "/openshift/install/etcd_install/",
	"title": "Etcd集群安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    概述 etcd 是 CoreOS 团队发起的开源项目，基于 Go 语言实现，做为一个分布式键值对存储，通过分布式锁，leader选举和写屏障(write barriers)来实现可靠的分布式协作。\n可用于服务注册发现与共享配置，具有以下优点。\n简单 ： 相比于晦涩难懂的paxos算法，etcd基于相对简单且易实现的raft算法实现一致性，并通过gRPC提供接口调用\n安全：支持TLS通信，并可以针对不同的用户进行对key的读写控制\n高性能：10,000 /秒的写性能\nETCD证书  etcd集群信息     主机名 IP地址 域名 etcd版本     etcd0 192.168.124.22 etcd0.51know.info etcd-3.2.15-1.el7.x86_64   etcd1 192.168.124.23 etcd1.51know.info etcd-3.2.15-1.el7.x86_64   etcd2 192.168.124.24 etcd0.51know.info etcd-3.2.15-1.el7.x86_64     证书生成     证书名称 配置文件 用途     etcd-root-ca.pem etcd-root-ca-csr.json etcd 根 CA 证书   etcd.pem etcd-gencert.json、etcd-csr.json etcd 集群证书     CFSSL 工具安装 首先下载 cfssl，并给予可执行权限，然后扔到 PATH 目录下  [root@openshift-master1 /opt]# wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 [root@openshift-master1 /opt]# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 [root@openshift-master1 /opt]# chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 [root@openshift-master1 /opt]# mv cfssl_linux-amd64 /usr/local/bin/cfssl [root@openshift-master1 /opt]# mv cfssljson_linux-amd64 /usr/local/bin/cfssljson   Etcd 证书生成所需配置文件如下:  [root@openshift-master1 /opt]# cat etcd-root-ca-csr.json { \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 4096 }, \u0026quot;names\u0026quot;: [ { \u0026quot;O\u0026quot;: \u0026quot;etcd\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;etcd Security\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Beijing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Beijing\u0026quot;, \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot; } ], \u0026quot;CN\u0026quot;: \u0026quot;etcd-root-ca\u0026quot; } [root@openshift-master1 /opt]# cat etcd-gencert.json { \u0026quot;signing\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot;, \u0026quot;client auth\u0026quot; ], \u0026quot;expiry\u0026quot;: \u0026quot;87600h\u0026quot; } } } [root@openshift-master1 /opt]# cat etcd-csr.json { \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 4096 }, \u0026quot;names\u0026quot;: [ { \u0026quot;O\u0026quot;: \u0026quot;etcd\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;etcd Security\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Beijing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Beijing\u0026quot;, \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot; } ], \u0026quot;CN\u0026quot;: \u0026quot;etcd\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;127.0.0.1\u0026quot;, \u0026quot;localhost\u0026quot;, \u0026quot;192.168.124.22\u0026quot;, \u0026quot;192.168.124.23\u0026quot;, \u0026quot;192.168.124.24\u0026quot; ] }  注意: hosts 要将 etcd 集群的所在节点的 IP地址,主机名(FQDN),都要加入到此列表中\n 生成 Etcd 证书  [root@openshift-master1 /opt]# cfssl gencert --initca=true etcd-root-ca-csr.json | cfssljson --bare etcd-root-ca [root@openshift-master1 /opt]# cfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd #生成的证书列表如下 [root@openshift-master1 /opt] # ll 总用量 36 -rw-r--r-- 1 root root 2033 3月 27 18:09 etcd.csr -rw-r--r-- 1 root root 513 3月 27 18:09 etcd-csr.json -rw-r--r-- 1 root root 204 3月 27 18:08 etcd-gencert.json -rw------- 1 root root 3247 3月 27 18:09 etcd-key.pem -rw-r--r-- 1 root root 2415 3月 27 18:09 etcd.pem -rw-r--r-- 1 root root 1708 3月 27 18:09 etcd-root-ca.csr -rw-r--r-- 1 root root 232 3月 27 18:07 etcd-root-ca-csr.json -rw------- 1 root root 3243 3月 27 18:09 etcd-root-ca-key.pem -rw-r--r-- 1 root root 2078 3月 27 18:09 etcd-root-ca.pem  部署 ETCD 集群 第一个节点etcd0 安装  安装etcd，并将证书拷贝安装目录，赋权  [root@openshift-master1 /opt]# yum install etcd -y [root@openshift-master1 /opt]# cp *.pem /etc/etcd/ [root@openshift-master1 /opt]# chown -R etcd:etcd /etc/etcd/ [root@openshift-master1 /opt]# chmod -R 755 /etc/etcd/   配置内容  [root@openshift-master etcd]# cat /etc/etcd/etcd.conf #[Member] ETCD_DATA_DIR=\u0026quot;/var/lib/etcd/default.etcd\u0026quot; ETCD_LISTEN_PEER_URLS=\u0026quot;https://192.168.124.22:2380\u0026quot; ETCD_LISTEN_CLIENT_URLS=\u0026quot;https://192.168.124.22:2379,http://localhost:2379\u0026quot; ETCD_NAME=\u0026quot;etcd0\u0026quot; ETCD_HEARTBEAT_INTERVAL=500 ETCD_ELECTION_TIMEOUT=2500 #[Clustering] ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026quot;https://192.168.124.22:2380\u0026quot; ETCD_ADVERTISE_CLIENT_URLS=\u0026quot;https://192.168.124.22:2379\u0026quot; ETCD_INITIAL_CLUSTER=\u0026quot;etcd0=https://192.168.124.22:2380,etcd1=https://192.168.124.23:2380,etcd2=https://192.168.124.24:2380\u0026quot; ETCD_INITIAL_CLUSTER_TOKEN=\u0026quot;etcd-cluster\u0026quot; ETCD_INITIAL_CLUSTER_STATE=\u0026quot;new\u0026quot; #[Security] ETCD_CERT_FILE=\u0026quot;/etc/etcd/etcd.pem\u0026quot; ETCD_KEY_FILE=\u0026quot;/etc/etcd/etcd-key.pem\u0026quot; ETCD_CLIENT_CERT_AUTH=\u0026quot;true\u0026quot; ETCD_TRUSTED_CA_FILE=\u0026quot;/etc/etcd/etcd-root-ca.pem\u0026quot; ETCD_AUTO_TLS=\u0026quot;true\u0026quot; ETCD_PEER_CERT_FILE=\u0026quot;/etc/etcd/etcd.pem\u0026quot; ETCD_PEER_KEY_FILE=\u0026quot;/etc/etcd/etcd-key.pem\u0026quot; ETCD_PEER_CLIENT_CERT_AUTH=\u0026quot;true\u0026quot; ETCD_PEER_TRUSTED_CA_FILE=\u0026quot;/etc/etcd/etcd-root-ca.pem\u0026quot; ETCD_PEER_AUTO_TLS=\u0026quot;true\u0026quot;   启动服务  [root@openshift-master1 /opt]# systemctl enable etcd [root@openshift-master1 /opt]# systemctl start etcd  其他2个节点安装  安装 etcd 软件包  yum install etcd -y   将第一个节点的配置拷贝到其他2个节点  [root@openshift-master ~]# cd /etc/etcd/ [root@openshift-master etcd]# ll total 20 -rwxr-xr-x 1 etcd etcd 920 Apr 18 06:11 etcd.conf -rwxr-xr-x 1 etcd etcd 3243 Apr 18 06:07 etcd-key.pem -rwxr-xr-x 1 etcd etcd 2167 Apr 18 06:07 etcd.pem -rwxr-xr-x 1 etcd etcd 3247 Apr 18 06:07 etcd-root-ca-key.pem -rwxr-xr-x 1 etcd etcd 2078 Apr 18 06:07 etcd-root-ca.pem [root@openshift-master1 etcd]# scp * openshift-master2:/etc/etcd/ etcd.conf 100% 920 0.9KB/s 00:00 etcd-key.pem 100% 3243 3.2KB/s 00:00 etcd.pem 100% 2167 2.1KB/s 00:00 etcd-root-ca-key.pem 100% 3247 3.2KB/s 00:00 etcd-root-ca.pem 100% 2078 2.0KB/s 00:00   在其他2个节点上修改如下配置项,ip地址改成本节点的对应的IP地址  ETCD_LISTEN_PEER_URLS=\u0026quot;https://192.168.124.23:2380\u0026quot; ETCD_LISTEN_CLIENT_URLS=\u0026quot;https://192.168.124.23:2379,http://localhost:2379\u0026quot; #ETCD节点名称 按顺序增加即可 ETCD_NAME=\u0026quot;etcd1\u0026quot; ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026quot;https://192.168.124.23:2380\u0026quot; ETCD_ADVERTISE_CLIENT_URLS=\u0026quot;https://192.168.124.23:2379\u0026quot;   启动服务即可  验证(3个节点都安装配置完成后) [root@openshift-master etcd]# export ETCDCTL_API=3 [root@openshift-master etcd]# etcdctl member list 2da38978bc038ba1, started, etcd1, https://192.168.124.22:2380, https://192.168.124.22:2379 56e71904a9636fcf, started, etcd0, https://192.168.124.23:2380, https://192.168.124.23:2379 faf6915e4bb01350, started, etcd2, https://192.168.124.24:2380, https://192.168.124.24:2379 [root@openshift-master etcd]# etcdctl --cacert=/etc/etcd/etcd-root-ca.pem --cert=/etc/etcd/etcd.pem --key=/etc/etcd/etcd-key.pem --endpoints=https://192.168.124.22:2379,https://192.168.124.23:2379,https://192.168.124.24:2379 endpoint health https://192.168.124.22:2379 is healthy: successfully committed proposal: took = 3.852481ms https://192.168.124.23:2379 is healthy: successfully committed proposal: took = 4.035725ms https://192.168.124.24:2379 is healthy: successfully committed proposal: took = 1.489679ms  至此,etcd集群安装完成\nopenshift 集群数据初始化 如果在安装过程中，可能需要重新初始化数据可以参考如下方法\n 导出API版本，这里使用3版本\nexport ETCDCTL_API=3  获取key\netcdctl get / --prefix --keys-only  删除 openshift.io 相关内容\netcdctl del --prefix=true /openshift.io  删除 kubernetes.io\netcdctl del --prefix=true /kubernetes.io  再次查看\netcdctl get / --prefix --keys-only   etcd 集群维护 找出集群中 有问题的etcd节点 [root@openshift-master ~]# etcdctl member list 2da38978bc038ba1, started, etcd1, https://192.168.124.30:2380, https://192.168.124.30:2379 56e71904a9636fcf, started, etcd0, https://192.168.124.22:2380, https://192.168.124.22:2379 faf6915e4bb01350, started, etcd2, https://192.168.124.46:2380, https://192.168.124.46:2379 [root@openshift-master ~]# etcdctl --cacert=/etc/etcd/etcd-root-ca.pem --cert=/etc/etcd/etcd.pem --key=/etc/etcd/etcd-key.pem --endpoints=https://192.168.124.22:2379,https://192.168.124.30:2379,https://192.168.124.46:2379 endpoint health https://192.168.124.22:2379 is healthy: successfully committed proposal: took = 14.574112ms https://192.168.124.30:2379 is healthy: successfully committed proposal: took = 16.811619ms https://192.168.124.46:2379 is unhealthy: failed to connect: dial tcp 192.168.124.46:2379: getsockopt: connection refused Error: unhealthy cluster  从集群中删除有问题的节点 这里删除 etcd2 集群内序号 faf6915e4bb01350\netcdctl --cacert=/etc/etcd/etcd-root-ca.pem --cert=/etc/etcd/etcd.pem --key=/etc/etcd/etcd-key.pem \\ --endpoints=https://192.168.124.22:2379,https://192.168.124.30:2379,https://192.168.124.46:2379 member remove faf6915e4bb01350  修改问题节点配置 # 修改节点配置 ETCD_INITIAL_CLUSTER_STATE=existing # 删除etcd数据目录 rm -fr /var/lib/etcd/member  重新加入问题节点 etcdctl --cacert=/etc/etcd/etcd-root-ca.pem --cert=/etc/etcd/etcd.pem --key=/etc/etcd/etcd-key.pem \\ --endpoints=https://192.168.124.22:2379 member add etcd2 --peer-urls=\u0026quot;https://192.168.124.46:2380\u0026quot;  重启问题节点即可 $ systemctl restart etcd  "
},
{
	"uri": "/openshift/install/master_install/",
	"title": "主服务安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    部署主机角色说明    主机角色 IP地址 负载域名     管理节点(Master) openshift-master1(192.168.124.22) openshift.ops.com   管理节点(Master) openshift-master2(192.168.124.23) openshift.ops.com   管理节点(Master) openshift-master3(192.168.124.24) openshift.ops.com    安装前基础环境检查 配置仓库源(如果自己建立本地源 直接跳过本步骤)  安装仓库源文件  yum install centos-release-openshift-origin.noarch   修改仓库配置 指向 V3.6版本(默认是指向最新的版本)  [root@openshift-master ~]# cat /etc/yum.repos.d/CentOS-OpenShift-Origin.repo [centos-openshift-origin] name=CentOS OpenShift Origin baseurl=http://mirrors.163.com/centos/7/paas/x86_64/openshift-origin36/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-PaaS 其他省略。。。。。。。。  主要是把 http://mirror.centos.org/centos/7/paas/x86_64/openshift-origin/ 修改为下面链接(使用163镜像) http://mirrors.163.com/centos/7/paas/x86_64/openshift-origin36/\n安装第一个master节点软件包  安装 origin-master  yum install origin-master -y   由于yum安装的master节点的证书不包含对外负载域名(openshift.ops.com),所以需要重新签发证书,并删除默认node配置  openshift start master --public-master='https://openshift.ops.com' --network-plugin='redhat/openshift-ovs-subnet' --write-config=/etc/origin/master rm -fr /etc/origin/node   查看证书生成的信息,DNS中会生成一个openshift.ops.com的域名  [root@openshift-master master]# openssl x509 -noout -text -in master.server.crt Certificate: Data: Version: 3 (0x2) Serial Number: 11 (0xb) Signature Algorithm: sha256WithRSAEncryption Issuer: CN=openshift-signer@1524139430 Validity Not Before: Apr 19 12:06:27 2018 GMT Not After : Apr 18 12:06:28 2020 GMT Subject: CN=127.0.0.1 Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:b1:93:7c:57:d5:e1:c1:2c:59:1a:28:9e:b0:df: 38:cc:de:ab:d3:ab:6a:fa:97:3a:f2:79:80:26:0b: f0:92:7f:e3:e8:be:da:37:43:d0:f6:ce:d9:c1:e0: a5:cb:cf:af:04:bb:a4:bc:84:2c:a4:97:08:d4:c1: a5:d5:48:4f:3a:96:fb:2e:66:ad:6e:1f:d1:4a:8d: 21:c4:68:3d:f2:79:e2:3e:c5:e1:ee:78:2b:63:96: d7:fa:f2:e8:b4:58:45:1c:ba:6c:ca:0f:4b:b3:cf: 26:95:43:fe:fa:43:88:a4:48:c7:4e:07:83:66:eb: fe:48:78:f2:07:24:7c:a8:f4:6f:7b:80:5a:7e:7d: 0f:b2:87:46:5b:76:05:e2:d3:f0:58:87:69:64:5a: 17:91:70:6f:81:90:89:ac:65:57:cc:f2:67:8b:c7: 26:0d:79:b7:84:3f:58:ec:5c:d7:a2:85:17:36:e8: 62:86:6d:3d:21:43:38:cf:1c:2c:c4:c9:3d:6c:b4: da:c3:0c:5e:ca:3f:74:ff:b7:39:1e:fb:63:bf:47: 66:54:54:8f:88:c3:8f:ba:a5:dd:70:ec:53:6a:ce: 49:48:77:1a:10:cc:81:bb:85:a4:55:b7:07:e9:fa: 7c:67:38:40:35:1c:bf:cf:ee:45:79:19:6b:69:45: 3d:0b Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Subject Alternative Name: DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, DNS:localhost, DNS:openshift, DNS:openshift.default, DNS:openshift.default.svc, DNS:openshift.default.svc.cluster.local, DNS:openshift.ops.com, DNS:127.0.0.1, DNS:172.30.0.1, DNS:192.168.124.22, IP Address:127.0.0.1, IP Address:172.30.0.1, IP Address:192.168.124.22 Signature Algorithm: sha256WithRSAEncryption d5:8b:11:66:64:0e:cc:b9:36:85:15:1f:75:02:d1:9b:5c:32: b5:af:1e:d9:38:85:e0:95:77:d7:5d:42:dd:e9:40:07:c8:d2: ae:4b:99:db:8f:61:49:e7:3b:37:4b:22:cc:0b:07:5d:6a:39: ce:82:e8:00:38:4e:af:14:1b:9c:78:6a:2e:58:b8:44:c0:62: 96:18:7d:58:2c:9c:db:87:e3:47:20:61:97:7f:ae:3f:74:c5: 4a:cc:88:e6:6b:1b:4c:b4:16:6d:66:99:4a:7f:bc:51:ec:b4: 17:66:56:ab:d5:16:0f:a8:2b:8b:5c:dc:91:e1:bc:3b:99:41: 5b:ad:cb:f0:52:20:23:93:46:44:de:cf:fe:70:27:ec:8d:eb: 65:23:84:5d:cb:75:18:31:19:d9:0d:8c:43:0b:6f:c7:97:1e: 02:41:d9:07:93:bb:b0:dc:53:08:54:0e:48:cc:1c:60:4d:87: c2:a8:be:56:55:af:53:62:21:29:2b:43:eb:38:45:f9:11:52: b6:d8:56:77:3d:a0:34:1c:69:3b:e1:3d:f9:85:46:f9:60:b9: 2e:b4:b2:e2:54:a7:20:7a:a3:50:de:38:ad:4b:31:e3:45:2c: 45:3a:b6:8c:a3:5f:80:47:97:f9:e8:2f:e6:b8:2d:11:55:3d: 0a:6a:fc:18   修改master etcd配置,使用前面配置好的etcd集群  修改前\n[root@openshift-master master]# cat /etc/origin/master/master-config.yaml .....省略..... etcdClientInfo: ca: ca.crt certFile: master.etcd-client.crt keyFile: master.etcd-client.key urls: - https://192.168.124.22:4001 etcdConfig: address: 192.168.124.22:4001 peerAddress: 192.168.124.22:7001 peerServingInfo: bindAddress: 0.0.0.0:7001 bindNetwork: tcp4 certFile: etcd.server.crt clientCA: ca.crt keyFile: etcd.server.key namedCertificates: null servingInfo: bindAddress: 0.0.0.0:4001 bindNetwork: tcp4 certFile: etcd.server.crt clientCA: ca.crt keyFile: etcd.server.key namedCertificates: null storageDirectory: /etc/origin/openshift.local.etcd etcdStorageConfig: kubernetesStoragePrefix: kubernetes.io kubernetesStorageVersion: v1 openShiftStoragePrefix: openshift.io openShiftStorageVersion: v1 .....省略.....  修改后\n[root@openshift-master master]# cat /etc/origin/master/master-config.yaml .....省略..... etcdClientInfo: ca: master.etcd-ca.crt certFile: master.etcd-client.crt keyFile: master.etcd-client.key urls: - https://192.168.124.22:2379 - https://192.168.124.23:2379 - https://192.168.124.24:2379 etcdStorageConfig: kubernetesStoragePrefix: kubernetes.io kubernetesStorageVersion: v1 openShiftStoragePrefix: openshift.io openShiftStorageVersion: v1 .....省略.....   openshift配置与etcd的证书对应关系     openshift证书名 etcd证书名     master.etcd-ca.crt etcd-root-ca.pem   master.etcd-client.crt etcd.pem   master.etcd-client.key etcd-key.pem     拷贝相关证书到master目录  [root@openshift-master master]# cat /etc/origin/master/master-config.yaml ^C [root@openshift-master master]# cp /etc/etcd/etcd-root-ca.pem /etc/origin/master/master.etcd-ca.crt [root@openshift-master master]# cp /etc/etcd/etcd.pem /etc/origin/master/master.etcd-client.crt cp: overwrite ‘/etc/origin/master/master.etcd-client.crt’? y [root@openshift-master master]# cp /etc/etcd/etcd-key.pem /etc/origin/master/master.etcd-client.key cp: overwrite ‘/etc/origin/master/master.etcd-client.key’? y   启动master  systemctl enable origin-master.service systemctl start origin-master.service  权限登录配置 超级admin用户登陆配置说明(只能命令行登陆)  admin 用户登陆使用密钥文件  /etc/origin/master/admin.kubeconfig   管理员登陆 使用证书密钥进行管理(当前用户是root)  [root@openshift-master ~]# mkdir -p /root/.kube [root@openshift-master ~]# cp /etc/origin/master/admin.kubeconfig /root/.kube/config [root@openshift-master ~]# oc login -u system:admin Logged into \u0026quot;https://192.168.124.22:8443\u0026quot; as \u0026quot;system:admin\u0026quot; using existing credentials. You have access to the following projects and can switch between them with 'oc project \u0026lt;projectname\u0026gt;': * default kube-public kube-system openshift openshift-infra Using project \u0026quot;default\u0026quot;.   执行 如下命令可以看到当前登录用户  [root@openshift-master ~]# oc whoami system:admin  WEBUI 登陆用户配置  安装完默认是任意密码登陆，这里配置htpasswd方式进行认证  修改前\n provider: apiVersion: v1 kind: AllowAllPasswordIdentityProvider  修改后\n provider: apiVersion: v1 kind: HTPasswdPasswordIdentityProvider file: /etc/origin/master/htpasswd   安装htpasswd  [root@openshift-master master]# yum install httpd-tools -y   添加ops账户  [root@openshift-master master]# htpasswd -c /etc/origin/master/htpasswd ops New password: Re-type new password: Adding password for user ops   将ops用户加入openshift-infra 和 default 项目，并赋予admin权限  [root@openshift-master master]# oc adm policy add-role-to-user admin ops -n openshift-infra role \u0026quot;admin\u0026quot; added: \u0026quot;ops\u0026quot; [root@openshift-master master]# oc adm policy add-role-to-user admin ops -n default role \u0026quot;admin\u0026quot; added: \u0026quot;ops\u0026quot;   重启master 节点  systemctl daemon-reload systemctl restart origin-master  至此 第一个master节点安装部署完成\n其他2个master节点安装  参考第一个节点,配置仓库,安装 origin-master 软件包\n 将第一个master节点的 ca配置和相关配置文件拷贝到其他master节点上\nscp ca.* master.etcd-* htpasswd openshift-master2:/etc/origin/master  在其他2个节点重新生成配置\nopenshift start master --public-master='https://openshift.ops.com' --network-plugin='redhat/openshift-ovs-subnet' --write-config=/etc/origin/master  参考上文设置 超级admin用户登陆配置\n 启动master节点即可\n 检查日志,无错误,安装完成\n  "
},
{
	"uri": "/openshift/install/haproxy_install/",
	"title": "Haproxy安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    部署主机说明    主机角色 IP地址 操作系统 摘要 域名     管理节点(Master) openshift-master1(192.168.124.22) CentOS Linux release 7.3.1611 (Core) x86_64 master + etcd + haproxy openshift.ops.com    安装haproxy [root@openshift-master1 /root]# yum install haproxy -y   修改配置文件  [root@openshift-master1 /root]# vim /etc/haproxy/haproxy.cfg # Global settings #--------------------------------------------------------------------- global maxconn 20000 log /dev/log local0 info chroot /var/lib/haproxy pidfile /var/run/haproxy.pid user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats #--------------------------------------------------------------------- # common defaults that all the 'listen' and 'backend' sections will # use if not designated in their block #--------------------------------------------------------------------- defaults mode http log global option httplog option dontlognull # option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 300s timeout server 300s timeout http-keep-alive 10s timeout check 10s maxconn 20000 listen stats :9000 mode http stats enable stats uri / frontend atomic-openshift-api bind *:443 default_backend atomic-openshift-api mode tcp option tcplog backend atomic-openshift-api balance source mode tcp server master0 192.168.124.22:8443 check server master1 192.168.124.23:8443 check server master2 192.168.124.24:8443 check  注意: 确保域名 openshift.ops.com 指向Haproxy (如没配置dns,请在各计算节点做hosts绑定)\n启动服务 systemctl enable haproxy systemctl start haproxy  "
},
{
	"uri": "/openshift/application/ceph_persistent/",
	"title": "应用数据持久化(CEPH)",
	"tags": [],
	"description": "openshift origin 使用ceph 分布式存储进行应用数据持久化",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-27 创建 开源方案 初始版本    持久化概念说明 openshift origin 用的是Kubernetes,在Kubernetes中使用两种API资源来管理存储，分别是PersistentVolume和PersistentVolumeClaim，下面分别介绍下这两种资源的概念：\nPersistentVolume(简称PV)：由管理员设置的存储，它是集群的一部分。就像节点(Node)是集群中的资源一样，PV也是集群中的资源。它包含存储类型，存储大小和访问模式。它的生命周期独立于Pod，例如当使用它的Pod销毁时对PV没有影响。\nPersistentVolumeClaim(简称PVC)： 是用户存储的请求，它和Pod类似。Pod消耗Node资源，PVC消耗PV资源。Pod可以请求特定级别的资源(CPU和MEM)。PVC可以请求特定大小和访问模式的PV。\n静态PV 集群管理员创建一些PV。它们带有可供集群用户使用的实际存储的细节，之后便可用于PVC消费。\n注意： 这种方式请求的PVC必须要与管理员创建的PV保持一致，如：存储大小和访问模式，否则不会将PVC绑定到PV上。\n动态PV 当管理员创建的静态PV都不匹配用户的PVC时，集群可以使用动态的为PVC创建卷，此配置基于StorageClass。\nPVC请求存储类(StorageClass)，且管理员必须要创建并配置该StorageClass，该StorageClass才能进行动态的创建。\n可以看出动态PV更灵活,根据应用的需要按需申请存储空间的大小，动态创建rbd images 并挂载到容器上,里我们使用 Ceph RBD 动态创建PV(persistent volumes)\n openshift origin 集群设置 ceph包安装(master和node节点都需要安装) #安装ceph客户端 yum install centos-release-ceph-jewel.noarch -y yum install -y ceph-common  所有openshift origin计算节点node节点配置更新 #添加如下配置文件，否则会导致无法挂载rbd。 # cat /etc/origin/node/node-config.yaml volumeConfig: dynamicProvisioningEnabled: true localQuota: perFSGroup: null # 重启计算节点 # systemctl restart origin-node  设置ceph(monitor 或者 管理机上操作) 为动态卷创建池子 $ ceph osd pool create kube 1024 # 创建kube池子用户访问授权 $ ceph auth get-or-create client.kube mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=kube' -o ceph.client.kube.keyring # 当前目录下会生成keyring # cat ceph.client.kube.keyring [client.kube] key = AQBmT+1a7WbLIxAADbOQ9EkVT6usAMn5wP5npw==  创建ceph admin访问密钥(base64编码) 在ceph monitor节点上执行获取client.admin的key # ceph auth get-key client.admin | base64 QVFCdUx1eGFLWWhBTlJBQVFiQmxoUURpYVlNbDJsQS90SUl2Wnc9PQ==  创建ceph-secret定义文件 ceph-secret.yaml apiVersion: v1 kind: Secret metadata: name: ceph-secret namespace: kube-system data: #key值是由上一步获取的client.admin的key以base64加密的秘钥 key: QVFCdUx1eGFLWWhBTlJBQVFiQmxoUURpYVlNbDJsQS90SUl2Wnc9PQ== type: kubernetes.io/rbd  openshift master中导入密钥 # oc create -f ceph-secret.yaml secret \u0026quot;ceph-secret\u0026quot; created  查看生成的ceph密钥 # [root@openshift-master ~]# oc get secret ceph-secret -n kube-system NAME TYPE DATA AGE ceph-secret kubernetes.io/rbd 1 28s  创建用户访问密钥 在ceph monitor节点上执行获取client.kube的key # ceph auth get-key client.kube | base64 QVFCbVQrMWE3V2JMSXhBQURiT1E5RWtWVDZ1c0FNbjV3UDVucHc9PQ==  创建ceph-user-secret定义文件(访问kube pool的验证授权) ceph-user-secret.yaml apiVersion: v1 kind: Secret metadata: name: ceph-user-secret namespace: my-project data: key: QVFCbVQrMWE3V2JMSXhBQURiT1E5RWtWVDZ1c0FNbjV3UDVucHc9PQ== type: kubernetes.io/rbd  注意： 上述定义中 namespace: my-project 说明我们是要在my-project项目使用ceph 存储，如果要在其他项目使用 就改成其他项目名称\n openshift master中导入密钥 $ oc create -f ceph-user-secret.yaml secret \u0026quot;ceph-user-secret\u0026quot; created  查看生成的ceph密钥 [root@openshift-master ~]# oc get secret ceph-user-secret -n my-project NAME TYPE DATA AGE ceph-user-secret kubernetes.io/rbd 1 18s  创建 ceph RBD Dynamic Storage Class 定义文件 持久化存储配置文件定义内容 cat ceph-storageclass.yaml apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata: name: dynamic annotations: storageclass.beta.kubernetes.io/is-default-class: \u0026quot;true\u0026quot; provisioner: kubernetes.io/rbd parameters: # ceph monitor节点 monitors: 192.168.124.31:6789 # 能够在pool中创建images 的用户ID,默认是 `admin`. adminId: admin adminSecretName: ceph-secret adminSecretNamespace: kube-system # Ceph RBD pool. Default is `rbd`, but that value is not recommended pool: kube # Ceph client ID that is used to map the Ceph RBD image. Default is the same as `adminId`. userId: kube # The name of Ceph Secret for `userId` to map Ceph RBD image. It must exist in the same namespace as PVCs. It is required unless its set as the default in new projects. userSecretName: ceph-user-secret  导入存储配置 # oc create -f ceph-storageclass.yaml -n openshift storageclass \u0026quot;dynamic\u0026quot; created  创建Persistent Volume Claim 定义文件 # cat ceph-claim.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: ceph-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi  创建pvc # oc create -f ceph-claim.yaml -n project persistentvolumeclaim \u0026quot;ceph-claim\u0026quot; created # oc get pvc NAME STATUS VOLUME CAPACITY ACCESSMODES STORAGECLASS AGE ceph-claim Bound pvc-9c78da45-506a-11e8-ac10-06d28000000c 2Gi RWO dynamic 1m  注意: pvc-9c78da45-506a-11e8-ac10-06d28000000c 这个就是持久卷请求动态创建的Ceph RBD PV\n解释下 PVC 和 PV PVC 会在kube RBD pool中请求创建 RBD image 并映射成PV(/dev/rbdx)设备\n查看在kube pool 中自动创建的images (ceph monitor节点上执行) # rbd list -p kube --name client.kube --keyring ceph.client.kube.keyring kubernetes-dynamic-pvc-9c7efeac-506a-11e8-ac10-06d28000000c  通过openshift web 创建PVC 创建 MariaDB 持久化数据库 再到ceph monitor节点查看下rbd images情况\n[root@open-ceph ~]# rbd list -p kube --name client.kube --keyring ceph.client.kube.keyring kubernetes-dynamic-pvc-31165112-5074-11e8-ac10-06d28000000c kubernetes-dynamic-pvc-98513d20-5074-11e8-ac10-06d28000000c kubernetes-dynamic-pvc-9c7efeac-506a-11e8-ac10-06d28000000c  总结 这边文档讲述了如何在openshift中使用Ceph RBD作为持久化存储. 我们可以提供高性能存储(ssd),大容量等需求,最终实现 storage-as-a-service!\n参考资料 https://docs.openshift.org/3.6/install_config/storage_examples/ceph_rbd_dynamic_example.html#ceph-rbd-dynamic-example-create-pool-for-dynamic-volumes\n"
},
{
	"uri": "/openshift/install/node_install/",
	"title": "计算节点安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    部署主机角色说明    主机角色 IP地址 域名     基础设施节点(Node) openshift-node1（192.168.124.30） lb.openshift.ops.com   计算节点(Node) openshift-node2（192.168.124.46） 无对外域名    安装node节点(计算节点)  设置仓库源（参考主服务安装）\n 安装docker\nyum install docker -y  配置docker\n  [root@openshift-node1 node]# cat /etc/docker/daemon.json { \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;http://ef017c13.m.daocloud.io\u0026quot;], \u0026quot;insecure-registries\u0026quot;: [ \u0026quot;172.30.0.0/16\u0026quot;,\u0026quot;172.30.102.47:5000\u0026quot;,\u0026quot;openshift-master1:5000\u0026quot;] }  注意: insecure-registries 代表docker 使用http 而不使用https(默认是https方式)访问 docker镜像仓库\n 启动docker  systemctl enable docker systemctl start docker   安装 node 软件包\nyum install origin-node origin-sdn-ovs -y  生成 node 配置文件(默认是没有的) 将master的 ca配置文件 拷贝到 node 节点 (master节点上操作)\ncd /etc/origin/master/ scp ca.crt ca.key ca.serial.txt openshift-node1:/etc/origin/node/  在node节点上 生成配置(node节点上操作)\n  oc adm create-node-config \\ --node-dir=/etc/origin/node \\ --node=openshift-node1 \\ --hostnames=openshift-node1,192.168.124.30 \\ --certificate-authority=\u0026quot;/etc/origin/node/ca.crt\u0026quot; \\ --signer-cert=\u0026quot;/etc/origin/node/ca.crt\u0026quot; \\ --signer-key=\u0026quot;/etc/origin/node/ca.key\u0026quot; \\ --signer-serial=\u0026quot;/etc/origin/node/ca.serial.txt\u0026quot; \\ --node-client-certificate-authority=\u0026quot;/etc/origin/node/ca.crt\u0026quot; \\ --network-plugin=\u0026quot;redhat/openshift-ovs-subnet\u0026quot; \\ --dns-ip='172.30.0.1' \\ --master='https://openshift.ops.com'  注意: 如下设置要改成对应计算节点的信息\n--node=openshift-node1 \\ --hostnames=openshift-node1,192.168.124.30 \\   启动node节点(确保openshhift.ops.com能够解析或者hosts绑定)  systemctl enable origin-node systemctl start origin-node   检查日志和到服务端确认node注册成功  [root@openshift-master master]# oc get node NAME STATUS AGE VERSION openshift-node1 Ready 44s v1.6.1+5115d708d7 [root@openshift-master master]# oc get hostsubnet NAME HOST HOST IP SUBNET openshift-node1 openshift-node1 192.168.124.30 10.128.0.0/23  "
},
{
	"uri": "/openshift/install/router_registry_install/",
	"title": "Router And Registry安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    部署主机角色说明    主机角色 IP地址 节点标签 域名     基础设施节点(Node) openshift-node1（192.168.124.30) zone=ops lb.openshift.ops.com    说明:\nrouter 组件是用户访问的入口，域名都需要指向Router组件所在运行的计算节点上\nregistry组件是openshift集群内部使用的docker仓库,主要存放源代码打包生成的镜像\n部署 Router 组件  给Node节点打标签  [root@openshift-master master]# oc label node openshift-node1 zone=ops node \u0026quot;openshift-node1\u0026quot; labeled [root@openshift-master master]# oc get node --show-labels NAME STATUS AGE VERSION LABELS openshift-node1 Ready 3h v1.6.1+5115d708d7 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=openshift-node1,zone=ops openshift-node2 Ready 31m v1.6.1+5115d708d7 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=openshift-node2  通过对Node节点打标签，在部署组件的时候，可以指定部署到特定节点上\n 建立一个 service account 关联router 并赋予权限  [root@openshift-master ~]# oc project default [root@openshift-master ~]# oadm policy add-scc-to-user privileged system:serviceaccount:default:router [root@openshift-master ~]# oadm policy add-cluster-role-to-user cluster-reader system:serviceaccount:default:router cluster role \u0026quot;cluster-reader\u0026quot; added: \u0026quot;system:serviceaccount:default:router\u0026quot;   创建一个名为 router01 的实例，在指定的计算节点上  [root@openshift-master ~]# oadm router router01 --replicas=1 --service-account=router --selector='zone=ops' info: password for stats user admin has been set to iC3sKtFY5k --\u0026gt; Creating router router01 ... serviceaccount \u0026quot;router\u0026quot; created clusterrolebinding \u0026quot;router-router01-role\u0026quot; created deploymentconfig \u0026quot;router01\u0026quot; created service \u0026quot;router01\u0026quot; created --\u0026gt; Success 查看状态 [root@openshift-master ~]# oc get pod -n default NAME READY STATUS RESTARTS AGE router01-1-deploy 0/1 ContainerCreating 0 1m 正在下载docker images 过几分钟再看(取决于下载速度) [root@openshift-master ~]# oc get pod -n default NAME READY STATUS RESTARTS AGE router01-1-hvc1j 1/1 Running 0 1m  部署registry [root@openshift-master ~]# oadm registry --config=/etc/origin/master/admin.kubeconfig --service-account=registry --selector='zone=ops' --\u0026gt; Creating registry registry ... serviceaccount \u0026quot;registry\u0026quot; created clusterrolebinding \u0026quot;registry-registry-role\u0026quot; created deploymentconfig \u0026quot;docker-registry\u0026quot; created service \u0026quot;docker-registry\u0026quot; created --\u0026gt; Success [root@openshift-master ~]# oc get pod NAME READY STATUS RESTARTS AGE docker-registry-1-deploy 0/1 ContainerCreating 0 3s router01-1-hvc1j 1/1 Running 0 14h [root@openshift-master ~]# oc get pod NAME READY STATUS RESTARTS AGE docker-registry-1-k5zq1 1/1 Running 0 3m router01-1-hvc1j 1/1 Running 0 14h  查看各个SERVICE的内部集群地址 [root@openshift-master ~]# oc get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE docker-registry 172.30.111.126 \u0026lt;none\u0026gt; 5000/TCP 9m kubernetes 172.30.0.1 \u0026lt;none\u0026gt; 443/TCP,53/UDP,53/TCP 22h router01 172.30.121.139 \u0026lt;none\u0026gt; 80/TCP,443/TCP,1936/TCP 15h  注意: 172.30.0.0/16 这个段是 cluster ip,如果容器出现问题或者迁移，这个 cluster ip 是不会改变的\n"
},
{
	"uri": "/openshift/install/metric_install/",
	"title": "度量系统安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    度量采集服务架构图 使用ansible进行部署  安装openshift ansible 软件包\nyum install ansible.noarch java-1.8.0-openjdk-headless -y  clone openshift-ansible 项目\n  cd /opt git clone https://github.com/openshift/openshift-ansible.git git checkout origin/release-3.6   配置metric_hosts部署文件  [root@openshift-master ~]# cat /etc/ansible/metric_hosts [OSEv3:children] masters nodes [OSEv3:vars] ansible_ssh_user=root openshift_deployment_type=origin openshift_metrics_install_metrics=True openshift_metrics_image_prefix=openshift/origin- #拉取的镜像版本 openshift_metrics_image_version=v3.6.1 openshift_metrics_resolution=10s #metrics_hawkular服务对外的域名(指向router节点) openshift_metrics_hawkular_hostname=metrics.ops.com #master的ip #if you have a custom install in which the Kubernetes master is not available under https://kubernetes.default.svc:443 #you can specify the value to use instead with the openshift_metrics_master_url parameter #因此 我们这里不需要指定 #openshift_metrics_master_url=https://openshift.ops.com # cassandra 使用临时存储 openshift_metrics_cassandra_limits_memory=1G openshift_metrics_cassandra_storage_type=emptydir [masters] openshift-master [nodes] openshift-node1 openshift-node2   执行ansible-playbook(第一次执行可能会失败，如果失败就再执行一次)  ansible-playbook -i /etc/ansible/metric_hosts /opt/openshift-ansible/playbooks/byo/openshift-cluster/openshift-metrics.yml   执行结果，部署的快慢根据由拉取镜像的速度决定，成功的结果如下(READY都是1/1表示成功)\n# oc get pod -n openshift-infra NAME READY STATUS RESTARTS AGE hawkular-cassandra-1-m1p40 1/1 Running 0 4m hawkular-metrics-lf9hb 1/1 Running 0 4m heapster-1c2h0 1/1 Running 0 4m  检查master-config.yaml配置\ncat /etc/origin/master/master-config.yaml ...... masterPublicURL: https://openshift.ops.com:443 metricsPublicURL: https://metrics.ops.com/hawkular/metrics #确认此选项配置 publicURL: https://openshift.ops.com:443/console/ ....  重启master\nsystemct restart origin-master   成功后查看监控信息 "
},
{
	"uri": "/openshift/install/efk_install/",
	"title": "EFK日志系统安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    部署日志采集服务  因为每个节点都需要采集日志，所以新建的logging项目选择的标签为空。\n[root@hz01-prod-ops-openshiftmaster-01 /]# oc adm new-project logging --node-selector=\u0026quot;\u0026quot;  修改ansible配置文件\n  cat /etc/ansible/log_hosts [OSEv3:children] masters nodes [OSEv3:vars] ansible_ssh_user=root openshift_deployment_type=origin openshift_logging_install_logging=True openshift_logging_image_version=v3.6.1 openshift_logging_kibana_hostname=kibana.ops.com #The URL for the Kubernetes master, this does not need to be public facing but should be accessible from within the cluster. #default is https://kubernetes.default.svc.cluster.local #openshift_logging_master_url=https://openshift.ops.com #The public facing URL for the Kubernetes master. This is used for Authentication redirection by the Kibana proxy. openshift_logging_master_public_url=https://openshift.ops.com openshift_logging_es_memory_limit=1G [masters] openshift-master [nodes] openshift-node1 openshift-node2   执行安装日志采集服务\nansible-playbook -i /etc/ansible/log_hosts /opt/openshift-ansible/playbooks/byo/openshift-cluster/openshift-logging.yml  确认部署是否成功\n[root@hz01-prod-ops-openshiftmaster-01 /root] # oc get pod NAME READY STATUS RESTARTS AGE logging-curator-1-01khs 1/1 Running 0 1d logging-es-data-master-f9r7g76t-1-bpvkq 1/1 Running 0 1d logging-fluentd-05cr8 1/1 Running 0 1d logging-fluentd-wc8ht 1/1 Running 0 1d logging-kibana-1-h4rl1 2/2 Running 0 1d   至此集中日志功能部署完成\nThe logs for the default,openshift, and openshift-infra projects are automatically aggregated and grouped into the .operations item in the Kibana interface. The project where you have deployed the EFK stack (logging, as documented here) is not aggregated into .operations and is found under its ID.\nIf you set openshift_logging_use_ops to true in your inventory file, Fluentd is configured to split logs between the main Elasticsearch cluster and another cluster reserved for operations logs, which are defined as node system logs and the projects default, openshift, and openshift-infra.\n 注意: 这里没有配置ops集群,所以基础系统项目的日志从kibana ui上是看不到的\n可能碰到的问题 log-es 集群启动会出现如下错误(通过 oc logs 查看) [2017-11-01 15:10:02,491][INFO ][container.run ] Begin Elasticsearch startup script -- | [2017-11-01 15:10:02,498][INFO ][container.run ] Comparing the specified RAM to the maximum recommended for Elasticsearch... | [2017-11-01 15:10:02,499][INFO ][container.run ] Inspecting the maximum RAM available... | [2017-11-01 15:10:02,503][INFO ][container.run ] ES_HEAP_SIZE: '4096m' | [2017-11-01 15:10:02,506][INFO ][container.run ] Setting heap dump location /elasticsearch/persistent/heapdump.hprof | [2017-11-01 15:10:02,509][INFO ][container.run ] Checking if Elasticsearch is ready on https://localhost:9200 | Exception in thread \u0026quot;main\u0026quot; java.lang.IllegalArgumentException: Unknown Discovery type [kubernetes] | at org.elasticsearch.discovery.DiscoveryModule.configure(DiscoveryModule.java:100) | at \u0026lt;\u0026lt;\u0026lt;guice\u0026gt;\u0026gt;\u0026gt; | at org.elasticsearch.node.Node.\u0026lt;init\u0026gt;(Node.java:213) | at org.elasticsearch.node.Node.\u0026lt;init\u0026gt;(Node.java:140) | at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:143) | at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:194) | at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:286) | at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:45) | Refer to the log for complete error details.  这个有3种解决方法\n 将ansible的host配置文中中 openshift_logging_image_version=v3.6.1 改为 openshift_logging_image_version=v3.6\n 修改已有的 dc(部署配置) oc edit dc/logging-es-data-master-we71py7f 将如下行\nimage: docker.io/openshift/origin-logging-elasticsearch:v3.6.1  变更成\nimage: docker.io/openshift/origin-logging-elasticsearch:v3.6  原因是请查看 https://hub.docker.com/r/openshift/origin-logging-elasticsearch/tags/ v3.6 与 v3.6.1 相比,从更新时间上来说 v3.6 属于最新版本\n 使用开源方案提供的镜像(对比官方，确定稳定版本，打tag到v3.6.1)\nopenshift_logging_image_prefix=51knowinfo/origin- openshift_logging_image_version=v3.6.1   查看es索引情况 进入容器\noc rsh logging-es-data-master-bhbc13z0-1-hv85d  查看索引(.operations.* 索引存放的是系统日志)\nsh-4.2$ curl -s --cacert /etc/elasticsearch/secret/admin-ca --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key --max-time 30 https://localhost:9200/_cat/indices?v health status index pri rep docs.count docs.deleted store.size pri.store.size green open .kibana 1 0 1 0 3.1kb 3.1kb green open .searchguard.logging-es-data-master-im3sye06 1 0 5 0 32kb 32kb green open project.cboard.0f516fc0-4870-11e8-86f9-06d28000000c.2018.04.25 1 0 6914 0 2.8mb 2.8mb green open project.cboard.0f516fc0-4870-11e8-86f9-06d28000000c.2018.04.26 1 0 1328 0 711.9kb 711.9kb green open project.logging.4abce3a2-4618-11e8-9d6b-06d28000000c.2018.04.25 1 0 7550 0 4.2mb 4.2mb green open project.logging.4abce3a2-4618-11e8-9d6b-06d28000000c.2018.04.26 1 0 1434 0 993.1kb 993.1kb green open .operations.2018.04.26 1 0 3129 0 1.5mb 1.5mb green open .kibana.a94a8fe5ccb19ba61c4c0873d391e987982fbbd3 1 0 2 0 26.2kb 26.2kb green open .operations.2018.04.25 1 0 16708 0 8.4mb 8.4mb green open .kibana.c62973cc56845b0e473e9e3c40b6e1f0a84662ef 1 0 2 0 26.2kb 26.2kb  查看系统日志 2018.04.26\nsh-4.2$ curl -s --cacert /etc/elasticsearch/secret/admin-ca --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key --max-time 30 https://localhost:9200/.operations.2018.04.26/_search  "
},
{
	"uri": "/openshift/install/harbor_install/",
	"title": "Harbor Docker仓库安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-27 创建 开源方案 初始版本    Harbor 介绍 Harbor 是 Vmwar 公司开源的 企业级的 Docker Registry 管理项目\n它主要 提供 Dcoker Registry 管理UI，可基于角色访问控制, AD/LDAP 集成，日志审核等功能，完全的支持中文。\nHarbor 的所有组件都在 Dcoker 中部署，所以 Harbor 可使用 Docker Compose 快速部署。\n部署主机角色说明    主机角色 IP地址 操作系统 摘要     主节点(Master) hz01-prod-ops-harbor-01(172.16.8.228) CentOS Linux release 7.3.1611 (Core) x86_64 harbor安装及配置   从节点(Node) hz01-prod-ops-harbor-02(172.16.8.245） CentOS Linux release 7.3.1611 (Core) X86-64 主节点数据复制    harbor 部署  开源项目地址：https://github.com/vmware/harbor 官方安装说明：https://github.com/vmware/harbor/blob/master/docs/installation_guide.md 下载安装包并解压：  [root@hz01-prod-ops-harbor-02 /opt]# wget https://storage.googleapis.com/harbor-releases/release-1.4.0/harbor-online-installer-v1.4.0.tgz [root@hz01-prod-ops-harbor-02 /opt]# tar xvf harbor-online-installer-v1.4.0.tgz   安装docker-compose  [root@hz01-prod-ops-harbor-02 /opt/harbor]# yum install docker [root@hz01-prod-ops-harbor-02 /opt/harbor]# yum install python2-pip [root@hz01-prod-ops-harbor-02 /opt/harbor]# yum install docker-compose   修改镜像源,增加docker hub 的镜像  [root@hz01-prod-ops-harbor-01 /opt/harbor]# vim /etc/sysconfig/docker { \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;http://ef017c13.m.daocloud.io\u0026quot;] }   修改harbor配置(reg.ops.com域名指向harbor所安装的服务器)  [root@hz01-prod-ops-harbor-02 /opt/harbor]# vim /opt/harbor/harbor.cfg # hostname 设置访问地址，支持IP，域名，主机名，禁止设置127.0.0.1 hostname = reg.ops.com # 访问协议，可设置 http,https ui_url_protocol = http # harbor WEB UI登陆使用的密码 harbor_admin_password = Harbor12345 # 认证方式，这里支持多种认证方式，默认是 db_auth ，既mysql数据库存储认证。 # 这里还支持 ldap 以及 本地文件存储方式。 auth_mode = db_auth # mysql root 账户的 密码 db_password = root123 self_registration= on use_compressed_js= on max_job_workers= 3 verify_remote_cert= on customize_crt= on #这些需要修改的其他的参数可以保持默认   安装harbor  [root@hz01-prod-ops-harbor-02 /opt/harbor]# cd /opt/harbor [root@hz01-prod-ops-harbor-02 /opt/harbor]# ./install.sh [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------- harbor-adminserver /harbor/start.sh Up harbor-db /usr/local/bin/docker-entr ... Up 3306/tcp harbor-jobservice /harbor/start.sh Up harbor-log /bin/sh -c /usr/local/bin/ ... Up 127.0.0.1:1514-\u0026gt;10514/tcp harbor-ui /harbor/start.sh Up nginx nginx -g daemon off; Up 0.0.0.0:443-\u0026gt;443/tcp, 0.0.0.0:4443-\u0026gt;4443/tcp, 0.0.0.0:80-\u0026gt;80/tcp registry /entrypoint.sh serve /etc/ ... Up 5000/tcp   通过终端登陆镜像仓库  [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker login reg.ops.com Username: admin Password: Error response from daemon: Get https://reg.ops.com/v1/users/: dial tcp 172.16.8.245:443: getsockopt: connection refused #这里配置的是http，docker login默认走的是https. [root@hz01-prod-ops-harbor-02 /opt/harbor]# cat /etc/docker/daemon.json { \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;http://ef017c13.m.daocloud.io\u0026quot;], \u0026quot;insecure-registries\u0026quot;: [ \u0026quot;reg.ops.com\u0026quot;] } [root@hz01-prod-ops-harbor-02 /opt/harbor]# systemctl daemon-reload [root@hz01-prod-ops-harbor-02 /opt/harbor]# systemctl restart docker [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker login hz01-prod-ops-harbor-02.sysadmin.xinguangnet.com Username: admin Password: Login Succeeded  推送镜像到harbor 登陆harbor，创建一个test测试的项目:\n#公网上随便拉个镜像 [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker pull mongo [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/mongo latest 5b1317f8158f 7 days ago 365.9 MB [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker tag mongo hz01-prod-ops-harbor-02.sysadmin.xinguangnet.com/test/mongodb:1.0 [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/mongo latest 5b1317f8158f 7 days ago 365.9 MB hz01-prod-ops-harbor-02.sysadmin.xinguangnet.com/test/mongodb 1.0 5b1317f8158f 7 days ago 365.9 MB [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker push hz01-prod-ops-harbor-02.sysadmin.xinguangnet.com/test/mongodb:1.0 The push refers to a repository [hz01-prod-ops-harbor-02.sysadmin.xinguangnet.com/test/mongodb] 99099bc0f52d: Pushed 5388bfbc2c01: Pushed d6ac487f7716: Pushed 2ecbdcef31f1: Pushed 4786aaf122f1: Pushed b597eb624250: Pushed d1a481118c6e: Pushed 217a81d3bde9: Pushed 54e8db6ab32d: Pushed 43efe85a991c: Pushed 1.0: digest: sha256:82fb1f2483179a7c26ac603d5ad0f9cf6992a27f272c82e277371a96657b799b size: 2407  配置docker镜像复制  登陆master节点的web ui  根据上文创建一个openshift的项目，这里不做演示了。\n 选择仓库管理，创建从节点的信息   填写node节点的信息，并测试连接   连接成功后，仓库管理会生成一条信息   点击复制管理，添加一条复制策略   新建复制规则，主要是复制源项目，目标节点，触发模式，之后选择保存   复制管理会生成一条oepnshift复制的规则   测试镜像复制策略是否生效  #推送一个镜像到openshift项目 [root@hz01-prod-ops-harbor-01 /root]# docker tag docker.io/mongo 172.16.8.228/openshift/mongodb:1.0 [root@hz01-prod-ops-harbor-01 /root]# docker push 172.16.8.228/openshift/mongodb:1.0 The push refers to a repository [172.16.8.228/openshift/mongodb] 99099bc0f52d: Pushed 5388bfbc2c01: Pushed d6ac487f7716: Pushed 2ecbdcef31f1: Pushed 4786aaf122f1: Pushed b597eb624250: Pushed d1a481118c6e: Pushed 217a81d3bde9: Pushed 54e8db6ab32d: Pushed 43efe85a991c: Pushed 1.0: digest: sha256:82fb1f2483179a7c26ac603d5ad0f9cf6992a27f272c82e277371a96657b799b size: 2407   在主节点web ui查看，生成了一条复制任务！   在从节点web ui查看，已经从主节点把镜像复制过来了  数据库备份 #根据文件定义数据文件放在/data/database/目录下 [root@hz01-prod-ops-harbor-01 /opt/harbor]# vim docker-compose.yml mysql: image: vmware/harbor-db:v1.4.0 container_name: harbor-db restart: always volumes: - /data/database:/var/lib/mysql:z [root@hz01-prod-ops-harbor-01 /data/database]# ls /data/database/ aria_log.00000001 aria_log_control created_in_mariadb.flag ib_buffer_pool ibdata1 ib_logfile0 ib_logfile1 ibtmp1 multi-master.info mysql performance_schema registry tc.log  "
},
{
	"uri": "/openshift/application/s2i_custom/",
	"title": "S2I Build 镜像制作",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-05-02 创建 开源方案 初始版本    S2I(Source-to-Image) 说明  开发人员完成代码开发 S2I过程负责将代码和运行环境(比如 python 或者 Java)整合构建,生成应用docker镜像 应用镜像构建完成并推送至openshift集群内部的docker镜像仓库,再由openshift进行各个环境的部署  准备环境  在Master上下载S2I的二进制执行文件。\n# cd /opt # wget -c https://github.com/openshift/source-to-image/releases/download/v1.1.7/source-to-image-v1.1.7-226afa1-linux-amd64.tar.gz  解压到/usr/bin目录下\n# tar zxvf source-to-image-v1.1.7-226afa1-linux-amd64.tar.gz -C /usr/bin   创建项目目录  通过s2i create命令创建一个名为tomcat-s2i的S2I Builder镜像。 第二个参数ops-tomcat-s2i为S2I Builder镜像名称。第三个参数tomcat-s2i-catalog定义了工作目录的名称。\n# s2i create ops-tomcat-s2i tomcat-s2i-catalog  执行find tomcat-s2i-catalog查看目录。\n[root@openshift-master S2I]# find tomcat-s2i-catalog tomcat-s2i-catalog tomcat-s2i-catalog/s2i tomcat-s2i-catalog/s2i/bin tomcat-s2i-catalog/s2i/bin/assemble tomcat-s2i-catalog/s2i/bin/run tomcat-s2i-catalog/s2i/bin/usage tomcat-s2i-catalog/s2i/bin/save-artifacts tomcat-s2i-catalog/Dockerfile tomcat-s2i-catalog/README.md tomcat-s2i-catalog/test tomcat-s2i-catalog/test/test-app tomcat-s2i-catalog/test/test-app/index.html tomcat-s2i-catalog/test/run tomcat-s2i-catalog/Makefile  s2i目录下为S2I脚本\n     脚本名称 功能作用     assemble 负责源代码的编译、构建以及构建产出物的部署   run S2I流程生成的最终镜像将以这个脚本作为容器的启动命令   usage 打印帮助信息，一般作为S2I Builder镜像的启动命令   save-artifacts 为了实现增量构建，在构建过程中会执行此脚本保存中间构建产物。此脚本并不是必需的    开始项目工作 编写Dockerfile 编写一个制作Tomcat的S2I镜像。Dockerfile的内容如下：\n# ops-tomcat-s2i FROM openshift/base-centos7 # TODO: Put the maintainer name in the image metadata MAINTAINER fuhua \u0026lt;fuhua@xinguangnet.com\u0026gt; # TODO: Rename the builder environment variable to inform users about application you provide them ENV BUILDER_VERSION 1.0 # TODO: Set labels used in OpenShift to describe the builder image LABEL io.k8s.description=\u0026quot;Platform for building tomcat\u0026quot; \\ io.k8s.display-name=\u0026quot;builder tomcat\u0026quot; \\ io.openshift.expose-services=\u0026quot;8080:http\u0026quot; \\ io.openshift.tags=\u0026quot;builder,tomcat,java,openjdk7,etc.\u0026quot; # Setting local yum repo #RUN rm -fr /etc/yum.repos.d/ #ADD ./local-mirror.repo /etc/yum.repos.d/ # TODO: Install required packages here: RUN yum install -y java-1.7.0-openjdk git wget\u0026amp;\u0026amp; yum clean all -y # TODO (optional): Copy the builder files into /opt/app-root # COPY ./\u0026lt;builder_folder\u0026gt;/ /opt/app-root/ # Dwonload And install Tomcat7 RUN mkdir -p /opt/app-root/ \u0026amp;\u0026amp; cd /opt/app-root/ \u0026amp;\u0026amp; wget http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-7/v7.0.82/bin/apache-tomcat-7.0.82.tar.gz RUN cd /opt/app-root/ \u0026amp;\u0026amp; tar -xzvf apache-tomcat-7.0.82.tar.gz RUN cd /opt/app-root/ \u0026amp;\u0026amp; rm -fr apache-tomcat-7.0.82.tar.gz \u0026amp;\u0026amp; ln -s apache-tomcat-7.0.82 tomcat7 RUN cd /opt/app-root/ \u0026amp;\u0026amp; rm -fr tomcat7/webapps/* # Download and install maven RUN mkdir -p /opt/app-root/ \u0026amp;\u0026amp; cd /opt/app-root/ \u0026amp;\u0026amp; wget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.5.2/binaries/apache-maven-3.5.2-bin.tar.gz RUN cd /opt/app-root/ \u0026amp;\u0026amp; tar -xzvf apache-maven-3.5.2-bin.tar.gz RUN cd /opt/app-root/ \u0026amp;\u0026amp; rm -fr apache-maven-3.5.2-bin.tar.gz \u0026amp;\u0026amp; ln -s apache-maven-3.5.2 maven RUN ln -s /opt/app-root/maven/bin/mvn /usr/bin/mvn # TODO: Copy the S2I scripts to /usr/libexec/s2i, since openshift/base-centos7 image # sets io.openshift.s2i.scripts-url label that way, or update that label COPY ./s2i/bin/ /usr/libexec/s2i # TODO: Drop the root user and make the content of /opt/app-root owned by user 1001 # RUN chown -R 1001:1001 /opt/app-root RUN chown -R 1001:0 /opt/app-root \u0026amp;\u0026amp; chown -R 1001:0 $HOME \u0026amp;\u0026amp; \\ chmod -R ug+rw /opt/app-root # This default user is created in the openshift/base-centos7 image USER 1001 # TODO: Set the default port for applications built using this image EXPOSE 8080 # TODO: Set the default CMD for the image CMD [\u0026quot;/usr/libexec/s2i/usage\u0026quot;]  注意: 通过USER 1001定义了一个新用户，并指定该用户为容器的启动用户。以root用户作为启动用户在某些情况下存在安全风险\n编辑s2i/bin/assemble脚本（负责源代码的编译、构建以及构建产出物的部署）。 在脚本最末尾添加如下代码：\nmvn -Dmaven.test.skip=true package find . -type f -name '*.war'|xargs -i cp {} /opt/app-root/tomcat7/webapps/ROOT.war mvn clean  这段代码会触发一次Maven构建，并将构建产生的WAR包拷贝到Tomcat服务器的webapps目录下进行部署。 完整的assemble脚本如下：\n[root@openshift-master s2i]# cat bin/assemble #!/bin/bash -e # # S2I assemble script for the 'ops-tomcat-s2i' image. # The 'assemble' script builds your application source so that it is ready to run. # # For more information refer to the documentation: # https://github.com/openshift/source-to-image/blob/master/docs/builder_image.md # # If the 'ops-tomcat-s2i' assemble script is executed with the '-h' flag, print the usage. if [[ \u0026quot;$1\u0026quot; == \u0026quot;-h\u0026quot; ]]; then exec /usr/libexec/s2i/usage fi # Restore artifacts from the previous build (if they exist). # if [ \u0026quot;$(ls /tmp/artifacts/ 2\u0026gt;/dev/null)\u0026quot; ]; then echo \u0026quot;---\u0026gt; Restoring build artifacts...\u0026quot; mv /tmp/artifacts/. ./ fi echo \u0026quot;---\u0026gt; Installing application source...\u0026quot; cp -Rf /tmp/src/. ./ echo \u0026quot;---\u0026gt; Building application from source...\u0026quot; # TODO: Add build steps for your application, eg npm install, bundle install, pip install, etc. mvn -Dmaven.test.skip=true package find . -type f -name '*.war'|xargs -i cp {} /opt/app-root/tomcat7/webapps/ROOT.war mvn clean  编辑s2i/bin/run脚本（S2I流程生成的最终镜像将以这个脚本作为容器的启动命令）。 替换为以下内容：\nexec /opt/app-root/tomcat7/bin/catalina.sh run  执行镜像构建 [root@openshift-master tomcat-s2i-catalog]# make docker build -t ops-tomcat-s2i . Sending build context to Docker daemon 23.04 kB Step 1 : FROM openshift/base-centos7 ---\u0026gt; 4842f0bd3d61 Step 2 : MAINTAINER fuhua \u0026lt;fuhua@xinguangnet.com\u0026gt; ---\u0026gt; Using cache ---\u0026gt; bfc335ce78e5 Step 3 : ENV BUILDER_VERSION 1.0 ---\u0026gt; Using cache ---\u0026gt; ecc162dc5cd5 Step 4 : LABEL io.k8s.description \u0026quot;Platform for building tomcat\u0026quot; io.k8s.display-name \u0026quot;builder tomcat\u0026quot; io.openshift.expose-services \u0026quot;8080:http\u0026quot; io.openshift.tags \u0026quot;builder,tomcat,java,openjdk7,etc.\u0026quot; ---\u0026gt; Using cache ---\u0026gt; cfa61aa14bb4 Step 5 : RUN yum install -y java-1.7.0-openjdk git wget\u0026amp;\u0026amp; yum clean all -y ---\u0026gt; Using cache ---\u0026gt; baf7452d6f3d Step 6 : RUN mkdir -p /opt/app-root/ \u0026amp;\u0026amp; cd /opt/app-root/ \u0026amp;\u0026amp; wget http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-7/v7.0.82/bin/apache-tomcat-7.0.82.tar.gz ---\u0026gt; Using cache ---\u0026gt; bda3da428f9f Step 7 : RUN cd /opt/app-root/ \u0026amp;\u0026amp; tar -xzvf apache-tomcat-7.0.82.tar.gz ---\u0026gt; Using cache ---\u0026gt; 8b9288b65eb6 Step 8 : RUN cd /opt/app-root/ \u0026amp;\u0026amp; rm -fr apache-tomcat-7.0.82.tar.gz \u0026amp;\u0026amp; ln -s apache-tomcat-7.0.82 tomcat7 ---\u0026gt; Using cache ---\u0026gt; 95e3f4fc9f01 Step 9 : RUN cd /opt/app-root/ \u0026amp;\u0026amp; rm -fr tomcat7/webapps/* ---\u0026gt; Using cache ---\u0026gt; 6977f7726e29 Step 10 : RUN mkdir -p /opt/app-root/ \u0026amp;\u0026amp; cd /opt/app-root/ \u0026amp;\u0026amp; wget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.5.2/binaries/apache-maven-3.5.2-bin.tar.gz ---\u0026gt; Using cache ---\u0026gt; b630e55722e8 Step 11 : RUN cd /opt/app-root/ \u0026amp;\u0026amp; tar -xzvf apache-maven-3.5.2-bin.tar.gz ---\u0026gt; Using cache ---\u0026gt; 048259dcf249 Step 12 : RUN cd /opt/app-root/ \u0026amp;\u0026amp; rm -fr apache-maven-3.5.2-bin.tar.gz \u0026amp;\u0026amp; ln -s apache-maven-3.5.2 maven ---\u0026gt; Using cache ---\u0026gt; 4b55059b8437 Step 13 : RUN ln -s /opt/app-root/maven/bin/mvn /usr/bin/mvn ---\u0026gt; Using cache ---\u0026gt; 6981f2ef18e0 Step 14 : COPY ./s2i/bin/ /usr/libexec/s2i ---\u0026gt; Using cache ---\u0026gt; ce773a99ab69 Step 15 : RUN groupadd -g 1002 tomcat \u0026amp;\u0026amp; useradd -ms /bin/bash -u 1002 -g 1002 tomcat \u0026amp;\u0026amp; chown -R 1002:1002 /opt/app-root \u0026amp;\u0026amp; chmod -R a+w /opt/app-root ---\u0026gt; Using cache ---\u0026gt; 1141afc04321 Step 16 : USER 1002 ---\u0026gt; Using cache ---\u0026gt; 704361ace73c Step 17 : EXPOSE 8080 ---\u0026gt; Using cache ---\u0026gt; 2530fdc4612f Step 18 : CMD /usr/libexec/s2i/usage ---\u0026gt; Using cache ---\u0026gt; 884edf911bcc Successfully built 884edf911bcc  导入镜像(reg.ops.com)  需要将 reg.ops.com 加入到 非安全连接列表中\n[root@openshift-master tomcat-s2i-catalog]# cat /etc/docker/daemon.json { \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;http://ef017c13.m.daocloud.io\u0026quot;], \u0026quot;insecure-registries\u0026quot;: [\u0026quot;reg.ops.com\u0026quot;] }  测试build镜像(这个镜像是jdk1.7)\ns2i build https://github.com/nichochen/mybank-demo-maven.git ops-tomcat-s2i ops-tomcat-s2i-app  使用新构建出的应用镜像启动\ndocker run -it -p 8080:8080 ops-tomcat-s2i-app  正常应该能看到tomcat的启动日志\n 推送到自己仓库 这里推送到 reg.ops.com 的registry仓库\n[root@openshift-master tomcat-s2i-catalog]# docker tag ops-tomcat-s2i:latest reg.ops.com/opssolution/ops-tomcat-s2i:latest [root@openshift-master tomcat-s2i-catalog]# docker push reg.ops.com/opssolution/ops-tomcat-s2i:latest The push refers to a repository [reg.ops.com/opssolution/ops-tomcat-s2i] b9b8ffca1e5a: Pushed be79645180dc: Pushed 3f84a2b90659: Pushed 04b90df78611: Pushed 0c56c93ac2a7: Pushed 50596ec3b0e4: Pushed bdac80fbd115: Pushed 7842c35bf593: Pushed fa84fac3a7ce: Pushed 4cec9f30e5a2: Pushed 10811061f79a: Pushed cb96aea742c3: Pushed f1bbaf33b49c: Pushed 4b1e8db0189a: Pushed 34e7b85d83e4: Pushed latest: digest: sha256:097d24d53e4f5abc1fe25dfba4914603c3b6027e01a5ad51e4eb6e69bbdcd22d size: 3461   注意: 需要在harbor 镜像仓库中先创建 opssolution 项目\n如果用自己的reg.ops.com的镜像仓库 需要修改master配置文件\nimagePolicyConfig: allowedRegistriesForImport: - domainName: docker.io - domainName: '*.docker.io' - domainName: '*.redhat.com' - domainName: gcr.io - domainName: quay.io - domainName: '*.amazonaws.com' disableScheduledImport: false maxImagesBulkImportedPerRepository: 5 maxScheduledImageImportsPerMinute: 60 scheduledImageImportMinimumIntervalSeconds: 900  在这里把 reg.ops.com 加入到上述配置中,这样才能导入镜像 在这里为了方便 镜像也推送到了 https://hub.docker.com/r/51knowinfo/ops-tomcat7-jdk7-s2i\n编写S2I Builder镜像模板 在此下载模板文件 tomcat-imagestream.json\n模板文件中主要注意annotations中的内容\n{ \u0026quot;name\u0026quot;: \u0026quot;Tomcat 7\u0026quot;, \u0026quot;annotations\u0026quot;: { \u0026quot;openshift.io/display-name\u0026quot;: \u0026quot;Tomcat7 (Latest)\u0026quot;, \u0026quot;openshift.io/provider-display-name\u0026quot;: \u0026quot;OpsSolution, Inc.\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;在Centos7上编译和运行tomcat应用. 更多关于此构建镜像(S2I)信息,请参考 https://github.com/openshift-s2i/s2i-wildfly/blob/master/README.md.\\n\\n警告: 使用此版本,你的应用将使用最新版本的tomca7, 包含主要版本更新\u0026quot;, \u0026quot;iconClass\u0026quot;: \u0026quot;icon-tomcat\u0026quot;, \u0026quot;tags\u0026quot;: \u0026quot;builder,tomcat,java,51know\u0026quot;, \u0026quot;supports\u0026quot;:\u0026quot;jee,java\u0026quot;, \u0026quot;sampleRepo\u0026quot;: \u0026quot;https://github.com/nichochen/mybank-demo-maven.git\u0026quot; }, \u0026quot;from\u0026quot;: { \u0026quot;kind\u0026quot;: \u0026quot;DockerImage\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;51knowinfo/ops-tomcat7-jdk7-s2i:latest\u0026quot; } }  注意: tags 标签项要特别注意，用于catalog 分类用\n查看刚刚导入的镜像 [root@openshift-master ~]# oc get is -n openshift | grep ops-tomcat-s2i ops-tomcat-s2i 172.30.188.209:5000/openshift/ops-tomcat-s2i latest 2 minutes ago  登录web console,使用新的s2i builder 镜像 直到构建镜像成功，会自动部署pod\n删除默认router,创建新的router, app.ops.com(指向router所运行计算节点的ip),访问效果如下 至此一个自定义版本的Tomcat S2I 就完成了\n"
},
{
	"uri": "/openshift/install/",
	"title": "集群安装",
	"tags": [],
	"description": "",
	"content": " 第一章 生产集群安装 本章主要讲述 Openshift Origin 安装配置步骤\n"
},
{
	"uri": "/openshift/application/deploy_app_cboard/",
	"title": "手工部署应用实战",
	"tags": [],
	"description": "openshift origin 部署应用(涉及到应用改造),并形成应用模板,后续可以一键部署应用",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-27 创建 开源方案 初始版本    Cboard应用拓扑图 标准的三层应用 前端 + 后端(Redis Cache) + 数据库\ncboard 是易用的,自服务的开源BI dashboard 应用 详细信息请参考 https://github.com/TuiQiao/CBoard\n后面操作基本都是 使用Openshift web console 进行应用部署\n首先部署mariadb数据库 这里使用持久化的数据库,防止数据丢失 注意红色箭头 一个是设定默认创建的的数据库(cboard) 一个是设定持久卷大小(10Gi) 部署完成后,服务正常 红色箭头指向的service name mariadb,在集群内部可以使用此名称 访问数据库\n其次部署redis服务 参照部署mariadb方式，通过catalog中 redis持久化模板创建应用即可\n部署cboard应用之学习了解 部署cboard 前面将cboard应用依赖的服务已经部署完成,下面将通过Source-to-Image builder来部署cboard应用\n这里我们使用 S2I Build 镜像制作 tomcat版本的S2I BUILDER来构建部署cboard应用\n这里分支我们指向的是v0.4.1,根据网速快慢,过一定时间就会build完成\n虽然cboard的pod已经正常运行了，但是数据库的初始化,数据库连接配置 都没有修改 都是默认的,还需要后面的工作才能让应用程序可用\n 进入cboard 容器学习  登陆容器(openshift master节点)\n# oc project Using project \u0026quot;my-project\u0026quot; on server \u0026quot;https://192.168.124.22:8443\u0026quot;. # oc get pod NAME READY STATUS RESTARTS AGE cboard-1-m4j6b 1/1 Running 0 2h cboard-2-build 0/1 Completed 0 2h mariadb-1-kz485 1/1 Running 1 1d redis-1-kcks5 1/1 Running 0 5h [root@openshift-master ~]# oc rsh cboard-1-m4j6b sh-4.2$  观察容器中的环境变量\nsh-4.2$ set |grep \u0026quot;REDIS\u0026quot; REDIS_PORT=tcp://172.30.71.182:6379 REDIS_PORT_6379_TCP=tcp://172.30.71.182:6379 REDIS_PORT_6379_TCP_ADDR=172.30.71.182 REDIS_PORT_6379_TCP_PORT=6379 REDIS_PORT_6379_TCP_PROTO=tcp REDIS_SERVICE_HOST=172.30.71.182 REDIS_SERVICE_PORT=6379 REDIS_SERVICE_PORT_REDIS=6379 sh-4.2$ set |grep \u0026quot;MARIADB\u0026quot; MARIADB_PORT=tcp://172.30.194.134:3306 MARIADB_PORT_3306_TCP=tcp://172.30.194.134:3306 MARIADB_PORT_3306_TCP_ADDR=172.30.194.134 MARIADB_PORT_3306_TCP_PORT=3306 MARIADB_PORT_3306_TCP_PROTO=tcp MARIADB_SERVICE_HOST=172.30.194.134 MARIADB_SERVICE_PORT=3306 MARIADB_SERVICE_PORT_MARIADB=3306 sh-4.2$   注意: cboard 依赖的服务 mariadb 和 redis 服务信息都以环境变量的方式注入到容器中了(同一个项目下的容器服务信息是通过这种方式共享的)\n  了解数据库相关信息  首先我们要找到数据库随机生成的用户和密码信息\n从上图知道,数据库用户密码相关信息通过Deployments(部署配置)加入环境变量的方式,注入到docker中,但是密码信息比较敏感,所以是以secrets方式保存这些信息\n改造Cboard应用(完成数据库自动初始化)重点 首先我们要让cboard pod中能看到maraiadb的数据库和用户名密码信息(通过注入secrets方式) 参考 maraiadb的部署配置,来设置cboard的部署配置,保存后,会重新部署cboard\n注意: 同一项目中(namespace相同),configmap 或者 secret 是可以通过环境变量共享注入的\n 再次登入cboard容器 # oc rsh cboard-2-3jgt2 sh-4.2$ set | grep mysql -i MYSQL_DATABASE=sampledb MYSQL_PASSWORD=cBpiDI4xs5yeM0Mt MYSQL_USER=userG54 sh-4.2$  可以看到环境变量中已经能够看到数据库相关信息了\n接下来 实现 自动导入数据库  需要在码云(git.oschina.net)建立个项目,从github(https://github.com/TuiQiao/CBoard.git)同步 基于码云项目,以v0.4.1 tag 创建一个新分支 ops-tagV0.4.1 基于新分支,添加flyway进行数据库版本管理,请参考 https://gitee.com/hunterfu/CBoard.git  增加完成后的内容目录如下\n# git branch master * ops-tagV0.4.1 # ll total 64 -rw-r--r-- 1 root root 0 Apr 18 04:06 changelog -rw-r--r-- 1 root root 1503 Apr 18 04:06 Dockerfile drwxr-xr-x 7 root root 168 Apr 18 04:06 flyway-5.0.7 # 新增加的目录 drwxr-xr-x 2 root root 217 Apr 18 04:06 lib -rw-r--r-- 1 root root 21013 Apr 18 04:06 LICENSE.txt -rwxr-xr-x 1 root root 189 May 8 05:01 migrate-database.sh # 增加的脚本 -rwxr-xr-x 1 root root 16748 Apr 18 04:06 pom.xml -rw-r--r-- 1 root root 12190 Apr 18 04:06 README.md drwxr-xr-x 5 root root 50 Apr 18 04:06 sql drwxr-xr-x 4 root root 30 Apr 18 04:06 src   将Cboard初始化数据库文件拷贝到flyway中(项目根目录)  # cp sql/mysql/mysql.sql flyway-5.0.7/sql/V1__init.sql   编写 migrate-database.sh 脚本,主要是利用环境变量来初始化数据库  # cat migrate-database.sh #!/bin/bash set -e /bin/sh flyway-5.0.7/flyway -user=${MYSQL_USER} -password=${MYSQL_PASSWORD} -url=jdbc:mysql://${MARIADB_SERVICE_HOST}:${MARIADB_SERVICE_PORT}/${MYSQL_DATABASE} migrate   修改 应用的的数据库连接配置文件,同理利用环境变量  # cat src/main/resources/config.properties validationQuery=SELECT 1 jdbc_url=jdbc:mysql://${MARIADB_SERVICE_HOST}:${MARIADB_SERVICE_PORT}/${MYSQL_DATABASE} jdbc_username=${MYSQL_USER} jdbc_password=${MYSQL_PASSWORD} #jdbc_url=jdbc:sqlserver://192.168.86.156:1433;databaseName=CBoard_Test #jdbc_username=uapp_cboard #jdbc_password=uapp_cboard # Service configuration dataprovider.resultLimit=1000000 admin_user_id=1 phantomjs_path=D:/phantomjs-2.1.1-windows/bin/phantomjs.exe mail.smtp.host=127.0.0.1 mail.smtp.port=8825 mail.smtp.from=test@test.com #mail.smtp.username=test@test.com #mail.smtp.password=111111 #mail.smtp.ssl.checkserveridentity=false # Cache Properties cache.redis.hostName=redis cache.redis.port=6379 org.quartz.threadPool.threadCount=10 # Storage File Syatem # 1 Stores data in file system aggregator.h2.url=jdbc:h2:~/H2Data/cboard;AUTO_SERVER=TRUE;LOG=0;UNDO_LOG=0 # 2 Stores data outside of the VM's heap - useful for large memory DBs without incurring GC costs. #aggregator.h2.url=jdbc:h2:nioMemFS:cboard;LOG=0;UNDO_LOG=0 aggregator.h2.database.name=cboard aggregator.h2.cleanjob.quarz=0 1 0 * * ? log.negativeFilter=List\\\\.do log.positveFilter=  修改完成后,提交代码,修改cboard build配置文件 如下图,分支指向新的分支\n然后手工触发重新build\n 重新部署完成后,再次进入容器,初始化数据库  # 默认当前目录就是源码目录 $ oc rsh cboard-3-7jhfn sh-4.2$ pwd /opt/app-root/src sh-4.2$ ls Dockerfile README.md flyway-5.0.7 migrate-database.sh sql LICENSE.txt changelog lib pom.xml src sh-4.2$ sh migrate-database.sh Flyway Community Edition 5.0.7 by Boxfuse Database: jdbc:mysql://172.30.194.134:3306/sampledb (MySQL 10.1) Successfully validated 1 migration (execution time 00:00.024s) Creating Schema History table: `sampledb`.`flyway_schema_history` Current version of schema `sampledb`: \u0026lt;\u0026lt; Empty Schema \u0026gt;\u0026gt; Migrating schema `sampledb` to version 1 - init Successfully applied 1 migration to schema `sampledb` (execution time 00:08.660s)  然后重新部署cboard就可以链接数据库了\n 为了能访问到web界面，需要建一个route暴露到集群外(默认route只能集群内部访问)  通过浏览器访问 http://app.ops.ops(此域名指向route计算节点IP) 用户名/密码: admin/root123\n通过部署配置HOOK实现自动导入数据库(类似其他初始化工作也是可以的) 上述工作中，我们还是手工进入容器,执行了数据库初始化脚本,我们需要用到一个机制,在部署时自动执行脚本\n在部署时自动执行脚本,需要设置 cboard DeploymentConfig 加入hook(钩子)\n上图操作后,保存配置,在部署cboard应用时会首先执行 migrate-database.sh 脚本命令,策略是retry 来达到自动导入数据任务\nmigrate-database.sh 这个脚本其实可以写的很高端,比如你可以根据环境变量来决定初始化那些数据,比如导入演示数据等,完全看想象力\n总结 通过本实例学习,了解了pod之间数据共享机制,对后续构建微服务,网格化服务等非常有帮\n下一篇文档,我们会将此套应用方案,做成模板,实现一键部署\n参考资料 https://docs.openshift.org/3.6/dev_guide/deployments/deployment_strategies.html#lifecycle-hooks\n"
},
{
	"uri": "/openshift/application/create_template/",
	"title": "创建应用模板",
	"tags": [],
	"description": "openshift中根据项目中部署的应用生成应用模板,做到后续部署一键化",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-05-09 创建 开源方案 初始版本    What is a template The official OpenShift 3 documentation states:\n A template describes a set of objects that can be parameterized and processed to produce a list of objects for creation by OpenShift. The objects to create can include anything that users have permission to create within a project, for example services, build configurations, and deployment configurations. A template may also define a set of labels to apply to every object defined in the template.\n This means that typically in a template we will have:\n A set of resources that will be created as part of “creating/deploying” the template A set of values for the parameters defined in the template A set of labels to describe the generated resources A template will be defined in JSON or YAML format, and will be loaded into OpenShift for user instantiation, also known as application creation.  The templates can have global visibility scope (visible for every OpenShift project) or project visibility scope (visible only for a specific project).\nBenefits of using templates A template provides developers with an easy way to create all the necessary OpenShift resources for their application to work. This allows a developer to quickly deploy an application without having to understand all of the internals of the OpenShift 3 platform.\n As a PaaS provider you have better control on what is being created and can make better usage of your resources. As a PaaS provider you can define different Service Level Agreements in templates, defining the amount of host resources (cpu, memory) each and every container can consume.  Predefined templates, or deploy your application on OpenShift Typically, the PaaS provider will provide users with a set of predefined templates that will cover all of the usages or typologies/topologies of applications that can be deployed on OpenShift.\nThe set of predefined templates will be accessible through the CLI or through the Web console.\nWhen creating your application using one of these templates, the user will typically provide the template with the source for the code of the application and some other configuration items such as the application name, database credentials, etc.\nCustom templates, or OpenShiftify your application Another use case is when you have a typology/topology of an application that does not fit into the provided templates and you want to create a template to model it. This will be the topic for the next article, a walkthrough on how to create a template for your application.\n从项目中创建模板 // 导出所有 services到模板 $ oc export service --all --as-template=my_template // 导出 标签是 name=test 的 services 和 deployment configurations 到模板 oc export svc,dc -l name=test --as-template=my_template  记住 上述命令会把模板内容输出到标准输出(stdout), 如果我们需要导出到文件,可以使用重定向到文件，也可以格式化输出 JSON 或者 YAML\n$ oc export -o json --as-template=my_template \u0026gt; my_template.json $ oc export -o yaml --as-template=my_template \u0026gt; my_template.yaml $ oc export dc,svc,routes -o json --as-template=cboard_template \u0026gt; cboard_template.json  请记住,导出只是制作模板的第一步,后续还需要修改编辑参数配置,以使应用可配置化\n重要的事情 最后,创建应用模板的时候,一些重要的事情您需要记住\n Design your template visually, as it helps understand the required components. Provide meaningful names to resources and use labels to describe your resources (labels are used as selectors for some resources). Templates can be shared or per-project, and common templates are in the openshift namespace/project. Currently there is no ability to set a Readme on templates, so be as verbose and complete in the template’s description. Once the resources in a template are processed and deployed, they can be modified with the CLI. You should constrain the CPU and memory a container in a pod can use. When the resources in a template are created, if there is a BuildConfiguration defined, it will only start an automated build if there is an ImageChange trigger defined. This will change in the next release and we will be able to launch a build on resource creation. Parameterize everything a user of your template might want to customize so they can control the behavior of the template when they instantiate it.  我们参考 cakephp-mysql-persistent 进行创建 参考官方示例模板 cakephp-mysql-persistent\ncboard模板请参考 https://gitee.com/hunterfu/CBoard/blob/ops-tagV0.4.1/cboard-mysql-redis-persistent.json\n参考资料\nhttps://blog.openshift.com/part-1-from-app-to-openshift-runtimes-and-templates/ https://blog.openshift.com/part-2-creating-a-template-a-technical-walkthrough/\n"
},
{
	"uri": "/openshift/install/ceph_install/",
	"title": "单容器版CEPH集群",
	"tags": [],
	"description": "介绍单容器版本的ceph集群搭建，用于系统演示",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-04 创建 开源方案 初始版本    这里我们搭建一个单容器版本的CEPH集群 注意: 这里搭建的ceph环境不能用于任何生产环境，但是可以快速运行ceph 系统\nceph 架构 Environment 我们使用的环境是 centos 7 x86-64 架构\nDocker: [root@open-ceph ~]# docker --version Docker version 1.13.1, build 774336d/1.13.1  Ceph: 开始安装 ceph-common:\n# yum install centos-release-ceph-jewel.noarch -y # yum install -y ceph-common ceph  SELinux and Security: 为了简化配置，这里环境是关闭 selinux 和 firewalld\n[root@open-ceph ~]# getenforce Disabled [root@open-ceph ~]# systemctl status firewalld (已经卸载) Unit firewalld.service could not be found.  运行docker版本镜像\ndocker pull ceph/demo:tag-build-stable-2.2.11-jewel-centos-7 docker run -d --net=host -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph \\ -e MON_IP=192.168.124.31 -e CEPH_PUBLIC_NETWORK=192.168.124.0/24 -e MON_NAME=mon01 ceph/demo:tag-build-stable-2.2.11-jewel-centos-7 [root@open-ceph ceph]# ps -e | grep ceph 19622 ? 00:00:00 ceph-mon 19661 ? 00:00:01 ceph-osd 19665 ? 00:00:00 ceph-mds  192.168.124.31 是docker所在运行计算节点IP地址,如果出现问题(docker logs 查看),可以 rm -rf /etc/ceph/* 和 rm -rf /var/lib/ceph/* 删除配置后,重新再试.\n在运行节点上，创建RBD IMAGE 并且映射到 /dev/rbdX 设备文件:\n# 确认模块加载 [root@open-ceph ceph]# modprobe rbd [root@open-ceph ceph]# lsmod | grep rbd rbd 83938 0 libceph 287066 1 rbd # 创建镜像文件,名字为 \u0026quot;demo\u0026quot; 大小1GB $ rbd create demo -s 1024 $ rbd map demo # 出现类似错误 rbd: sysfs write failed RBD image feature set mismatch. You can disable features unsupported by the kernel with \u0026quot;rbd feature disable\u0026quot;. In some cases useful info is found in syslog - try \u0026quot;dmesg | tail\u0026quot; or so. rbd: map failed: (6) No such device or address # ceph新版中在map image时，给image默认加上了许多feature，通过rbd info可以查看到： [root@open-ceph ceph]# rbd info demo rbd image 'demo': size 1024 MB in 256 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.5e2874b0dc51 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten flags: # 可以看到foo image拥有： layering, exclusive-lock, object-map, fast-diff, deep-flatten。 # 不过遗憾的是CentOS的3.10内核仅支持其中的layering feature，其他feature概不支持。我们需要手动disable这些features： [root@open-ceph ceph]# uname -r 3.10.0-514.el7.x86_64 [root@open-ceph ceph]# rbd feature disable demo exclusive-lock, object-map, fast-diff, deep-flatten [root@open-ceph ceph]# rbd map demo /dev/rbd0 [root@open-ceph ceph]# rbd showmapped id pool image snap device 0 rbd demo - /dev/rbd0 # 格式化文件系统 $ mkfs.ext4 /dev/rbd0 [root@open-ceph ceph]# rbd --image demo -p rbd info rbd image 'demo': size 1024 MB in 256 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.5e2874b0dc51 format: 2 features: layering flags: # 后面就可以挂在此设备了  Ceph Secret: ceph-rbd 存储插件使用ceph secret 来做验证. 编码密钥是在ceph monitor机器上生成:\n# 在 ceph monitor host (这里是运行ceph 容器的节点主机上): $ ceph auth get-key client.admin AQDva7JVEuVJBBAAc8e1ZBWhqUB9K/zNZdOHoQ== #copy the above value and paste it in the echo command below: $ echo \u0026quot;AQDva7JVEuVJBBAAc8e1ZBWhqUB9K/zNZdOHoQ==\u0026quot;| base64 QVFEdmE3SlZFdVZKQkJBQWM4ZTFaQldocVVCOUsvek5aZE9Ib1E9PQo= # copy the above base64 output  要挂在ceph rbd的节点测试 端口6789必须能被访问\n# nc 测试 $ nc 192.168.124.31 6789 \u0026lt;/dev/null $ echo $? #返回值必须是 0 #or $ telnet 192.168.124.31 6789 # error if refused, else CTRL-C to exit telnet  更多信息 请参考ceph 文档\n"
},
{
	"uri": "/openshift/application/",
	"title": "应用部署",
	"tags": [],
	"description": "",
	"content": " 第二章 应用部署 本章主要讲述 openshift origin 平台上应用部署相关配置实践\n"
},
{
	"uri": "/openshift/monitor/",
	"title": "平台监控",
	"tags": [],
	"description": "",
	"content": " 第三章 平台监控 本章主要讲述 openshift origin 集群监控相关配置实践\n"
},
{
	"uri": "/openshift/appcenter/",
	"title": "组件中心",
	"tags": [],
	"description": "",
	"content": " 第四章 组件中心 本章主要讲述如何构建自己企业内部的组件或者应用中心的相关配置实践\n"
},
{
	"uri": "/openshift/security/",
	"title": "平台安全",
	"tags": [],
	"description": "",
	"content": " 第五章 平台安全 本章主要讲述Openshift Origin 平台信息安全配置等实践\n"
},
{
	"uri": "/openshift/devops/",
	"title": "开发运维",
	"tags": [],
	"description": "",
	"content": " 第六章 DevOps 本章主要主要讲述DevOps在Openshift Origin 平台上的相关实践\n"
},
{
	"uri": "/openshift/faq/",
	"title": "常见问题",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-05-02 创建 开源方案 初始版本    error: Build error: Failed to push image: After retrying 6 times, Push image still failed. 问题原因: 没有配置openshift sdn 网络\n 解决方法  所有计算节点都需要安装 origin-sdn-ovs 软件包 (如果master节点也做计算节点的话，也需要安装)\nyum install origin-sdn-ovs.x86_64 -y  修改 主节点(master) 和 计算节点(node)的配置文件（红色框框原来默认是空的） /etc/origin/master/master-config.yaml 重启master 节点\nsystemctl daemon-reload systemctl restart origin-master  修改所有计算节点的配置,与master设置一致 /etc/origin/node/node-config.yaml 重启计算节点(node)\nsystemctl daemon-reload systemctl restart origin-node  注意： master 节点网络配置 和 计算节点(node) 一定要一致\n 通过在master 节点执行如下命令查看分配到各个节点的子网\n[root@openshift-master ~]# oc get hostsubnets NAME HOST HOST IP SUBNET openshift-node1 openshift-node1 192.168.124.30 10.128.0.0/23 openshift-node2 openshift-node2 192.168.124.46 10.129.0.0/23   rbd: image foo: image uses unsupported features: 0x38 我们在rbd pool中(在上述命令中未指定pool name，默认image建立在rbd pool中)创建一个大小为1024Mi的ceph image foo，rbd list命令的输出告诉我们foo image创建成功\n# rbd create foo -s 1024 # rbd list foo  接下来，我们尝试将foo image映射到内核，并格式化该image：\nroot@ceph-1:~# rbd map foo rbd: sysfs write failed RBD image feature set mismatch. You can disable features unsupported by the kernel with \u0026quot;rbd feature disable\u0026quot;. In some cases useful info is found in syslog - try \u0026quot;dmesg | tail\u0026quot; or so. rbd: map failed: (6) No such device or address map操作报错。通过 “dmesg | tail”可以看到如下报错 rbd: image foo: image uses unsupported features: 0x38  ceph新版中在map image时，给image默认加上了许多feature，通过rbd info可以查看到：\n# rbd info foo rbd image 'foo': size 1024 MB in 256 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.10612ae7234b format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten flags:  特性说明:\nlayering: 支持分层\nexclusive-lock: 支持独占锁\nobject-map: 支持对象映射（依赖 exclusive-lock ）\nfast-diff: 快速计算差异（依赖 object-map ）\ndeep-flatten: 支持快照扁平化操作\n问题原因: CentOS 7 的3.10内核仅支持ceph image其中的layering feature，其他feature概不支持\n 解决方法  手动disable这些features  # rbd feature disable foo exclusive-lock, object-map, fast-diff, deep-flatten root@ceph-1:/var/log/ceph# rbd info foo rbd image 'foo': size 1024 MB in 256 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.10612ae7234b format: 2 features: layering flags:   配置方法在各个cluster node的/etc/ceph/ceph.conf中加上这样一行配置  #仅是layering对应的bit码所对应的整数值 rbd_default_features = 1 #设置完后，通过下面命令查看配置变化： # ceph --show-config|grep rbd|grep featuresrbd_default_features = 1   在单个创建rbd的时候也可以通过修改format的版本来实现挂载，这里把format指定为1之后其实上面的rbd的很多特性功能都已经去掉了：  rbd create foo --size 10G --image-format 1 --image-feature layering   再次map一下foo这个image：  # rbd map foo # ls -l /dev/rbd0 # map后，我们就可以像格式化一个空image那样对其进行格式化了，这里格成ext4文件系统 # mkfs.ext4 /dev/rbd0 mke2fs 1.42.9 (4-Feb-2014) Discarding device blocks: done Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) Stride=1024 blocks, Stripe width=1024 blocks 65536 inodes, 262144 blocks 13107 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=268435456 8 block groups 32768 blocks per group, 32768 fragments per group 8192 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376 Allocating group tables: done Writing inode tables: done Creating journal (8192 blocks): done Writing superblocks and filesystem accounting information: done  挂载到系统中，就可以使用了\n"
},
{
	"uri": "/openshift/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/openshift/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/openshift/",
	"title": "红帽 OpenShift",
	"tags": [],
	"description": "",
	"content": " 红帽 OpenShift 红帽® OpenShift 是一款容器应用平台，它将 Docker 和 Kubernetes 技术带入企业。\n无论您采用何种应用架构，OpenShift 都能让您在任意架构中（公共或私有云中）轻松、快速实现应用的构建、开发和部署。\n无论是在企业内部，公共云，或是托管环境中，您都能凭借这一备受业务青睐的平台，快速您的最新创意推向市场，从而在激烈的市场竞争中脱颖而出。\n本套文档涉及到的版本    版本 类型 备注     CentOS Linux release 7.3.1611 (Core) 操作系统 使用centos,yum仓库默认配置EPEL仓库源   Openshift Origin v3.6.1 容器云 使用开源版本    [root@openshift-master ~]# oc version oc v3.6.1+008f2d5 kubernetes v1.6.1+5115d708d7 features: Basic-Auth GSSAPI Kerberos SPNEGO Server https://192.168.124.22:8443 openshift v3.6.1+008f2d5 kubernetes v1.6.1+5115d708d7  问题反馈 在看文档过程中，有任何问题，请到这里留言反馈，谢谢。\n我们会在第一时间响应回复\n openshift origin 使用客户列表 如果您在真实的生产工作中，使用了openshift origin, 请反馈给我们,也能看出在国内的使用情况,请到这里推荐给我们\nOpenshift 功能和优势  企业级 Kubernetes\n红帽 OpenShift 作为一个功能全面容器应用平台，其自身具备 Docker 和 Kubernetes 等最新技术，是一款功能强大的容器集群管理和编排系统。 该平台和红帽企业 Linux系统一同为企业提供坚实技术基础。 OpenShift 集成了企业所需的多种架构、流程、平台及服务，可为开发和运营团队同时提供强大支持。\n 有状态和无状态的应用\n红帽 OpenShift 可运行和支持有状态和无状态的应用。因此，您无需完全重构您的企业应用，即可充分利用容器技术。\n 实现应用现代化\nOpenShift 与红帽 JBoss 中间件相结合后，可以实现多种综合性云原生服务，包括开发人员工具、整合、业务自动化、数据管理等。因而，有助您更加快速、智能、灵活地开发应用，克服使用微服务构建分布式系统的挑战。\n 构建更出色、更强大的混合云\n就像红帽企业 Linux，这一用于构建众多公共、私有云环境的基础平台一样，您可以在任意位置部署 OpenShift 和获得相应支持。这些云环境包括 Amazon Web Services、Azure、Google 云平台、VMware 等等。依托 OpenShift，您可以在这些公共云和私有云上，提供单个容器应用平台。另外，借助在 Microsoft Azure 上运行的红帽 OpenShift 容器平台，您可以构建、部署和管理容器化服务和应用。\n 拥抱 DevOps\nOpenShift 为开发和运营团队提供了一个通用平台和一整套开发和管理工具。这使得两个团队能够步调一致，保持持续、协调化的应用开发和维护流程。通过此方法，您可以消除耗时缓慢的业务流程和手动操作，根据业务需求灵活开展运营。\n  "
}]