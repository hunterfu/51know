[
{
	"uri": "/openshift/appcenter/custom_category/",
	"title": "定制 Catalog 分类",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-27 创建 开源方案 初始版本    类目分类说明 类目分类是用来组织模板和builder image的分组方式,我们可以按照自己需要的方式，对应用模板进行归类. 主要匹配的是 模板或者ImageStream的 tag 标签项\n定制类目方法 创建下面配置js脚本,比如 my-catalog-categories.js // Add Go to the Languages category var category = _.find(window.OPENSHIFT_CONSTANTS.CATALOG_CATEGORIES, { id: 'languages' }); category.items.splice(2,0,{ // Insert at the third spot // Required. Must be unique id: \u0026quot;go\u0026quot;, // Required label: \u0026quot;Go\u0026quot;, // Optional. If specified, defines a unique icon for this item iconClass: \u0026quot;font-icon icon-go-gopher\u0026quot;, // Optional. If specified, enables matching other tag values to this category // item categoryAliases: [ \u0026quot;golang\u0026quot; ] }); // Add a Featured category section at the top of the catalog window.OPENSHIFT_CONSTANTS.CATALOG_CATEGORIES.unshift({ // Required. Must be unique id: \u0026quot;opssolution\u0026quot;, // Required label: \u0026quot;开源方案之解决方案\u0026quot;, // Optional. If specified, each item in the category will utilize this icon // as a default iconClassDefault: \u0026quot;fa fa-code\u0026quot;, items: [ { // Required. Must be unique id: \u0026quot;go\u0026quot;, // Required label: \u0026quot;Go\u0026quot;, // Optional. If specified, defines a unique icon for this item iconClass: \u0026quot;font-icon icon-go-gopher\u0026quot;, // Optional. If specified, enables matching other tag values to this // category item categoryAliases: [ \u0026quot;golang\u0026quot; ], // Optional. If specified, will display below the item label description: \u0026quot;An open source programming language developed at Google in \u0026quot; + \u0026quot;2007 by Robert Griesemer, Rob Pike, and Ken Thompson.\u0026quot; }, { // Required. Must be unique id: \u0026quot;httpd\u0026quot;, // Required label: \u0026quot;HttpdServer\u0026quot;, // Optional. If specified, defines a unique icon for this item iconClass: \u0026quot;font-icon icon-apache\u0026quot;, // Optional. If specified, enables matching other tag values to this // category item categoryAliases: [ \u0026quot;httpd\u0026quot; ], // Optional. If specified, will display below the item label description: \u0026quot;开源web server\u0026quot; + \u0026quot;最老牌的http server\u0026quot; }, { // Required. Must be unique 开源方案的ID id: \u0026quot;51know\u0026quot;, // Required label: \u0026quot;开源方案之解决方案\u0026quot;, // Optional. If specified, defines a unique icon for this item iconClass: \u0026quot;font-icon icon-load-balancer\u0026quot;, // Optional. If specified, enables matching other tag values to this // category item categoryAliases: [ \u0026quot;httpd\u0026quot; ], // Optional. If specified, will display below the item label description: \u0026quot;由开源方案维护支持的解决方案\u0026quot; + \u0026quot;涉及到常见的应用解决方案\u0026quot; }, { // Required. Must be unique id: \u0026quot;jenkins\u0026quot;, // Required label: \u0026quot;Jenkins\u0026quot;, // Optional. If specified, defines a unique icon for this item iconClass: \u0026quot;font-icon icon-jenkins\u0026quot;, // Optional. If specified, will display below the item label description: \u0026quot;An open source continuous integration tool written in Java.\u0026quot; } ] });  保存文件,并修改 master 配置文件 /etc/origin/master/master-config.yaml  assetConfig: ... extensionScripts: - /path/to/my-catalog-categories.js  重启master服务 # systemctl restart origin-master  定制后，效果如下 参考资料\nhttps://docs.openshift.org/3.6/install_config/web_console_customization.html#configuring-catalog-categories\nhttps://docs.openshift.org/3.6/dev_guide/templates.html#writing-description\n"
},
{
	"uri": "/openshift/application/import_template/",
	"title": "导入示例模板",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-27 创建 开源方案 初始版本    导入模板和镜像(导入了部分模板) 本节的所涉及到的操作都是在openshift master节点上操作\n导入基础镜像(S2I RUNTIME) oc project openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/image-streams/image-streams-centos7.json -n openshift imagestream \u0026quot;httpd\u0026quot; created imagestream \u0026quot;ruby\u0026quot; created imagestream \u0026quot;nodejs\u0026quot; created imagestream \u0026quot;perl\u0026quot; created imagestream \u0026quot;php\u0026quot; created imagestream \u0026quot;python\u0026quot; created imagestream \u0026quot;wildfly\u0026quot; created imagestream \u0026quot;mysql\u0026quot; created imagestream \u0026quot;mariadb\u0026quot; created imagestream \u0026quot;postgresql\u0026quot; created imagestream \u0026quot;mongodb\u0026quot; created imagestream \u0026quot;redis\u0026quot; created imagestream \u0026quot;jenkins\u0026quot; created  导入应用模板(openshift项目中) oc project openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/mariadb-ephemeral-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/mariadb-persistent-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/mongodb-ephemeral-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/mongodb-persistent-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/mysql-ephemeral-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/mysql-persistent-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/redis-ephemeral-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/db-templates/redis-persistent-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/quickstart-templates/cakephp-mysql.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/quickstart-templates/cakephp-mysql-persistent.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/quickstart-templates/jenkins-ephemeral-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/quickstart-templates/jenkins-persistent-template.json -n openshift oc create -f https://raw.githubusercontent.com/openshift/openshift-ansible/release-3.6/roles/openshift_examples/files/examples/v3.6/quickstart-templates/httpd.json -n openshift  查看导入的镜像列表 [root@openshift-master ~]# oc project openshift Now using project \u0026quot;openshift\u0026quot; on server \u0026quot;https://192.168.124.22:8443\u0026quot;. [root@openshift-master ~]# oc get is -n openshift NAME DOCKER REPO TAGS UPDATED httpd 172.30.111.126:5000/openshift/httpd latest,2.4 About an hour ago jenkins 172.30.111.126:5000/openshift/jenkins latest,1,2 About an hour ago mariadb 172.30.111.126:5000/openshift/mariadb 10.1,latest About an hour ago mongodb 172.30.111.126:5000/openshift/mongodb 2.4,latest,3.2 + 1 more... About an hour ago mysql 172.30.111.126:5000/openshift/mysql latest,5.7,5.6 + 1 more... About an hour ago nodejs 172.30.111.126:5000/openshift/nodejs latest,0.10,4 + 1 more... About an hour ago perl 172.30.111.126:5000/openshift/perl latest,5.24,5.20 + 1 more... About an hour ago php 172.30.111.126:5000/openshift/php 5.5,latest,7.0 + 1 more... About an hour ago postgresql 172.30.111.126:5000/openshift/postgresql latest,9.5,9.4 + 1 more... About an hour ago python 172.30.111.126:5000/openshift/python latest,3.5,3.4 + 2 more... About an hour ago redis 172.30.111.126:5000/openshift/redis latest,3.2 About an hour ago ruby 172.30.111.126:5000/openshift/ruby 2.3,2.2,2.0 + 1 more... About an hour ago wildfly 172.30.111.126:5000/openshift/wildfly latest,10.1,10.0 + 2 more... About an hour ago  查看导入的应用模板列表(相当于企业内部的APPSTORE 应用市场) [root@openshift-master ~]# oc get templates -n openshift NAME DESCRIPTION PARAMETERS OBJECTS cakephp-mysql-example An example CakePHP application with a MySQL database. For more information ab... 19 (4 blank) 8 cakephp-mysql-persistent An example CakePHP application with a MySQL database. For more information ab... 20 (4 blank) 9 httpd-example An example Httpd application that serves static content. For more information... 9 (3 blank) 5 jenkins-ephemeral Jenkins service, without persistent storage.... 7 (all set) 6 jenkins-persistent Jenkins service, with persistent storage.... 8 (all set) 7 mariadb-ephemeral MariaDB database service, without persistent storage. For more information ab... 7 (3 generated) 3 mariadb-persistent MariaDB database service, with persistent storage. For more information about... 8 (3 generated) 4 mongodb-ephemeral MongoDB database service, without persistent storage. For more information ab... 8 (3 generated) 3 mongodb-persistent MongoDB database service, with persistent storage. For more information about... 9 (3 generated) 4 mysql-ephemeral MySQL database service, without persistent storage. For more information abou... 8 (3 generated) 3 mysql-persistent MySQL database service, with persistent storage. For more information about u... 9 (3 generated) 4 redis-ephemeral Redis in-memory data structure store, without persistent storage. For more in... 5 (1 generated) 3 redis-persistent Redis in-memory data structure store, with persistent storage. For more infor... 6 (1 generated) 4  "
},
{
	"uri": "/openshift/install/arch_intro/",
	"title": "架构介绍",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    Openshift Origin v3.6 架构图 OpenShift 是一款容器应用平台，它将 Docker 和 Kubernetes 技术带入企业。\n上图是openshift origin 总体架构图\n上图是 openshift 和 k8s 所在容器云平台的关系\n主服务和计算节点关系结构 主服务器(Masters)依赖于基于etcd的分布式目录， 主要用来提供配置共享和服务发现\n计算节点(Nodes) 主要用来作为PODS的宿主和运行容器\n整体应用概念介绍 上述应用架构图中， 概念来源于Kubernetes的概念， 需要明白以下主要的对象。\n 一个 POD 是一个Docker 容器的运行环境(如果需要共享本地的资源， 我们将在单独的POD中布署两种类别的容器) 一个 Service 服务是一个入口(VIP)，抽象出一个均衡访问负载到一组相同的容器，理论上， 最少是一个服务对应一个架构层 一个服务布署者(Service Deployer)或布署配置(Deployment Config)是一个对象， 用来描述基于触发器的容器的布署策略(比如，当docker仓库中有新版本的映象时， 重新布署) 一个复制控制器(Replication Controller)是一个技术组件， 主要负责POD 的弹性。 一个路由(Route)是用来暴露一个应用的入口(域名解析， 主机名或VIP)  "
},
{
	"uri": "/openshift/install/env_intro/",
	"title": "部署环境介绍",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    Openshift v3.6 高可用部署架构图  Masters Node 负载均衡使用 HAPROXY 做基于tcp模式的负载(SSL证书穿透) Data Store 是使用etcd作为信息的存储数据库 Infrastructure Node 基础设施节点,用于运行平台自身的管理服务(route,docker仓库,度量数据,日志数据等),也可以定义一些其他功能节点 比如ops Node 主要运维相关的服务(zabbix,cmdb,ticket等),dev Node 主要运行研发相关服务(maven镜像库,gitlab,go-cd等) Persistent Storage 持久卷这里使用的是 CEPH https://ceph.com/ 分布式文件系统 操作系统发行版本使用的是 CENTOS 7  部署主机角色说明    主机角色 IP地址 操作系统 摘要 域名     管理节点(Master) openshift-master1(192.168.124.22) CentOS Linux release 7.3.1611 (Core) x86_64 master + etcd + haproxy openshift.ops.com   管理节点(Master) openshift-master2(192.168.124.23) CentOS Linux release 7.3.1611 (Core) x86_64 master + etcd openshift.ops.com   管理节点(Master) openshift-master3(192.168.124.24) CentOS Linux release 7.3.1611 (Core) x86_64 master + etcd openshift.ops.com   基础设施节点(Node) openshift-node1（192.168.124.30） CentOS Linux release 7.3.1611 (Core) X86-64 router + registry lb.openshift.ops.com   计算节点(Node) openshift-node2（192.168.124.46） CentOS Linux release 7.3.1611 (Core) X86-64 计算节点 无对外域名    注：本环境中 master节点的负载均衡haproxy 示例没有配置2个节点,如需要,可以参考互联网上 haproxy + keeplived 方式实现\n域名设置说明    域名角色 通配域名(泛域名) CNAME地址     开发环境域名 *.dev.openshift.ops.com lb.openshift.ops.com   测试环境域名 *.test.openshift.ops.com lb.openshift.ops.com   生产环境域名 *.prod.openshift.ops.com lb.openshift.ops.com    注：这里为每个环境都配置范域名解析,都指向 router 所在的计算节点上,如果没有配置DNS,做hosts绑定域名也可以\n但是 master的高可用负载域名 openshift.ops.com(指向haproxy) 在master节点所配置的dns必须能够解析,hosts绑定是无效的, 原因是计算节点运行的容器内部 /etc/resolve.conf dns是指向master节点的,如果容器内需要访问 openshift.ops.com 这个域名就会forward 主服务节点配置的dns上\n "
},
{
	"uri": "/openshift/install/etcd_install/",
	"title": "Etcd集群安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    ETCD证书  etcd集群信息     主机名 IP地址 域名 etcd版本     etcd0 192.168.124.22 etcd0.51know.info etcd-3.2.15-1.el7.x86_64   etcd1 192.168.124.23 etcd1.51know.info etcd-3.2.15-1.el7.x86_64   etcd2 192.168.124.24 etcd0.51know.info etcd-3.2.15-1.el7.x86_64     证书生成     证书名称 配置文件 用途     etcd-root-ca.pem etcd-root-ca-csr.json etcd 根 CA 证书   etcd.pem etcd-gencert.json、etcd-csr.json etcd 集群证书     CFSSL 工具安装 首先下载 cfssl，并给予可执行权限，然后扔到 PATH 目录下  [root@openshift-master1 /opt]# wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 [root@openshift-master1 /opt]# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 [root@openshift-master1 /opt]# chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 [root@openshift-master1 /opt]# mv cfssl_linux-amd64 /usr/local/bin/cfssl [root@openshift-master1 /opt]# mv cfssljson_linux-amd64 /usr/local/bin/cfssljson   Etcd 证书生成所需配置文件如下:  [root@openshift-master1 /opt]# cat etcd-root-ca-csr.json { \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 4096 }, \u0026quot;names\u0026quot;: [ { \u0026quot;O\u0026quot;: \u0026quot;etcd\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;etcd Security\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Beijing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Beijing\u0026quot;, \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot; } ], \u0026quot;CN\u0026quot;: \u0026quot;etcd-root-ca\u0026quot; } [root@openshift-master1 /opt]# cat etcd-gencert.json { \u0026quot;signing\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot;, \u0026quot;client auth\u0026quot; ], \u0026quot;expiry\u0026quot;: \u0026quot;87600h\u0026quot; } } } [root@openshift-master1 /opt]# cat etcd-csr.json { \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 4096 }, \u0026quot;names\u0026quot;: [ { \u0026quot;O\u0026quot;: \u0026quot;etcd\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;etcd Security\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Beijing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Beijing\u0026quot;, \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot; } ], \u0026quot;CN\u0026quot;: \u0026quot;etcd\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;127.0.0.1\u0026quot;, \u0026quot;localhost\u0026quot;, \u0026quot;192.168.124.22\u0026quot;, \u0026quot;192.168.124.23\u0026quot;, \u0026quot;192.168.124.24\u0026quot; ] }  注意: hosts 要将 etcd 集群的所在节点的 IP地址,主机名(FQDN),都要加入到此列表中\n 生成 Etcd 证书  [root@openshift-master1 /opt]# cfssl gencert --initca=true etcd-root-ca-csr.json | cfssljson --bare etcd-root-ca [root@openshift-master1 /opt]# cfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd #生成的证书列表如下 [root@openshift-master1 /opt] # ll 总用量 36 -rw-r--r-- 1 root root 2033 3月 27 18:09 etcd.csr -rw-r--r-- 1 root root 513 3月 27 18:09 etcd-csr.json -rw-r--r-- 1 root root 204 3月 27 18:08 etcd-gencert.json -rw------- 1 root root 3247 3月 27 18:09 etcd-key.pem -rw-r--r-- 1 root root 2415 3月 27 18:09 etcd.pem -rw-r--r-- 1 root root 1708 3月 27 18:09 etcd-root-ca.csr -rw-r--r-- 1 root root 232 3月 27 18:07 etcd-root-ca-csr.json -rw------- 1 root root 3243 3月 27 18:09 etcd-root-ca-key.pem -rw-r--r-- 1 root root 2078 3月 27 18:09 etcd-root-ca.pem  部署 ETCD 集群 第一个节点etcd0 安装  安装etcd，并将证书拷贝安装目录，赋权  [root@openshift-master1 /opt]# yum install etcd -y [root@openshift-master1 /opt]# cp *.pem /etc/etcd/ [root@openshift-master1 /opt]# chown -R etcd:etcd /etc/etcd/ [root@openshift-master1 /opt]# chmod -R 755 /etc/etcd/   配置内容  [root@openshift-master etcd]# cat /etc/etcd/etcd.conf #[Member] ETCD_DATA_DIR=\u0026quot;/var/lib/etcd/default.etcd\u0026quot; ETCD_LISTEN_PEER_URLS=\u0026quot;https://192.168.124.22:2380\u0026quot; ETCD_LISTEN_CLIENT_URLS=\u0026quot;https://192.168.124.22:2379,http://localhost:2379\u0026quot; ETCD_NAME=\u0026quot;etcd0\u0026quot; ETCD_HEARTBEAT_INTERVAL=500 ETCD_ELECTION_TIMEOUT=2500 #[Clustering] ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026quot;https://192.168.124.22:2380\u0026quot; ETCD_ADVERTISE_CLIENT_URLS=\u0026quot;https://192.168.124.22:2379\u0026quot; ETCD_INITIAL_CLUSTER=\u0026quot;etcd0=https://192.168.124.22:2380,etcd1=https://192.168.124.23:2380,etcd2=https://192.168.124.24:2380\u0026quot; ETCD_INITIAL_CLUSTER_TOKEN=\u0026quot;etcd-cluster\u0026quot; ETCD_INITIAL_CLUSTER_STATE=\u0026quot;new\u0026quot; #[Security] ETCD_CERT_FILE=\u0026quot;/etc/etcd/etcd.pem\u0026quot; ETCD_KEY_FILE=\u0026quot;/etc/etcd/etcd-key.pem\u0026quot; ETCD_CLIENT_CERT_AUTH=\u0026quot;true\u0026quot; ETCD_TRUSTED_CA_FILE=\u0026quot;/etc/etcd/etcd-root-ca.pem\u0026quot; ETCD_AUTO_TLS=\u0026quot;true\u0026quot; ETCD_PEER_CERT_FILE=\u0026quot;/etc/etcd/etcd.pem\u0026quot; ETCD_PEER_KEY_FILE=\u0026quot;/etc/etcd/etcd-key.pem\u0026quot; ETCD_PEER_CLIENT_CERT_AUTH=\u0026quot;true\u0026quot; ETCD_PEER_TRUSTED_CA_FILE=\u0026quot;/etc/etcd/etcd-root-ca.pem\u0026quot; ETCD_PEER_AUTO_TLS=\u0026quot;true\u0026quot;   启动服务  [root@openshift-master1 /opt]# systemctl enable etcd [root@openshift-master1 /opt]# systemctl start etcd  其他2个节点安装  安装 etcd 软件包  yum install etcd -y   将第一个节点的配置拷贝到其他2个节点  [root@openshift-master ~]# cd /etc/etcd/ [root@openshift-master etcd]# ll total 20 -rwxr-xr-x 1 etcd etcd 920 Apr 18 06:11 etcd.conf -rwxr-xr-x 1 etcd etcd 3243 Apr 18 06:07 etcd-key.pem -rwxr-xr-x 1 etcd etcd 2167 Apr 18 06:07 etcd.pem -rwxr-xr-x 1 etcd etcd 3247 Apr 18 06:07 etcd-root-ca-key.pem -rwxr-xr-x 1 etcd etcd 2078 Apr 18 06:07 etcd-root-ca.pem [root@openshift-master1 etcd]# scp * openshift-master2:/etc/etcd/ etcd.conf 100% 920 0.9KB/s 00:00 etcd-key.pem 100% 3243 3.2KB/s 00:00 etcd.pem 100% 2167 2.1KB/s 00:00 etcd-root-ca-key.pem 100% 3247 3.2KB/s 00:00 etcd-root-ca.pem 100% 2078 2.0KB/s 00:00   在其他2个节点上修改如下配置项,ip地址改成本节点的对应的IP地址  ETCD_LISTEN_PEER_URLS=\u0026quot;https://192.168.124.23:2380\u0026quot; ETCD_LISTEN_CLIENT_URLS=\u0026quot;https://192.168.124.23:2379,http://localhost:2379\u0026quot; #ETCD节点名称 按顺序增加即可 ETCD_NAME=\u0026quot;etcd1\u0026quot; ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026quot;https://192.168.124.23:2380\u0026quot; ETCD_ADVERTISE_CLIENT_URLS=\u0026quot;https://192.168.124.23:2379\u0026quot;   启动服务即可  验证(3个节点都安装配置完成后) [root@openshift-master etcd]# export ETCDCTL_API=3 [root@openshift-master etcd]# etcdctl member list 2da38978bc038ba1, started, etcd1, https://192.168.124.22:2380, https://192.168.124.22:2379 56e71904a9636fcf, started, etcd0, https://192.168.124.23:2380, https://192.168.124.23:2379 faf6915e4bb01350, started, etcd2, https://192.168.124.24:2380, https://192.168.124.24:2379 [root@openshift-master etcd]# etcdctl --cacert=/etc/etcd/etcd-root-ca.pem --cert=/etc/etcd/etcd.pem --key=/etc/etcd/etcd-key.pem --endpoints=https://192.168.124.22:2379,https://192.168.124.23:2379,https://192.168.124.24:2379 endpoint health https://192.168.124.22:2379 is healthy: successfully committed proposal: took = 3.852481ms https://192.168.124.23:2379 is healthy: successfully committed proposal: took = 4.035725ms https://192.168.124.24:2379 is healthy: successfully committed proposal: took = 1.489679ms  至此,etcd集群安装完成\n数据初始化 如果在安装过程中，可能需要重新初始化数据可以参考如下方法\n 导出API版本，这里使用3版本\nexport ETCDCTL_API=3  获取key\netcdctl get / --prefix --keys-only  删除 openshift.io 相关内容\netcdctl del --prefix=true /openshift.io  删除 kubernetes.io\netcdctl del --prefix=true /kubernetes.io  再次查看\netcdctl get / --prefix --keys-only   "
},
{
	"uri": "/openshift/install/master_install/",
	"title": "主服务安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    部署主机角色说明    主机角色 IP地址 负载域名     管理节点(Master) openshift-master1(192.168.124.22) openshift.ops.com   管理节点(Master) openshift-master2(192.168.124.23) openshift.ops.com   管理节点(Master) openshift-master3(192.168.124.24) openshift.ops.com    安装前基础环境检查 配置仓库源(如果自己建立本地源 直接跳过本步骤)  安装仓库源文件  yum install centos-release-openshift-origin.noarch   修改仓库配置 指向 V3.6版本(默认是指向最新的版本)  [root@openshift-master ~]# cat /etc/yum.repos.d/CentOS-OpenShift-Origin.repo [centos-openshift-origin] name=CentOS OpenShift Origin baseurl=http://mirrors.163.com/centos/7/paas/x86_64/openshift-origin36/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-PaaS 其他省略。。。。。。。。  主要是把 http://mirror.centos.org/centos/7/paas/x86_64/openshift-origin/ 修改为下面链接(使用163镜像) http://mirrors.163.com/centos/7/paas/x86_64/openshift-origin36/\n安装第一个master节点软件包  安装 origin-master  yum install origin-master -y   由于yum安装的master节点的证书不包含对外负载域名(openshift.ops.com),所以需要重新签发证书,并删除默认node配置  openshift start master --public-master='https://openshift.ops.com' --network-plugin='redhat/openshift-ovs-subnet' --write-config=/etc/origin/master rm -fr /etc/origin/node   查看证书生成的信息,DNS中会生成一个openshift.ops.com的域名  [root@openshift-master master]# openssl x509 -noout -text -in master.server.crt Certificate: Data: Version: 3 (0x2) Serial Number: 11 (0xb) Signature Algorithm: sha256WithRSAEncryption Issuer: CN=openshift-signer@1524139430 Validity Not Before: Apr 19 12:06:27 2018 GMT Not After : Apr 18 12:06:28 2020 GMT Subject: CN=127.0.0.1 Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:b1:93:7c:57:d5:e1:c1:2c:59:1a:28:9e:b0:df: 38:cc:de:ab:d3:ab:6a:fa:97:3a:f2:79:80:26:0b: f0:92:7f:e3:e8:be:da:37:43:d0:f6:ce:d9:c1:e0: a5:cb:cf:af:04:bb:a4:bc:84:2c:a4:97:08:d4:c1: a5:d5:48:4f:3a:96:fb:2e:66:ad:6e:1f:d1:4a:8d: 21:c4:68:3d:f2:79:e2:3e:c5:e1:ee:78:2b:63:96: d7:fa:f2:e8:b4:58:45:1c:ba:6c:ca:0f:4b:b3:cf: 26:95:43:fe:fa:43:88:a4:48:c7:4e:07:83:66:eb: fe:48:78:f2:07:24:7c:a8:f4:6f:7b:80:5a:7e:7d: 0f:b2:87:46:5b:76:05:e2:d3:f0:58:87:69:64:5a: 17:91:70:6f:81:90:89:ac:65:57:cc:f2:67:8b:c7: 26:0d:79:b7:84:3f:58:ec:5c:d7:a2:85:17:36:e8: 62:86:6d:3d:21:43:38:cf:1c:2c:c4:c9:3d:6c:b4: da:c3:0c:5e:ca:3f:74:ff:b7:39:1e:fb:63:bf:47: 66:54:54:8f:88:c3:8f:ba:a5:dd:70:ec:53:6a:ce: 49:48:77:1a:10:cc:81:bb:85:a4:55:b7:07:e9:fa: 7c:67:38:40:35:1c:bf:cf:ee:45:79:19:6b:69:45: 3d:0b Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Subject Alternative Name: DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, DNS:localhost, DNS:openshift, DNS:openshift.default, DNS:openshift.default.svc, DNS:openshift.default.svc.cluster.local, DNS:openshift.ops.com, DNS:127.0.0.1, DNS:172.30.0.1, DNS:192.168.124.22, IP Address:127.0.0.1, IP Address:172.30.0.1, IP Address:192.168.124.22 Signature Algorithm: sha256WithRSAEncryption d5:8b:11:66:64:0e:cc:b9:36:85:15:1f:75:02:d1:9b:5c:32: b5:af:1e:d9:38:85:e0:95:77:d7:5d:42:dd:e9:40:07:c8:d2: ae:4b:99:db:8f:61:49:e7:3b:37:4b:22:cc:0b:07:5d:6a:39: ce:82:e8:00:38:4e:af:14:1b:9c:78:6a:2e:58:b8:44:c0:62: 96:18:7d:58:2c:9c:db:87:e3:47:20:61:97:7f:ae:3f:74:c5: 4a:cc:88:e6:6b:1b:4c:b4:16:6d:66:99:4a:7f:bc:51:ec:b4: 17:66:56:ab:d5:16:0f:a8:2b:8b:5c:dc:91:e1:bc:3b:99:41: 5b:ad:cb:f0:52:20:23:93:46:44:de:cf:fe:70:27:ec:8d:eb: 65:23:84:5d:cb:75:18:31:19:d9:0d:8c:43:0b:6f:c7:97:1e: 02:41:d9:07:93:bb:b0:dc:53:08:54:0e:48:cc:1c:60:4d:87: c2:a8:be:56:55:af:53:62:21:29:2b:43:eb:38:45:f9:11:52: b6:d8:56:77:3d:a0:34:1c:69:3b:e1:3d:f9:85:46:f9:60:b9: 2e:b4:b2:e2:54:a7:20:7a:a3:50:de:38:ad:4b:31:e3:45:2c: 45:3a:b6:8c:a3:5f:80:47:97:f9:e8:2f:e6:b8:2d:11:55:3d: 0a:6a:fc:18   修改master etcd配置,使用前面配置好的etcd集群  修改前\n[root@openshift-master master]# cat /etc/origin/master/master-config.yaml .....省略..... etcdClientInfo: ca: ca.crt certFile: master.etcd-client.crt keyFile: master.etcd-client.key urls: - https://192.168.124.22:4001 etcdConfig: address: 192.168.124.22:4001 peerAddress: 192.168.124.22:7001 peerServingInfo: bindAddress: 0.0.0.0:7001 bindNetwork: tcp4 certFile: etcd.server.crt clientCA: ca.crt keyFile: etcd.server.key namedCertificates: null servingInfo: bindAddress: 0.0.0.0:4001 bindNetwork: tcp4 certFile: etcd.server.crt clientCA: ca.crt keyFile: etcd.server.key namedCertificates: null storageDirectory: /etc/origin/openshift.local.etcd etcdStorageConfig: kubernetesStoragePrefix: kubernetes.io kubernetesStorageVersion: v1 openShiftStoragePrefix: openshift.io openShiftStorageVersion: v1 .....省略.....  修改后\n[root@openshift-master master]# cat /etc/origin/master/master-config.yaml .....省略..... etcdClientInfo: ca: master.etcd-ca.crt certFile: master.etcd-client.crt keyFile: master.etcd-client.key urls: - https://192.168.124.22:2379 - https://192.168.124.23:2379 - https://192.168.124.24:2379 etcdStorageConfig: kubernetesStoragePrefix: kubernetes.io kubernetesStorageVersion: v1 openShiftStoragePrefix: openshift.io openShiftStorageVersion: v1 .....省略.....   openshift配置与etcd的证书对应关系     openshift证书名 etcd证书名     master.etcd-ca.crt etcd-root-ca.pem   master.etcd-client.crt etcd.pem   master.etcd-client.key etcd-key.pem     拷贝相关证书到master目录  [root@openshift-master master]# cat /etc/origin/master/master-config.yaml ^C [root@openshift-master master]# cp /etc/etcd/etcd-root-ca.pem /etc/origin/master/master.etcd-ca.crt [root@openshift-master master]# cp /etc/etcd/etcd.pem /etc/origin/master/master.etcd-client.crt cp: overwrite ‘/etc/origin/master/master.etcd-client.crt’? y [root@openshift-master master]# cp /etc/etcd/etcd-key.pem /etc/origin/master/master.etcd-client.key cp: overwrite ‘/etc/origin/master/master.etcd-client.key’? y   启动master  systemctl enable origin-master.service systemctl start origin-master.service  权限登录配置 超级admin用户登陆配置说明(只能命令行登陆)  admin 用户登陆使用密钥文件  /etc/origin/master/admin.kubeconfig   管理员登陆 使用证书密钥进行管理(当前用户是root)  [root@openshift-master ~]# mkdir -p /root/.kube [root@openshift-master ~]# cp /etc/origin/master/admin.kubeconfig /root/.kube/config [root@openshift-master ~]# oc login -u system:admin Logged into \u0026quot;https://192.168.124.22:8443\u0026quot; as \u0026quot;system:admin\u0026quot; using existing credentials. You have access to the following projects and can switch between them with 'oc project \u0026lt;projectname\u0026gt;': * default kube-public kube-system openshift openshift-infra Using project \u0026quot;default\u0026quot;.   执行 如下命令可以看到当前登录用户  [root@openshift-master ~]# oc whoami system:admin  WEBUI 登陆用户配置  安装完默认是任意密码登陆，这里配置htpasswd方式进行认证  修改前\n provider: apiVersion: v1 kind: AllowAllPasswordIdentityProvider  修改后\n provider: apiVersion: v1 kind: HTPasswdPasswordIdentityProvider file: /etc/origin/master/htpasswd   安装htpasswd  [root@openshift-master master]# yum install httpd-tools -y   添加ops账户  [root@openshift-master master]# htpasswd -c /etc/origin/master/htpasswd ops New password: Re-type new password: Adding password for user ops   将ops用户加入openshift-infra 和 default 项目，并赋予admin权限  [root@openshift-master master]# oc adm policy add-role-to-user admin ops -n openshift-infra role \u0026quot;admin\u0026quot; added: \u0026quot;ops\u0026quot; [root@openshift-master master]# oc adm policy add-role-to-user admin ops -n default role \u0026quot;admin\u0026quot; added: \u0026quot;ops\u0026quot;   重启master 节点  systemctl daemon-reload systemctl restart origin-master  至此 第一个master节点安装部署完成\n其他2个master节点安装  参考第一个节点,配置仓库,安装 origin-master 软件包\n 将第一个master节点的 ca配置和相关配置文件拷贝到其他master节点上\nscp ca.* master.etcd-* htpasswd openshift-master2:/etc/origin/master  在其他2个节点重新生成配置\nopenshift start master --public-master='https://openshift.ops.com' --network-plugin='redhat/openshift-ovs-subnet' --write-config=/etc/origin/master  参考上文设置 超级admin用户登陆配置\n 启动master节点即可\n 检查日志,无错误,安装完成\n  "
},
{
	"uri": "/openshift/install/haproxy_install/",
	"title": "Haproxy安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    部署主机说明    主机角色 IP地址 操作系统 摘要 域名     管理节点(Master) openshift-master1(192.168.124.22) CentOS Linux release 7.3.1611 (Core) x86_64 master + etcd + haproxy openshift.ops.com    安装haproxy [root@openshift-master1 /root]# yum install haproxy -y   修改配置文件  [root@openshift-master1 /root]# vim /etc/haproxy/haproxy.cfg # Global settings #--------------------------------------------------------------------- global maxconn 20000 log /dev/log local0 info chroot /var/lib/haproxy pidfile /var/run/haproxy.pid user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats #--------------------------------------------------------------------- # common defaults that all the 'listen' and 'backend' sections will # use if not designated in their block #--------------------------------------------------------------------- defaults mode http log global option httplog option dontlognull # option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 300s timeout server 300s timeout http-keep-alive 10s timeout check 10s maxconn 20000 listen stats :9000 mode http stats enable stats uri / frontend atomic-openshift-api bind *:443 default_backend atomic-openshift-api mode tcp option tcplog backend atomic-openshift-api balance source mode tcp server master0 192.168.124.22:8443 check server master1 192.168.124.23:8443 check server master2 192.168.124.24:8443 check  注意: 确保域名 openshift.ops.com 指向Haproxy (如没配置dns,请在各计算节点做hosts绑定)\n启动服务 systemctl enable haproxy systemctl start haproxy  "
},
{
	"uri": "/openshift/install/node_install/",
	"title": "计算节点安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    部署主机角色说明    主机角色 IP地址 域名     基础设施节点(Node) openshift-node1（192.168.124.30） lb.openshift.ops.com   计算节点(Node) openshift-node2（192.168.124.46） 无对外域名    安装node节点(计算节点)  设置仓库源（参考主服务安装）\n 安装docker\nyum install docker -y  配置docker\n  [root@openshift-node1 node]# cat /etc/docker/daemon.json { \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;http://ef017c13.m.daocloud.io\u0026quot;], \u0026quot;insecure-registries\u0026quot;: [ \u0026quot;172.30.0.0/16\u0026quot;,\u0026quot;172.30.102.47:5000\u0026quot;,\u0026quot;openshift-master1:5000\u0026quot;] }  注意: insecure-registries 代表docker 使用http 而不使用https(默认是https方式)访问 docker镜像仓库\n 启动docker  systemctl enable docker systemctl start docker   安装 node 软件包\nyum install origin-node origin-sdn-ovs -y  生成 node 配置文件(默认是没有的) 将master的 ca配置文件 拷贝到 node 节点 (master节点上操作)\ncd /etc/origin/master/ scp ca.crt ca.key ca.serial.txt openshift-node1:/etc/origin/node/  在node节点上 生成配置(node节点上操作)\n  oc adm create-node-config \\ --node-dir=/etc/origin/node \\ --node=openshift-node1 \\ --hostnames=openshift-node1,192.168.124.30 \\ --certificate-authority=\u0026quot;/etc/origin/node/ca.crt\u0026quot; \\ --signer-cert=\u0026quot;/etc/origin/node/ca.crt\u0026quot; \\ --signer-key=\u0026quot;/etc/origin/node/ca.key\u0026quot; \\ --signer-serial=\u0026quot;/etc/origin/node/ca.serial.txt\u0026quot; \\ --node-client-certificate-authority=\u0026quot;/etc/origin/node/ca.crt\u0026quot; \\ --network-plugin=\u0026quot;redhat/openshift-ovs-subnet\u0026quot; \\ --dns-ip='172.30.0.1' \\ --master='https://openshift.ops.com'  注意: 如下设置要改成对应计算节点的信息\n--node=openshift-node1 \\ --hostnames=openshift-node1,192.168.124.30 \\   启动node节点(确保openshhift.ops.com能够解析或者hosts绑定)  systemctl enable origin-node systemctl start origin-node   检查日志和到服务端确认node注册成功  [root@openshift-master master]# oc get node NAME STATUS AGE VERSION openshift-node1 Ready 44s v1.6.1+5115d708d7 [root@openshift-master master]# oc get hostsubnet NAME HOST HOST IP SUBNET openshift-node1 openshift-node1 192.168.124.30 10.128.0.0/23  "
},
{
	"uri": "/openshift/install/router_registry_install/",
	"title": "Router And Registry安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    部署主机角色说明    主机角色 IP地址 节点标签 域名     基础设施节点(Node) openshift-node1（192.168.124.30) zone=ops lb.openshift.ops.com    说明:\nrouter 组件是用户访问的入口，域名都需要指向Router组件所在运行的计算节点上\nregistry组件是openshift集群内部使用的docker仓库,主要存放源代码打包生成的镜像\n部署 Router 组件  给Node节点打标签  [root@openshift-master master]# oc label node openshift-node1 zone=ops node \u0026quot;openshift-node1\u0026quot; labeled [root@openshift-master master]# oc get node --show-labels NAME STATUS AGE VERSION LABELS openshift-node1 Ready 3h v1.6.1+5115d708d7 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=openshift-node1,zone=ops openshift-node2 Ready 31m v1.6.1+5115d708d7 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=openshift-node2  通过对Node节点打标签，在部署组件的时候，可以指定部署到特定节点上\n 建立一个 service account 关联router 并赋予权限  [root@openshift-master ~]# oc project default [root@openshift-master ~]# oadm policy add-scc-to-user privileged system:serviceaccount:default:router [root@openshift-master ~]# oadm policy add-cluster-role-to-user cluster-reader system:serviceaccount:default:router cluster role \u0026quot;cluster-reader\u0026quot; added: \u0026quot;system:serviceaccount:default:router\u0026quot;   创建一个名为 router01 的实例，在指定的计算节点上  [root@openshift-master ~]# oadm router router01 --replicas=1 --service-account=router --selector='zone=ops' info: password for stats user admin has been set to iC3sKtFY5k --\u0026gt; Creating router router01 ... serviceaccount \u0026quot;router\u0026quot; created clusterrolebinding \u0026quot;router-router01-role\u0026quot; created deploymentconfig \u0026quot;router01\u0026quot; created service \u0026quot;router01\u0026quot; created --\u0026gt; Success 查看状态 [root@openshift-master ~]# oc get pod -n default NAME READY STATUS RESTARTS AGE router01-1-deploy 0/1 ContainerCreating 0 1m 正在下载docker images 过几分钟再看(取决于下载速度) [root@openshift-master ~]# oc get pod -n default NAME READY STATUS RESTARTS AGE router01-1-hvc1j 1/1 Running 0 1m  部署registry [root@openshift-master ~]# oadm registry --config=/etc/origin/master/admin.kubeconfig --service-account=registry --selector='zone=ops' --\u0026gt; Creating registry registry ... serviceaccount \u0026quot;registry\u0026quot; created clusterrolebinding \u0026quot;registry-registry-role\u0026quot; created deploymentconfig \u0026quot;docker-registry\u0026quot; created service \u0026quot;docker-registry\u0026quot; created --\u0026gt; Success [root@openshift-master ~]# oc get pod NAME READY STATUS RESTARTS AGE docker-registry-1-deploy 0/1 ContainerCreating 0 3s router01-1-hvc1j 1/1 Running 0 14h [root@openshift-master ~]# oc get pod NAME READY STATUS RESTARTS AGE docker-registry-1-k5zq1 1/1 Running 0 3m router01-1-hvc1j 1/1 Running 0 14h  查看各个SERVICE的内部集群地址 [root@openshift-master ~]# oc get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE docker-registry 172.30.111.126 \u0026lt;none\u0026gt; 5000/TCP 9m kubernetes 172.30.0.1 \u0026lt;none\u0026gt; 443/TCP,53/UDP,53/TCP 22h router01 172.30.121.139 \u0026lt;none\u0026gt; 80/TCP,443/TCP,1936/TCP 15h  注意: 172.30.0.0/16 这个段是 cluster ip,如果容器出现问题或者迁移，这个 cluster ip 是不会改变的\n"
},
{
	"uri": "/openshift/install/metric_install/",
	"title": "度量系统安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    度量采集服务架构图 使用ansible进行部署  安装openshift ansible 软件包\nyum install ansible.noarch java-1.8.0-openjdk-headless -y  clone openshift-ansible 项目\n  cd /opt git clone https://github.com/openshift/openshift-ansible.git git checkout origin/release-3.6   配置metric_hosts部署文件  [root@openshift-master ~]# cat /etc/ansible/metric_hosts [OSEv3:children] masters nodes [OSEv3:vars] ansible_ssh_user=root openshift_deployment_type=origin openshift_metrics_install_metrics=True openshift_metrics_image_prefix=openshift/origin- #拉取的镜像版本 openshift_metrics_image_version=v3.6.1 openshift_metrics_resolution=10s #metrics_hawkular服务对外的域名(指向router节点) openshift_metrics_hawkular_hostname=metrics.ops.com #master的ip #if you have a custom install in which the Kubernetes master is not available under https://kubernetes.default.svc:443 #you can specify the value to use instead with the openshift_metrics_master_url parameter #因此 我们这里不需要指定 #openshift_metrics_master_url=https://openshift.ops.com # cassandra 使用临时存储 openshift_metrics_cassandra_limits_memory=1G openshift_metrics_cassandra_storage_type=emptydir [masters] openshift-master [nodes] openshift-node1 openshift-node2   执行ansible-playbook(第一次执行可能会失败，如果失败就再执行一次)  ansible-playbook -i /etc/ansible/metric_hosts /opt/openshift-ansible/playbooks/byo/openshift-cluster/openshift-metrics.yml   执行结果，部署的快慢根据由拉取镜像的速度决定，成功的结果如下(READY都是1/1表示成功)\n# oc get pod -n openshift-infra NAME READY STATUS RESTARTS AGE hawkular-cassandra-1-m1p40 1/1 Running 0 4m hawkular-metrics-lf9hb 1/1 Running 0 4m heapster-1c2h0 1/1 Running 0 4m  检查master-config.yaml配置\ncat /etc/origin/master/master-config.yaml ...... masterPublicURL: https://openshift.ops.com:443 metricsPublicURL: https://metrics.ops.com/hawkular/metrics #确认此选项配置 publicURL: https://openshift.ops.com:443/console/ ....  重启master\nsystemct restart origin-master   成功后查看监控信息 "
},
{
	"uri": "/openshift/install/efk_install/",
	"title": "EFK日志系统安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-17 创建 开源方案 初始版本    部署日志采集服务  因为每个节点都需要采集日志，所以新建的logging项目选择的标签为空。\n[root@hz01-prod-ops-openshiftmaster-01 /]# oc adm new-project logging --node-selector=\u0026quot;\u0026quot;  修改ansible配置文件\n  cat /etc/ansible/log_hosts [OSEv3:children] masters nodes [OSEv3:vars] ansible_ssh_user=root openshift_deployment_type=origin openshift_logging_install_logging=True openshift_logging_image_version=v3.6.1 openshift_logging_kibana_hostname=kibana.ops.com #The URL for the Kubernetes master, this does not need to be public facing but should be accessible from within the cluster. #default is https://kubernetes.default.svc.cluster.local #openshift_logging_master_url=https://openshift.ops.com #The public facing URL for the Kubernetes master. This is used for Authentication redirection by the Kibana proxy. openshift_logging_master_public_url=https://openshift.ops.com openshift_logging_es_memory_limit=1G [masters] openshift-master [nodes] openshift-node1 openshift-node2   执行安装日志采集服务\nansible-playbook -i /etc/ansible/log_hosts /opt/openshift-ansible/playbooks/byo/openshift-cluster/openshift-logging.yml  确认部署是否成功\n[root@hz01-prod-ops-openshiftmaster-01 /root] # oc get pod NAME READY STATUS RESTARTS AGE logging-curator-1-01khs 1/1 Running 0 1d logging-es-data-master-f9r7g76t-1-bpvkq 1/1 Running 0 1d logging-fluentd-05cr8 1/1 Running 0 1d logging-fluentd-wc8ht 1/1 Running 0 1d logging-kibana-1-h4rl1 2/2 Running 0 1d   至此集中日志功能部署完成\nThe logs for the default,openshift, and openshift-infra projects are automatically aggregated and grouped into the .operations item in the Kibana interface. The project where you have deployed the EFK stack (logging, as documented here) is not aggregated into .operations and is found under its ID.\nIf you set openshift_logging_use_ops to true in your inventory file, Fluentd is configured to split logs between the main Elasticsearch cluster and another cluster reserved for operations logs, which are defined as node system logs and the projects default, openshift, and openshift-infra.\n 注意: 这里没有配置ops集群,所以基础系统项目的日志从kibana ui上是看不到的\n可能碰到的问题 log-es 集群启动会出现如下错误(通过 oc logs 查看) [2017-11-01 15:10:02,491][INFO ][container.run ] Begin Elasticsearch startup script -- | [2017-11-01 15:10:02,498][INFO ][container.run ] Comparing the specified RAM to the maximum recommended for Elasticsearch... | [2017-11-01 15:10:02,499][INFO ][container.run ] Inspecting the maximum RAM available... | [2017-11-01 15:10:02,503][INFO ][container.run ] ES_HEAP_SIZE: '4096m' | [2017-11-01 15:10:02,506][INFO ][container.run ] Setting heap dump location /elasticsearch/persistent/heapdump.hprof | [2017-11-01 15:10:02,509][INFO ][container.run ] Checking if Elasticsearch is ready on https://localhost:9200 | Exception in thread \u0026quot;main\u0026quot; java.lang.IllegalArgumentException: Unknown Discovery type [kubernetes] | at org.elasticsearch.discovery.DiscoveryModule.configure(DiscoveryModule.java:100) | at \u0026lt;\u0026lt;\u0026lt;guice\u0026gt;\u0026gt;\u0026gt; | at org.elasticsearch.node.Node.\u0026lt;init\u0026gt;(Node.java:213) | at org.elasticsearch.node.Node.\u0026lt;init\u0026gt;(Node.java:140) | at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:143) | at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:194) | at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:286) | at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:45) | Refer to the log for complete error details.  这个有3种解决方法\n 将ansible的host配置文中中 openshift_logging_image_version=v3.6.1 改为 openshift_logging_image_version=v3.6\n 修改已有的 dc(部署配置) oc edit dc/logging-es-data-master-we71py7f 将如下行\nimage: docker.io/openshift/origin-logging-elasticsearch:v3.6.1  变更成\nimage: docker.io/openshift/origin-logging-elasticsearch:v3.6  原因是请查看 https://hub.docker.com/r/openshift/origin-logging-elasticsearch/tags/ v3.6 与 v3.6.1 相比,从更新时间上来说 v3.6 属于最新版本\n 使用开源方案提供的镜像(对比官方，确定稳定版本，打tag到v3.6.1)\nopenshift_logging_image_prefix=51knowinfo/origin- openshift_logging_image_version=v3.6.1   查看es索引情况 进入容器\noc rsh logging-es-data-master-bhbc13z0-1-hv85d  查看索引(.operations.* 索引存放的是系统日志)\nsh-4.2$ curl -s --cacert /etc/elasticsearch/secret/admin-ca --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key --max-time 30 https://localhost:9200/_cat/indices?v health status index pri rep docs.count docs.deleted store.size pri.store.size green open .kibana 1 0 1 0 3.1kb 3.1kb green open .searchguard.logging-es-data-master-im3sye06 1 0 5 0 32kb 32kb green open project.cboard.0f516fc0-4870-11e8-86f9-06d28000000c.2018.04.25 1 0 6914 0 2.8mb 2.8mb green open project.cboard.0f516fc0-4870-11e8-86f9-06d28000000c.2018.04.26 1 0 1328 0 711.9kb 711.9kb green open project.logging.4abce3a2-4618-11e8-9d6b-06d28000000c.2018.04.25 1 0 7550 0 4.2mb 4.2mb green open project.logging.4abce3a2-4618-11e8-9d6b-06d28000000c.2018.04.26 1 0 1434 0 993.1kb 993.1kb green open .operations.2018.04.26 1 0 3129 0 1.5mb 1.5mb green open .kibana.a94a8fe5ccb19ba61c4c0873d391e987982fbbd3 1 0 2 0 26.2kb 26.2kb green open .operations.2018.04.25 1 0 16708 0 8.4mb 8.4mb green open .kibana.c62973cc56845b0e473e9e3c40b6e1f0a84662ef 1 0 2 0 26.2kb 26.2kb  查看系统日志 2018.04.26\nsh-4.2$ curl -s --cacert /etc/elasticsearch/secret/admin-ca --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key --max-time 30 https://localhost:9200/.operations.2018.04.26/_search  "
},
{
	"uri": "/openshift/install/harbor_install/",
	"title": "Harbor Docker仓库安装",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-04-27 创建 开源方案 初始版本    Harbor 介绍 Harbor 是 Vmwar 公司开源的 企业级的 Docker Registry 管理项目\n它主要 提供 Dcoker Registry 管理UI，可基于角色访问控制, AD/LDAP 集成，日志审核等功能，完全的支持中文。\nHarbor 的所有组件都在 Dcoker 中部署，所以 Harbor 可使用 Docker Compose 快速部署。\n部署主机角色说明    主机角色 IP地址 操作系统 摘要     主节点(Master) hz01-prod-ops-harbor-01(172.16.8.228) CentOS Linux release 7.3.1611 (Core) x86_64 harbor安装及配置   从节点(Node) hz01-prod-ops-harbor-02(172.16.8.245） CentOS Linux release 7.3.1611 (Core) X86-64 主节点数据复制    harbor 部署  开源项目地址：https://github.com/vmware/harbor 官方安装说明：https://github.com/vmware/harbor/blob/master/docs/installation_guide.md 下载安装包并解压：  [root@hz01-prod-ops-harbor-02 /opt]# wget https://storage.googleapis.com/harbor-releases/release-1.4.0/harbor-online-installer-v1.4.0.tgz [root@hz01-prod-ops-harbor-02 /opt]# tar xvf harbor-online-installer-v1.4.0.tgz   安装docker-compose  [root@hz01-prod-ops-harbor-02 /opt/harbor]# yum install docker [root@hz01-prod-ops-harbor-02 /opt/harbor]# yum install python2-pip [root@hz01-prod-ops-harbor-02 /opt/harbor]# yum install docker-compose   修改镜像源,增加docker hub 的镜像  [root@hz01-prod-ops-harbor-01 /opt/harbor]# vim /etc/sysconfig/docker { \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;http://ef017c13.m.daocloud.io\u0026quot;] }   修改harbor配置(reg.ops.com域名指向harbor所安装的服务器)  [root@hz01-prod-ops-harbor-02 /opt/harbor]# vim /opt/harbor/harbor.cfg # hostname 设置访问地址，支持IP，域名，主机名，禁止设置127.0.0.1 hostname = reg.ops.com # 访问协议，可设置 http,https ui_url_protocol = http # harbor WEB UI登陆使用的密码 harbor_admin_password = Harbor12345 # 认证方式，这里支持多种认证方式，默认是 db_auth ，既mysql数据库存储认证。 # 这里还支持 ldap 以及 本地文件存储方式。 auth_mode = db_auth # mysql root 账户的 密码 db_password = root123 self_registration= on use_compressed_js= on max_job_workers= 3 verify_remote_cert= on customize_crt= on #这些需要修改的其他的参数可以保持默认   安装harbor  [root@hz01-prod-ops-harbor-02 /opt/harbor]# cd /opt/harbor [root@hz01-prod-ops-harbor-02 /opt/harbor]# ./install.sh [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------- harbor-adminserver /harbor/start.sh Up harbor-db /usr/local/bin/docker-entr ... Up 3306/tcp harbor-jobservice /harbor/start.sh Up harbor-log /bin/sh -c /usr/local/bin/ ... Up 127.0.0.1:1514-\u0026gt;10514/tcp harbor-ui /harbor/start.sh Up nginx nginx -g daemon off; Up 0.0.0.0:443-\u0026gt;443/tcp, 0.0.0.0:4443-\u0026gt;4443/tcp, 0.0.0.0:80-\u0026gt;80/tcp registry /entrypoint.sh serve /etc/ ... Up 5000/tcp   通过终端登陆镜像仓库  [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker login reg.ops.com Username: admin Password: Error response from daemon: Get https://reg.ops.com/v1/users/: dial tcp 172.16.8.245:443: getsockopt: connection refused #这里配置的是http，docker login默认走的是https. [root@hz01-prod-ops-harbor-02 /opt/harbor]# cat /etc/docker/daemon.json { \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;http://ef017c13.m.daocloud.io\u0026quot;], \u0026quot;insecure-registries\u0026quot;: [ \u0026quot;reg.ops.com\u0026quot;] } [root@hz01-prod-ops-harbor-02 /opt/harbor]# systemctl daemon-reload [root@hz01-prod-ops-harbor-02 /opt/harbor]# systemctl restart docker [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker login hz01-prod-ops-harbor-02.sysadmin.xinguangnet.com Username: admin Password: Login Succeeded  推送镜像到harbor 登陆harbor，创建一个test测试的项目:\n#公网上随便拉个镜像 [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker pull mongo [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/mongo latest 5b1317f8158f 7 days ago 365.9 MB [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker tag mongo hz01-prod-ops-harbor-02.sysadmin.xinguangnet.com/test/mongodb:1.0 [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/mongo latest 5b1317f8158f 7 days ago 365.9 MB hz01-prod-ops-harbor-02.sysadmin.xinguangnet.com/test/mongodb 1.0 5b1317f8158f 7 days ago 365.9 MB [root@hz01-prod-ops-harbor-02 /opt/harbor]# docker push hz01-prod-ops-harbor-02.sysadmin.xinguangnet.com/test/mongodb:1.0 The push refers to a repository [hz01-prod-ops-harbor-02.sysadmin.xinguangnet.com/test/mongodb] 99099bc0f52d: Pushed 5388bfbc2c01: Pushed d6ac487f7716: Pushed 2ecbdcef31f1: Pushed 4786aaf122f1: Pushed b597eb624250: Pushed d1a481118c6e: Pushed 217a81d3bde9: Pushed 54e8db6ab32d: Pushed 43efe85a991c: Pushed 1.0: digest: sha256:82fb1f2483179a7c26ac603d5ad0f9cf6992a27f272c82e277371a96657b799b size: 2407  配置docker镜像复制  登陆master节点的web ui  根据上文创建一个openshift的项目，这里不做演示了。\n 选择仓库管理，创建从节点的信息   填写node节点的信息，并测试连接   连接成功后，仓库管理会生成一条信息   点击复制管理，添加一条复制策略   新建复制规则，主要是复制源项目，目标节点，触发模式，之后选择保存   复制管理会生成一条oepnshift复制的规则   测试镜像复制策略是否生效  #推送一个镜像到openshift项目 [root@hz01-prod-ops-harbor-01 /root]# docker tag docker.io/mongo 172.16.8.228/openshift/mongodb:1.0 [root@hz01-prod-ops-harbor-01 /root]# docker push 172.16.8.228/openshift/mongodb:1.0 The push refers to a repository [172.16.8.228/openshift/mongodb] 99099bc0f52d: Pushed 5388bfbc2c01: Pushed d6ac487f7716: Pushed 2ecbdcef31f1: Pushed 4786aaf122f1: Pushed b597eb624250: Pushed d1a481118c6e: Pushed 217a81d3bde9: Pushed 54e8db6ab32d: Pushed 43efe85a991c: Pushed 1.0: digest: sha256:82fb1f2483179a7c26ac603d5ad0f9cf6992a27f272c82e277371a96657b799b size: 2407   在主节点web ui查看，生成了一条复制任务！   在从节点web ui查看，已经从主节点把镜像复制过来了  数据库备份 #根据文件定义数据文件放在/data/database/目录下 [root@hz01-prod-ops-harbor-01 /opt/harbor]# vim docker-compose.yml mysql: image: vmware/harbor-db:v1.4.0 container_name: harbor-db restart: always volumes: - /data/database:/var/lib/mysql:z [root@hz01-prod-ops-harbor-01 /data/database]# ls /data/database/ aria_log.00000001 aria_log_control created_in_mariadb.flag ib_buffer_pool ibdata1 ib_logfile0 ib_logfile1 ibtmp1 multi-master.info mysql performance_schema registry tc.log  "
},
{
	"uri": "/openshift/application/s2i_custom/",
	"title": "S2I Build 镜像制作",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-05-02 创建 开源方案 初始版本    S2I(Source-to-Image) 说明  开发人员完成代码开发 S2I过程负责将代码和运行环境(比如 python 或者 Java)整合构建,生成应用docker镜像 应用镜像构建完成并推送至openshift集群内部的docker镜像仓库,再由openshift进行各个环境的部署  准备环境  在Master上下载S2I的二进制执行文件。\n# cd /opt # wget -c https://github.com/openshift/source-to-image/releases/download/v1.1.7/source-to-image-v1.1.7-226afa1-linux-amd64.tar.gz  解压到/usr/bin目录下\n# tar zxvf source-to-image-v1.1.7-226afa1-linux-amd64.tar.gz -C /usr/bin   创建项目目录  通过s2i create命令创建一个名为tomcat-s2i的S2I Builder镜像。 第二个参数ops-tomcat-s2i为S2I Builder镜像名称。第三个参数tomcat-s2i-catalog定义了工作目录的名称。\n# s2i create ops-tomcat-s2i tomcat-s2i-catalog  执行find tomcat-s2i-catalog查看目录。\n[root@openshift-master S2I]# find tomcat-s2i-catalog tomcat-s2i-catalog tomcat-s2i-catalog/s2i tomcat-s2i-catalog/s2i/bin tomcat-s2i-catalog/s2i/bin/assemble tomcat-s2i-catalog/s2i/bin/run tomcat-s2i-catalog/s2i/bin/usage tomcat-s2i-catalog/s2i/bin/save-artifacts tomcat-s2i-catalog/Dockerfile tomcat-s2i-catalog/README.md tomcat-s2i-catalog/test tomcat-s2i-catalog/test/test-app tomcat-s2i-catalog/test/test-app/index.html tomcat-s2i-catalog/test/run tomcat-s2i-catalog/Makefile  s2i目录下为S2I脚本\n     脚本名称 功能作用     assemble 负责源代码的编译、构建以及构建产出物的部署   run S2I流程生成的最终镜像将以这个脚本作为容器的启动命令   usage 打印帮助信息，一般作为S2I Builder镜像的启动命令   save-artifacts 为了实现增量构建，在构建过程中会执行此脚本保存中间构建产物。此脚本并不是必需的    开始项目工作 编写Dockerfile 编写一个制作Tomcat的S2I镜像。Dockerfile的内容如下：\n# ops-tomcat-s2i FROM openshift/base-centos7 # TODO: Put the maintainer name in the image metadata MAINTAINER fuhua \u0026lt;fuhua@xinguangnet.com\u0026gt; # TODO: Rename the builder environment variable to inform users about application you provide them ENV BUILDER_VERSION 1.0 # TODO: Set labels used in OpenShift to describe the builder image LABEL io.k8s.description=\u0026quot;Platform for building tomcat\u0026quot; \\ io.k8s.display-name=\u0026quot;builder tomcat\u0026quot; \\ io.openshift.expose-services=\u0026quot;8080:http\u0026quot; \\ io.openshift.tags=\u0026quot;builder,tomcat,java,openjdk7,etc.\u0026quot; # Setting local yum repo #RUN rm -fr /etc/yum.repos.d/ #ADD ./local-mirror.repo /etc/yum.repos.d/ # TODO: Install required packages here: RUN yum install -y java-1.7.0-openjdk git wget\u0026amp;\u0026amp; yum clean all -y # TODO (optional): Copy the builder files into /opt/app-root # COPY ./\u0026lt;builder_folder\u0026gt;/ /opt/app-root/ # Dwonload And install Tomcat7 RUN mkdir -p /opt/app-root/ \u0026amp;\u0026amp; cd /opt/app-root/ \u0026amp;\u0026amp; wget http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-7/v7.0.82/bin/apache-tomcat-7.0.82.tar.gz RUN cd /opt/app-root/ \u0026amp;\u0026amp; tar -xzvf apache-tomcat-7.0.82.tar.gz RUN cd /opt/app-root/ \u0026amp;\u0026amp; rm -fr apache-tomcat-7.0.82.tar.gz \u0026amp;\u0026amp; ln -s apache-tomcat-7.0.82 tomcat7 RUN cd /opt/app-root/ \u0026amp;\u0026amp; rm -fr tomcat7/webapps/* # Download and install maven RUN mkdir -p /opt/app-root/ \u0026amp;\u0026amp; cd /opt/app-root/ \u0026amp;\u0026amp; wget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.5.2/binaries/apache-maven-3.5.2-bin.tar.gz RUN cd /opt/app-root/ \u0026amp;\u0026amp; tar -xzvf apache-maven-3.5.2-bin.tar.gz RUN cd /opt/app-root/ \u0026amp;\u0026amp; rm -fr apache-maven-3.5.2-bin.tar.gz \u0026amp;\u0026amp; ln -s apache-maven-3.5.2 maven RUN ln -s /opt/app-root/maven/bin/mvn /usr/bin/mvn # TODO: Copy the S2I scripts to /usr/libexec/s2i, since openshift/base-centos7 image # sets io.openshift.s2i.scripts-url label that way, or update that label COPY ./s2i/bin/ /usr/libexec/s2i # TODO: Drop the root user and make the content of /opt/app-root owned by user 1001 # RUN chown -R 1001:1001 /opt/app-root RUN chown -R 1001:0 /opt/app-root \u0026amp;\u0026amp; chown -R 1001:0 $HOME \u0026amp;\u0026amp; \\ chmod -R ug+rw /opt/app-root # This default user is created in the openshift/base-centos7 image USER 1001 # TODO: Set the default port for applications built using this image EXPOSE 8080 # TODO: Set the default CMD for the image CMD [\u0026quot;/usr/libexec/s2i/usage\u0026quot;]  注意: 通过USER 1001定义了一个新用户，并指定该用户为容器的启动用户。以root用户作为启动用户在某些情况下存在安全风险\n编辑s2i/bin/assemble脚本（负责源代码的编译、构建以及构建产出物的部署）。 在脚本最末尾添加如下代码：\nmvn -Dmaven.test.skip=true package find . -type f -name '*.war'|xargs -i cp {} /opt/app-root/tomcat7/webapps/ROOT.war mvn clean  这段代码会触发一次Maven构建，并将构建产生的WAR包拷贝到Tomcat服务器的webapps目录下进行部署。 完整的assemble脚本如下：\n[root@openshift-master s2i]# cat bin/assemble #!/bin/bash -e # # S2I assemble script for the 'ops-tomcat-s2i' image. # The 'assemble' script builds your application source so that it is ready to run. # # For more information refer to the documentation: # https://github.com/openshift/source-to-image/blob/master/docs/builder_image.md # # If the 'ops-tomcat-s2i' assemble script is executed with the '-h' flag, print the usage. if [[ \u0026quot;$1\u0026quot; == \u0026quot;-h\u0026quot; ]]; then exec /usr/libexec/s2i/usage fi # Restore artifacts from the previous build (if they exist). # if [ \u0026quot;$(ls /tmp/artifacts/ 2\u0026gt;/dev/null)\u0026quot; ]; then echo \u0026quot;---\u0026gt; Restoring build artifacts...\u0026quot; mv /tmp/artifacts/. ./ fi echo \u0026quot;---\u0026gt; Installing application source...\u0026quot; cp -Rf /tmp/src/. ./ echo \u0026quot;---\u0026gt; Building application from source...\u0026quot; # TODO: Add build steps for your application, eg npm install, bundle install, pip install, etc. mvn -Dmaven.test.skip=true package find . -type f -name '*.war'|xargs -i cp {} /opt/app-root/tomcat7/webapps/ROOT.war mvn clean  编辑s2i/bin/run脚本（S2I流程生成的最终镜像将以这个脚本作为容器的启动命令）。 替换为以下内容：\nexec /opt/app-root/tomcat7/bin/catalina.sh run  执行镜像构建 [root@openshift-master tomcat-s2i-catalog]# make docker build -t ops-tomcat-s2i . Sending build context to Docker daemon 23.04 kB Step 1 : FROM openshift/base-centos7 ---\u0026gt; 4842f0bd3d61 Step 2 : MAINTAINER fuhua \u0026lt;fuhua@xinguangnet.com\u0026gt; ---\u0026gt; Using cache ---\u0026gt; bfc335ce78e5 Step 3 : ENV BUILDER_VERSION 1.0 ---\u0026gt; Using cache ---\u0026gt; ecc162dc5cd5 Step 4 : LABEL io.k8s.description \u0026quot;Platform for building tomcat\u0026quot; io.k8s.display-name \u0026quot;builder tomcat\u0026quot; io.openshift.expose-services \u0026quot;8080:http\u0026quot; io.openshift.tags \u0026quot;builder,tomcat,java,openjdk7,etc.\u0026quot; ---\u0026gt; Using cache ---\u0026gt; cfa61aa14bb4 Step 5 : RUN yum install -y java-1.7.0-openjdk git wget\u0026amp;\u0026amp; yum clean all -y ---\u0026gt; Using cache ---\u0026gt; baf7452d6f3d Step 6 : RUN mkdir -p /opt/app-root/ \u0026amp;\u0026amp; cd /opt/app-root/ \u0026amp;\u0026amp; wget http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-7/v7.0.82/bin/apache-tomcat-7.0.82.tar.gz ---\u0026gt; Using cache ---\u0026gt; bda3da428f9f Step 7 : RUN cd /opt/app-root/ \u0026amp;\u0026amp; tar -xzvf apache-tomcat-7.0.82.tar.gz ---\u0026gt; Using cache ---\u0026gt; 8b9288b65eb6 Step 8 : RUN cd /opt/app-root/ \u0026amp;\u0026amp; rm -fr apache-tomcat-7.0.82.tar.gz \u0026amp;\u0026amp; ln -s apache-tomcat-7.0.82 tomcat7 ---\u0026gt; Using cache ---\u0026gt; 95e3f4fc9f01 Step 9 : RUN cd /opt/app-root/ \u0026amp;\u0026amp; rm -fr tomcat7/webapps/* ---\u0026gt; Using cache ---\u0026gt; 6977f7726e29 Step 10 : RUN mkdir -p /opt/app-root/ \u0026amp;\u0026amp; cd /opt/app-root/ \u0026amp;\u0026amp; wget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.5.2/binaries/apache-maven-3.5.2-bin.tar.gz ---\u0026gt; Using cache ---\u0026gt; b630e55722e8 Step 11 : RUN cd /opt/app-root/ \u0026amp;\u0026amp; tar -xzvf apache-maven-3.5.2-bin.tar.gz ---\u0026gt; Using cache ---\u0026gt; 048259dcf249 Step 12 : RUN cd /opt/app-root/ \u0026amp;\u0026amp; rm -fr apache-maven-3.5.2-bin.tar.gz \u0026amp;\u0026amp; ln -s apache-maven-3.5.2 maven ---\u0026gt; Using cache ---\u0026gt; 4b55059b8437 Step 13 : RUN ln -s /opt/app-root/maven/bin/mvn /usr/bin/mvn ---\u0026gt; Using cache ---\u0026gt; 6981f2ef18e0 Step 14 : COPY ./s2i/bin/ /usr/libexec/s2i ---\u0026gt; Using cache ---\u0026gt; ce773a99ab69 Step 15 : RUN groupadd -g 1002 tomcat \u0026amp;\u0026amp; useradd -ms /bin/bash -u 1002 -g 1002 tomcat \u0026amp;\u0026amp; chown -R 1002:1002 /opt/app-root \u0026amp;\u0026amp; chmod -R a+w /opt/app-root ---\u0026gt; Using cache ---\u0026gt; 1141afc04321 Step 16 : USER 1002 ---\u0026gt; Using cache ---\u0026gt; 704361ace73c Step 17 : EXPOSE 8080 ---\u0026gt; Using cache ---\u0026gt; 2530fdc4612f Step 18 : CMD /usr/libexec/s2i/usage ---\u0026gt; Using cache ---\u0026gt; 884edf911bcc Successfully built 884edf911bcc  导入镜像(reg.ops.com)  需要将 reg.ops.com 加入到 非安全连接列表中\n[root@openshift-master tomcat-s2i-catalog]# cat /etc/docker/daemon.json { \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;http://ef017c13.m.daocloud.io\u0026quot;], \u0026quot;insecure-registries\u0026quot;: [\u0026quot;reg.ops.com\u0026quot;] }  测试build镜像(这个镜像是jdk1.7)\ns2i build https://github.com/nichochen/mybank-demo-maven.git ops-tomcat-s2i ops-tomcat-s2i-app  使用新构建出的应用镜像启动\ndocker run -it -p 8080:8080 ops-tomcat-s2i-app  正常应该能看到tomcat的启动日志\n 推送到自己仓库 这里推送到 reg.ops.com 的registry仓库\n[root@openshift-master tomcat-s2i-catalog]# docker tag ops-tomcat-s2i:latest reg.ops.com/opssolution/ops-tomcat-s2i:latest [root@openshift-master tomcat-s2i-catalog]# docker push reg.ops.com/opssolution/ops-tomcat-s2i:latest The push refers to a repository [reg.ops.com/opssolution/ops-tomcat-s2i] b9b8ffca1e5a: Pushed be79645180dc: Pushed 3f84a2b90659: Pushed 04b90df78611: Pushed 0c56c93ac2a7: Pushed 50596ec3b0e4: Pushed bdac80fbd115: Pushed 7842c35bf593: Pushed fa84fac3a7ce: Pushed 4cec9f30e5a2: Pushed 10811061f79a: Pushed cb96aea742c3: Pushed f1bbaf33b49c: Pushed 4b1e8db0189a: Pushed 34e7b85d83e4: Pushed latest: digest: sha256:097d24d53e4f5abc1fe25dfba4914603c3b6027e01a5ad51e4eb6e69bbdcd22d size: 3461   注意: 需要在harbor 镜像仓库中先创建 opssolution 项目\n如果用自己的reg.ops.com的镜像仓库 需要修改master配置文件\nimagePolicyConfig: allowedRegistriesForImport: - domainName: docker.io - domainName: '*.docker.io' - domainName: '*.redhat.com' - domainName: gcr.io - domainName: quay.io - domainName: '*.amazonaws.com' disableScheduledImport: false maxImagesBulkImportedPerRepository: 5 maxScheduledImageImportsPerMinute: 60 scheduledImageImportMinimumIntervalSeconds: 900  在这里把 reg.ops.com 加入到上述配置中,这样才能导入镜像 在这里为了方便 镜像也推送到了 https://hub.docker.com/r/51knowinfo/ops-tomcat7-jdk7-s2i\n编写S2I Builder镜像模板 在此下载模板文件 tomcat-imagestream.json\n模板文件中主要注意annotations中的内容\n{ \u0026quot;name\u0026quot;: \u0026quot;Tomcat 7\u0026quot;, \u0026quot;annotations\u0026quot;: { \u0026quot;openshift.io/display-name\u0026quot;: \u0026quot;Tomcat7 (Latest)\u0026quot;, \u0026quot;openshift.io/provider-display-name\u0026quot;: \u0026quot;OpsSolution, Inc.\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;在Centos7上编译和运行tomcat应用. 更多关于此构建镜像(S2I)信息,请参考 https://github.com/openshift-s2i/s2i-wildfly/blob/master/README.md.\\n\\n警告: 使用此版本,你的应用将使用最新版本的tomca7, 包含主要版本更新\u0026quot;, \u0026quot;iconClass\u0026quot;: \u0026quot;icon-tomcat\u0026quot;, \u0026quot;tags\u0026quot;: \u0026quot;builder,tomcat,java,51know\u0026quot;, \u0026quot;supports\u0026quot;:\u0026quot;jee,java\u0026quot;, \u0026quot;sampleRepo\u0026quot;: \u0026quot;https://github.com/nichochen/mybank-demo-maven.git\u0026quot; }, \u0026quot;from\u0026quot;: { \u0026quot;kind\u0026quot;: \u0026quot;DockerImage\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;51knowinfo/ops-tomcat7-jdk7-s2i:latest\u0026quot; } }  注意: tags 标签项要特别注意，用于catalog 分类用\n查看刚刚导入的镜像 [root@openshift-master ~]# oc get is -n openshift | grep ops-tomcat-s2i ops-tomcat-s2i 172.30.188.209:5000/openshift/ops-tomcat-s2i latest 2 minutes ago  登录web console,使用新的s2i builder 镜像 直到构建镜像成功，会自动部署pod\n删除默认router,创建新的router, app.ops.com(指向router所运行计算节点的ip),访问效果如下 至此一个自定义版本的Tomcat S2I 就完成了\n"
},
{
	"uri": "/openshift/install/",
	"title": "集群安装",
	"tags": [],
	"description": "",
	"content": " 第一章 生产集群安装 本章主要讲述 Openshift Origin 安装配置步骤\n"
},
{
	"uri": "/openshift/application/",
	"title": "应用部署",
	"tags": [],
	"description": "",
	"content": " 第二章 应用部署 本章主要讲述 openshift origin 平台上应用部署相关配置实践\n"
},
{
	"uri": "/openshift/monitor/",
	"title": "平台监控",
	"tags": [],
	"description": "",
	"content": " 第三章 平台监控 本章主要讲述 openshift origin 集群监控相关配置实践\n"
},
{
	"uri": "/openshift/appcenter/",
	"title": "组件中心",
	"tags": [],
	"description": "",
	"content": " 第四章 组件中心 本章主要讲述如何构建自己企业内部的组件或者应用中心的相关配置实践\n"
},
{
	"uri": "/openshift/security/",
	"title": "平台安全",
	"tags": [],
	"description": "",
	"content": " 第五章 平台安全 本章主要讲述Openshift Origin 平台信息安全配置等实践\n"
},
{
	"uri": "/openshift/devops/",
	"title": "开发运维",
	"tags": [],
	"description": "",
	"content": " 第六章 DevOps 本章主要主要讲述DevOps在Openshift Origin 平台上的相关实践\n"
},
{
	"uri": "/openshift/faq/",
	"title": "常见问题",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2018-05-02 创建 开源方案 初始版本    error: Build error: Failed to push image: After retrying 6 times, Push image still failed. 问题原因: 没有配置openshift sdn 网络\n 解决方法  所有计算节点都需要安装 origin-sdn-ovs 软件包 (如果master节点也做计算节点的话，也需要安装)\nyum install origin-sdn-ovs.x86_64 -y  修改 主节点(master) 和 计算节点(node)的配置文件（红色框框原来默认是空的） /etc/origin/master/master-config.yaml 重启master 节点\nsystemctl daemon-reload systemctl restart origin-master  修改所有计算节点的配置,与master设置一致 /etc/origin/node/node-config.yaml 重启计算节点(node)\nsystemctl daemon-reload systemctl restart origin-node  注意： master 节点网络配置 和 计算节点(node) 一定要一致\n 通过在master 节点执行如下命令查看分配到各个节点的子网\n[root@openshift-master ~]# oc get hostsubnets NAME HOST HOST IP SUBNET openshift-node1 openshift-node1 192.168.124.30 10.128.0.0/23 openshift-node2 openshift-node2 192.168.124.46 10.129.0.0/23   "
},
{
	"uri": "/openshift/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/openshift/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/openshift/",
	"title": "红帽 OpenShift",
	"tags": [],
	"description": "",
	"content": " 红帽 OpenShift 红帽® OpenShift 是一款容器应用平台，它将 Docker 和 Kubernetes 技术带入企业。\n无论您采用何种应用架构，OpenShift 都能让您在任意架构中（公共或私有云中）轻松、快速实现应用的构建、开发和部署。\n无论是在企业内部，公共云，或是托管环境中，您都能凭借这一备受业务青睐的平台，快速您的最新创意推向市场，从而在激烈的市场竞争中脱颖而出。\n本套文档涉及到的版本    版本 类型 备注     CentOS Linux release 7.3.1611 (Core) 操作系统 使用centos,yum仓库默认配置EPEL仓库源   Openshift Origin v3.6.1 容器云 使用开源版本    [root@openshift-master ~]# oc version oc v3.6.1+008f2d5 kubernetes v1.6.1+5115d708d7 features: Basic-Auth GSSAPI Kerberos SPNEGO Server https://192.168.124.22:8443 openshift v3.6.1+008f2d5 kubernetes v1.6.1+5115d708d7  问题反馈 在看文档过程中，有任何问题，请到这里留言反馈，谢谢。\n我们会在第一时间响应回复\n openshift origin 使用客户列表 如果您在真实的生产工作中，使用了openshift origin, 请反馈给我们,也能看出在国内的使用情况,请到这里推荐给我们\nOpenshift 功能和优势  企业级 Kubernetes\n红帽 OpenShift 作为一个功能全面容器应用平台，其自身具备 Docker 和 Kubernetes 等最新技术，是一款功能强大的容器集群管理和编排系统。 该平台和红帽企业 Linux系统一同为企业提供坚实技术基础。 OpenShift 集成了企业所需的多种架构、流程、平台及服务，可为开发和运营团队同时提供强大支持。\n 有状态和无状态的应用\n红帽 OpenShift 可运行和支持有状态和无状态的应用。因此，您无需完全重构您的企业应用，即可充分利用容器技术。\n 实现应用现代化\nOpenShift 与红帽 JBoss 中间件相结合后，可以实现多种综合性云原生服务，包括开发人员工具、整合、业务自动化、数据管理等。因而，有助您更加快速、智能、灵活地开发应用，克服使用微服务构建分布式系统的挑战。\n 构建更出色、更强大的混合云\n就像红帽企业 Linux，这一用于构建众多公共、私有云环境的基础平台一样，您可以在任意位置部署 OpenShift 和获得相应支持。这些云环境包括 Amazon Web Services、Azure、Google 云平台、VMware 等等。依托 OpenShift，您可以在这些公共云和私有云上，提供单个容器应用平台。另外，借助在 Microsoft Azure 上运行的红帽 OpenShift 容器平台，您可以构建、部署和管理容器化服务和应用。\n 拥抱 DevOps\nOpenShift 为开发和运营团队提供了一个通用平台和一整套开发和管理工具。这使得两个团队能够步调一致，保持持续、协调化的应用开发和维护流程。通过此方法，您可以消除耗时缓慢的业务流程和手动操作，根据业务需求灵活开展运营。\n  "
}]