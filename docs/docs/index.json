[
{
	"uri": "/docs/devops/mysql/",
	"title": "Mysql常用用法总结",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2016-03-28 创建 HunterFu 创建文档    在日常工作中，会简单的使用一下mysql,故对常见命令操作总结一下，常用方式如下\nmysqldump 命令的使用  备份和导出数据库\nmysqldump -h database_ip -u Username -p --opt databasename \u0026gt; backup-file.sql  只导出数据库表结构\nmysqldump -h database_ip -d -u Username -p databasename \u0026gt;database_structure.sql  只导出数据库中的某个表\nmysqldump --opt --add-drop-table -u Username -p databasename tablename \u0026gt; dump.sql  如果不想手工输入密码 请使用--password 参数\nmysqldump -h database_ip -u Username --password=123456 --opt databasename \u0026gt; backup-file.sql mysqldump -h database_ip -d -u Username --password=123456 databasename \u0026gt;database_structure.sql  修改root用户密码\nmysqladmin -u root password \u0026quot;123456\u0026quot;   mysql 命令使用  将查询结果保存到文件\nselect title from book into outfile '/tmp/outfile.txt';  查找表中多余的重复记录，重复记录是根据某个字段（peopleId）来判断\nselect * from people where peopleId in (select peopleId from people group by peopleId having count(peopleId) \u0026gt; 1);  查询表中不重复记录(排除重复记录)\nselect * from phome_ecms_wma where title in (select distinct title from phome_ecms_wma);  删除表中重复记录,重复记录是根据某个字段（title）来判断\nselect *,count(distinct title) INTO OUTFILE '/tmp/table.bak' from phome_ecms_wma group by title; delete from phome_ecms_wma; LOAD DATA INFILE '/tmp/table.bak' REPLACE INTO TABLE phome_ecms_wma character set utf8;  随机选取记录\nmysql\u0026gt; SELECT *FROM url ORDER BY RAND() LIMIT 5;  查询数据库当前编码\nmysql\u0026gt; show variables like \u0026quot;character_set%\u0026quot;;  修改表字段类型\nmysql\u0026gt; alter table table_name change last_action last_action datetime NOT NULL default '0000-00-00 00:00:00';  给表添加一个新字段\nmysql\u0026gt; ALTER TABLE host ADD ks_mac VARCHAR(100);  从表中删除一个字段\nmysql\u0026gt; ALTER TABLE table_name DROP field_name;  重命名表\nmysql\u0026gt;alter table t1 rename t2;  给字段加索引\nmysql\u0026gt; alter table tablename add index 索引名 (字段名1[，字段名2 …]); mysql\u0026gt; alter table tablename add index emp_name (name);  加主关键字的索引\nmysql\u0026gt; alter table tablename add primary key(id);  加唯一限制条件的索引\nmysql\u0026gt; alter table tablename add unique emp_name2(cardnumber);  删除某个索引\nmysql\u0026gt;alter table tablename drop index emp_name;  远程访问mysql 设置(单独一个IP)\nmysql\u0026gt; CREATE DATABASE IF NOT EXISTS database_test; mysql\u0026gt; GRANT ALL PRIVILEGES ON database_test.* to root@192.168.1.9 IDENTIFIED BY '123456'; mysql\u0026gt; FLUSH PRIVILEGES;  授权远程访问(一个网段)\nmysql\u0026gt; grant all privileges on *.* to root@'192.168.124.%' identified by '123456'; mysql\u0026gt; FLUSH PRIVILEGES;   shell 命令中 mysql 使用 mysql -u root -p -e 'CREATE DATABASE IF NOT EXISTS database_test;' mysql -u root -p -e \u0026quot;GRANT ALL PRIVILEGES ON database_test.* to root@'%' IDENTIFIED BY '123456';\u0026quot;  "
},
{
	"uri": "/docs/cloudstack/",
	"title": "私有云",
	"tags": [],
	"description": "",
	"content": " 私有云 CloudStack CloudStack 本分类主要讲述 cloudstack 相关配置等实践\n"
},
{
	"uri": "/docs/devops/python_tips/",
	"title": "Python脚本知识总结",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2016-03-28 创建 HunterFu 创建文档   V1.1 2017-08-07 修订 HunterFu 增加 指定占位符宽度(左对齐)    数据对象持久化 在某些时候，需要将数据对象的内容保存下来，方便下次程序启动时读取，这个就需要将对象持久化，请看如下例子\nimport pickle # create the test dictionary before_d = {} before_d[1]=\u0026quot;Name 1\u0026quot; before_d[2]=\u0026quot;Name 2\u0026quot; before_d[3]=\u0026quot;Name 3\u0026quot; # pickle dump the dictionary fout = open(\u0026quot;dict1.dat\u0026quot;, \u0026quot;w\u0026quot;) pickle.dump(before_d, fout, protocol=0) fout.close() # pickle load the dictionary fin = open(\u0026quot;dict1.dat\u0026quot;, \u0026quot;r\u0026quot;) after_d = pickle.load(fin) fin.close() print( before_d ) # {1: 'Name 1', 2: 'Name 2', 3: 'Name 3'} print( after_d ) # {1: 'Name 1', 2: 'Name 2', 3: 'Name 3'}  可以看出，我们将数据对象内容以文件的方式保存，可以作一些简单的cache处理，尤其是在写一些比较小的程序时，非常有用\n正则表达式替换 目标: 将字符串line中的 overview.gif 替换成其他字符串\n\u0026gt;\u0026gt;\u0026gt; line = '\u0026lt;IMG ALIGN=\u0026quot;middle\u0026quot; SRC=\u0026quot;overview.gif\u0026quot; BORDER=\u0026quot;0\u0026quot; ALT=\u0026quot;\u0026quot;\u0026gt;' \u0026gt;\u0026gt;\u0026gt; mo=re.compile(r'(?\u0026lt;=SRC=)\u0026quot;([\\w+\\.]+)\u0026quot;',re.I) \u0026gt;\u0026gt;\u0026gt; mo.sub(r'\u0026quot;\\1****\u0026quot;',line) '\u0026lt;IMG ALIGN=\u0026quot;middle\u0026quot; SRC=\u0026quot;cdn_overview.gif****\u0026quot; BORDER=\u0026quot;0\u0026quot; ALT=\u0026quot;\u0026quot;\u0026gt;' \u0026gt;\u0026gt;\u0026gt; mo.sub(r'replace_str_\\1',line) '\u0026lt;IMG ALIGN=\u0026quot;middle\u0026quot; SRC=replace_str_overview.gif BORDER=\u0026quot;0\u0026quot; ALT=\u0026quot;\u0026quot;\u0026gt;' \u0026gt;\u0026gt;\u0026gt; mo.sub(r'\u0026quot;testetstset\u0026quot;',line) '\u0026lt;IMG ALIGN=\u0026quot;middle\u0026quot; SRC=\u0026quot;testetstset\u0026quot; BORDER=\u0026quot;0\u0026quot; ALT=\u0026quot;\u0026quot;\u0026gt;'  注意: 其中 \\1 是匹配到的数据，可以通过这样的方式直接引用\n遍历目录方法 在某些时候，我们需要遍历某个目录找出特定的文件列表，可以通过os.walk方法来遍历,非常方便\nimport os fileList = [] rootdir = \u0026quot;/tmp\u0026quot; for root, subFolders, files in os.walk(rootdir): if '.svn' in subFolders: subFolders.remove('.svn') # 排除特定目录 for file in files: if file.find(\u0026quot;.t2t\u0026quot;) != -1:\t# 查找特定扩展名的文件 file_dir_path = os.path.join(root,file) fileList.append(file_dir_path) print fileList  列表按列排序(list sort) 如果列表的每个元素都是一个元组(tuple),我们要根据元组的某列来排序的化，可参考如下方法\n下面例子我们是根据元组的第2列和第3列数据来排序的,而且是倒序(reverse=True)\n\u0026gt;\u0026gt;\u0026gt; a = [('2011-03-17', '2.26', 6429600, '0.0'), ('2011-03-16', '2.26', 12036900, '-3.0'), ('2011-03-15', '2.33', 15615500,'-19.1')] \u0026gt;\u0026gt;\u0026gt; print a[0][0] 2011-03-17 \u0026gt;\u0026gt;\u0026gt; b = sorted(a, key=lambda result: result[1],reverse=True) \u0026gt;\u0026gt;\u0026gt; print b [('2011-03-15', '2.33', 15615500, '-19.1'), ('2011-03-17', '2.26', 6429600, '0.0'), ('2011-03-16', '2.26', 12036900, '-3.0')] \u0026gt;\u0026gt;\u0026gt; c = sorted(a, key=lambda result: result[2],reverse=True) \u0026gt;\u0026gt;\u0026gt; print c [('2011-03-15', '2.33', 15615500, '-19.1'), ('2011-03-16', '2.26', 12036900, '-3.0'), ('2011-03-17', '2.26', 6429600, '0.0')]  列表去重(list uniq) 有时候需要将list中重复的元素删除，就要使用如下方法\n\u0026gt;\u0026gt;\u0026gt; lst= [(1,'sss'),(2,'fsdf'),(1,'sss'),(3,'fd')] \u0026gt;\u0026gt;\u0026gt; set(lst) set([(2, 'fsdf'), (3, 'fd'), (1, 'sss')]) \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; lst = [1, 1, 3, 4, 4, 5, 6, 7, 6] \u0026gt;\u0026gt;\u0026gt; set(lst) set([1, 3, 4, 5, 6, 7])  字典排序(dict sort) 一般来说，我们都是根据字典的key来进行排序，但是我们如果想根据字典的value值来排序，就使用如下方法\n\u0026gt;\u0026gt;\u0026gt; from operator import itemgetter \u0026gt;\u0026gt;\u0026gt; aa = {\u0026quot;a\u0026quot;:\u0026quot;1\u0026quot;,\u0026quot;sss\u0026quot;:\u0026quot;2\u0026quot;,\u0026quot;ffdf\u0026quot;:'5',\u0026quot;ffff2\u0026quot;:'3'} \u0026gt;\u0026gt;\u0026gt; sort_aa = sorted(aa.items(),key=itemgetter(1)) \u0026gt;\u0026gt;\u0026gt; sort_aa [('a', '1'), ('sss', '2'), ('ffff2', '3'), ('ffdf', '5')]  从上面的运行结果看到，按照字典的value值进行排序的\n字典,列表,字符串互转  以下是生成数据库连接字符串,从字典转换到字符串\n\u0026gt;\u0026gt;\u0026gt; params = {\u0026quot;server\u0026quot;:\u0026quot;mpilgrim\u0026quot;, \u0026quot;database\u0026quot;:\u0026quot;master\u0026quot;, \u0026quot;uid\u0026quot;:\u0026quot;sa\u0026quot;, \u0026quot;pwd\u0026quot;:\u0026quot;secret\u0026quot;} \u0026gt;\u0026gt;\u0026gt; [\u0026quot;%s=%s\u0026quot; % (k, v) for k, v in params.items()] ['server=mpilgrim', 'uid=sa', 'database=master', 'pwd=secret'] \u0026gt;\u0026gt;\u0026gt; \u0026quot;;\u0026quot;.join([\u0026quot;%s=%s\u0026quot; % (k, v) for k, v in params.items()]) 'server=mpilgrim;uid=sa;database=master;pwd=secret'  下面的例子 是将字符串转化为字典\n\u0026gt;\u0026gt;\u0026gt; a = 'server=mpilgrim;uid=sa;database=master;pwd=secret' \u0026gt;\u0026gt;\u0026gt; aa = {} \u0026gt;\u0026gt;\u0026gt; for i in a.split(';'):aa[i.split('=',1)[0]] = i.split('=',1)[1] ... \u0026gt;\u0026gt;\u0026gt; aa {'pwd': 'secret', 'database': 'master', 'uid': 'sa', 'server': 'mpilgrim'}   时间对象操作  将时间对象转换成字符串\n\u0026gt;\u0026gt;\u0026gt; import datetime \u0026gt;\u0026gt;\u0026gt; datetime.datetime.now().strftime(\u0026quot;%Y-%m-%d %H:%M\u0026quot;) '2011-01-20 14:05'  时间大小比较\n\u0026gt;\u0026gt;\u0026gt; import time \u0026gt;\u0026gt;\u0026gt; t1 = time.strptime('2011-01-20 14:05',\u0026quot;%Y-%m-%d %H:%M\u0026quot;) \u0026gt;\u0026gt;\u0026gt; t2 = time.strptime('2011-01-20 16:05',\u0026quot;%Y-%m-%d %H:%M\u0026quot;) \u0026gt;\u0026gt;\u0026gt; t1 \u0026gt; t2 False \u0026gt;\u0026gt;\u0026gt; t1 \u0026lt; t2 True  时间差值计算,计算8小时前的时间\n\u0026gt;\u0026gt;\u0026gt; datetime.datetime.now().strftime(\u0026quot;%Y-%m-%d %H:%M\u0026quot;) '2011-01-20 15:02' \u0026gt;\u0026gt;\u0026gt; (datetime.datetime.now() - datetime.timedelta(hours=8)).strftime(\u0026quot;%Y-%m-%d %H:%M\u0026quot;) '2011-01-20 07:03'  将字符串转换成时间对象\n\u0026gt;\u0026gt;\u0026gt; endtime=datetime.datetime.strptime('20100701',\u0026quot;%Y%m%d\u0026quot;) \u0026gt;\u0026gt;\u0026gt; type(endtime) \u0026lt;type 'datetime.datetime'\u0026gt; \u0026gt;\u0026gt;\u0026gt; print endtime 2010-07-01 00:00:00 \u0026gt;\u0026gt;\u0026gt;  将从 1970-01-01 00:00:00 UTC 到现在的秒数，格式化输出\n\u0026gt;\u0026gt;\u0026gt; import time \u0026gt;\u0026gt;\u0026gt; a = 1302153828 \u0026gt;\u0026gt;\u0026gt; time.strftime(\u0026quot;%Y-%m-%d %H:%M:%S\u0026quot;,time.localtime(a)) '2011-04-07 13:23:48'  将日期转换成秒数\n\u0026gt;\u0026gt;\u0026gt; endtime=datetime.datetime.strptime('20100701',\u0026quot;%Y%m%d\u0026quot;) \u0026gt;\u0026gt;\u0026gt; time.mktime(endtime.timetuple()) 1277913600.0  strptime | strftime 支持的时间格式化列表\n     格式化字符 意义     %a 星期几的简写 Weekday name, abbr.   %A 星期几的全称 Weekday name, full   %b 月分的简写 Month name, abbr.   %B 月份的全称 Month name, full   %c 标准的日期的时间串 Complete date and time representation   %d 十进制表示的每月的第几天 Day of the month   %H 24小时制的小时 Hour (24-hour clock)   %I 12小时制的小时 Hour (12-hour clock)   %j 十进制表示的每年的第几天 Day of the year   %m 十进制表示的月份 Month number   %M 十时制表示的分钟数 Minute number   %s seconds since 00:00:00 1970-01-01 UTC (a GNU extension)   %S 十进制的秒数 Second number   %U 第年的第几周，把星期日做为第一天（值从0到53）Week number (Sunday first weekday)   %w 十进制表示的星期几（值从0到6，星期天为0）weekday number   %W 每年的第几周，把星期一做为第一天（值从0到53） Week number (Monday first weekday)   %x 标准的日期串 Complete date representation (e.g. 13/01/08)   %X 标准的时间串 Complete time representation (e.g. 17:02:10)   %y 不带世纪的十进制年份（值从0到99）Year number within century   %Y 带世纪部分的十制年份 Year number   %Z 时区名称，如果不能得到时区名称则返回空字符。Name of time zone   %% 输出百分号    命令行参数解析(getopt) 通常在编写一些日运维脚本时，需要根据不同的条件，输入不同的命令行选项来实现不同的功能, 在Python中提供了getopt模块很好的实现了命令行参数的解析,下面距离说明。 请看如下程序:\n#!/usr/bin/env python # -*- coding: utf-8 -*- import sys,os,getopt def usage(): print ''' Usage: analyse_stock.py [options...] Options: -e : Exchange Name -c : User-Defined Category Name -f : Read stock info from file and save to db -d : delete from db by stock code -n : stock name -s : stock code -h : this help info test.py -s haha -n \u0026quot;HA Ha\u0026quot; ''' try: opts, args = getopt.getopt(sys.argv[1:],'he:c:f:d:n:s:') except getopt.GetoptError: usage() sys.exit() if len(opts) == 0: usage() sys.exit() for opt, arg in opts: if opt in ('-h', '--help'): usage() sys.exit() elif opt == '-d': print \u0026quot;del stock %s\u0026quot; % arg elif opt == '-f': print \u0026quot;read file %s\u0026quot; % arg elif opt == '-c': print \u0026quot;user-defined %s \u0026quot; % arg elif opt == '-e': print \u0026quot;Exchange Name %s\u0026quot; % arg elif opt == '-s': print \u0026quot;Stock code %s\u0026quot; % arg elif opt == '-n': print \u0026quot;Stock name %s\u0026quot; % arg sys.exit()  注意: 这里我们使用短格式分析串\u0026lsquo;he:c:f:d:n:s:\u0026rsquo;, 当一个选项只是表示开关状态时,即后面不带附加参数时,在分析串中写入选项字符\n当选项后面是带一个附加参数时，在分析串中写入选项字符同时后面加一个 : 冒号\n所以 \u0026lsquo;he:c:f:d:n:s:\u0026rsquo; 就表示 \u0026lsquo;h\u0026rsquo;是一个开关选项, \u0026lsquo;e:c:f:d:n:s:\u0026lsquo;则表示这些选项后面应该带一个参数.\nprint 格式化输出 格式化输出字符串  截取字符串输出,下面例子将只输出字符串的前3个字母\n\u0026gt;\u0026gt;\u0026gt; str=\u0026quot;abcdefg\u0026quot; \u0026gt;\u0026gt;\u0026gt; print \u0026quot;%.3s\u0026quot; % str abc  按固定宽度输出，不足使用空格补全,下面例子输出宽度为10\n\u0026gt;\u0026gt;\u0026gt; str=\u0026quot;abcdefg\u0026quot; \u0026gt;\u0026gt;\u0026gt; print \u0026quot;%10s\u0026quot; % str abcdefg  指定占位符宽度(左对齐)\n\u0026gt;\u0026gt;\u0026gt; print (\u0026quot;Name:%-10s Age:%-8d Height:%-8.2f\u0026quot;%(\u0026quot;Aviad\u0026quot;,25,1.83)) Name:Aviad Age:25 Height:1.83 \u0026gt;\u0026gt;\u0026gt;  截取字符串，按照固定宽度输出\n\u0026gt;\u0026gt;\u0026gt; str=\u0026quot;abcdefg\u0026quot; \u0026gt;\u0026gt;\u0026gt; print \u0026quot;%10.3s\u0026quot; % str abc  浮点类型数据位数保留\n\u0026gt;\u0026gt;\u0026gt; import fpformat \u0026gt;\u0026gt;\u0026gt; a= 0.0030000000005 \u0026gt;\u0026gt;\u0026gt; b=fpformat.fix(a,6) \u0026gt;\u0026gt;\u0026gt; print b 0.003000  对浮点数四舍五入,主要使用到round函数\n\u0026gt;\u0026gt;\u0026gt; from decimal import * \u0026gt;\u0026gt;\u0026gt; a =\u0026quot;2.26\u0026quot; \u0026gt;\u0026gt;\u0026gt; b =\u0026quot;2.29\u0026quot; \u0026gt;\u0026gt;\u0026gt; c = Decimal(a) - Decimal(b) \u0026gt;\u0026gt;\u0026gt; print c -0.03 \u0026gt;\u0026gt;\u0026gt; c / Decimal(a) * 100 Decimal('-1.327433628318584070796460177') \u0026gt;\u0026gt;\u0026gt; Decimal(str(round(c / Decimal(a) * 100, 2))) Decimal('-1.33') \u0026gt;\u0026gt;\u0026gt;   进制转换 有些时候需要作不同进制转换，可以参考下面的例子(%x 十六进制,%d 十进制,%o十进制)\n \u0026gt;\u0026gt;\u0026gt; num = 10 \u0026gt;\u0026gt;\u0026gt; print \u0026quot;Hex = %x,Dec = %d,Oct = %o\u0026quot; %(num,num,num) Hex = a,Dec = 10,Oct = 12  Python调用系统命令或者脚本  使用 os.system() 调用系统命令 , 程序中无法获得到输出和返回值\n\u0026gt;\u0026gt;\u0026gt; import os \u0026gt;\u0026gt;\u0026gt; os.system('ls -l /proc/cpuinfo') \u0026gt;\u0026gt;\u0026gt; os.system(\u0026quot;ls -l /proc/cpuinfo\u0026quot;) -r--r--r-- 1 root root 0 3月 29 16:53 /proc/cpuinfo 0  使用 os.popen() 调用系统命令, 程序中可以获得命令输出，但是不能得到执行的返回值\n\u0026gt;\u0026gt;\u0026gt; out = os.popen(\u0026quot;ls -l /proc/cpuinfo\u0026quot;) \u0026gt;\u0026gt;\u0026gt; print out.read() -r--r--r-- 1 root root 0 3月 29 16:59 /proc/cpuinfo \u0026gt;\u0026gt;\u0026gt;  使用 commands.getstatusoutput() 调用系统命令, 程序中可以获得命令输出和执行的返回值\n\u0026gt;\u0026gt;\u0026gt; import commands \u0026gt;\u0026gt;\u0026gt; commands.getstatusoutput('ls /bin/ls') (0, '/bin/ls') \u0026gt;\u0026gt;\u0026gt;   Python 捕获用户 Ctrl+C ,Ctrl+D 事件 有些时候，需要在程序中捕获用户键盘事件，比如ctrl+c退出，这样可以更好的安全退出程序\n try: do_some_func() except KeyboardInterrupt: print \u0026quot;User Press Ctrl+C,Exit\u0026quot; except EOFError: print \u0026quot;User Press Ctrl+D,Exit\u0026quot;  Python 读写文件  一次性读入文件到列表，速度较快，适用文件比较小的情况下\ntrack_file = \u0026quot;track_stock.conf\u0026quot; fd = open(track_file) content_list = fd.readlines() fd.close() for line in content_list: print line  逐行读入，速度较慢,适用没有足够内存读取整个文件(文件太大)\nfd = open(file_path) fd.seek(0) title = fd.readline() keyword = fd.readline() uuid = fd.readline() fd.close()   写文件 write 与 writelines 的区别\nFd.write(str) : 把str写到文件中，write()并不会在str后加上一个换行符\nFd.writelines(content) : 把content的内容全部写到文件中,原样写入，不会在每行后面加上任何东西\n Python 读取 YAML 配置文件 简单的配置文件范例 配置文件如下,一般YAML文件扩展名为.yaml:\n [root@localhost]# cat us-cdn.yaml templateName: \u0026quot;us-cdn\u0026quot; version: \u0026quot;0.1\u0026quot; actionsequence: - install - sync sync: - cmd: \u0026quot;svn export http://svn.test.com/\u0026quot; SuccessMsg: \u0026quot;OK\u0026quot; FailedMsg: \u0026quot;Run Cmd Error %s\u0026quot; install: - cmd: \u0026quot;yum install squid squid-script\u0026quot; SuccessMsg: \u0026quot;OK\u0026quot; FailedMsg: \u0026quot;Run Cmd Error %s\u0026quot;  从上面的配置文件来看，非常容易读懂和理解，层次关系也非常明了。\nPyYAML 模块来解析YAML配置文件 参考测试程序:\n [root@localhost]# cat test_yaml.py #!/usr/bin/env python # -*- coding: utf-8 -*- import os import yaml import sys from pprint import pprint base_dir = os.path.abspath(os.path.dirname(sys.argv[0])) configFile = \u0026quot;%s/%s\u0026quot; % (base_dir,\u0026quot;us-cdn.yaml\u0026quot;) stream = file(configFile, 'r') data = yaml.load(stream) pprint(data)  执行结果如下:\n [root@localhost]# python test_yaml.py {'actionsequence': ['install', 'sync'], 'install': [{'FailedMsg': 'Run Cmd Error %s', 'SuccessMsg': 'OK', 'cmd': 'yum install squid squid-script'}], 'sync': [{'FailedMsg': 'Run Cmd Error %s', 'SuccessMsg': 'OK', 'cmd': 'svn export http://svn.test.com/'}], 'templateName': 'us-cdn', 'version': '0.1'}  从执行结果可以看出,最终返回的是字典结构，你可以方便的引用。在写一些系统运维管理脚本的化，可以参考使用yaml格式的配置文件.\n常见问题 UnicodeDecodeError: \u0026lsquo;ascii\u0026rsquo; codec can\u0026rsquo;t decode byte 0xe8 in position 0: ordinal not in range(128) 程序加入如下3行，问题解决\nimport sys reload(sys) sys.setdefaultencoding('utf-8')  "
},
{
	"uri": "/docs/openshift/",
	"title": "容器云",
	"tags": [],
	"description": "",
	"content": " 容器云 OpenShift Origin OpenShift 本分类主要讲述 openshift origin 相关配置等实践\n"
},
{
	"uri": "/docs/devops/shell/",
	"title": "Shell脚本知识总结",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2016-03-28 创建 HunterFu 创建文档    在日常系统管理工作中，需要编写脚本来完成特定的功能，编写shell脚本是一个基本功了！\n在编写的过程中，掌握一些常用的技巧和语法就可以完成大部分功能了，也就是2/8原则.\n单引号和双引号的区别 单引号与双引号的最大不同在于双引号仍然可以引用变量的内容，但单引号内仅是 普通字符 ，不会作变量的引用，直接输出字符窜。请看如下例子：\n [root@linux ~]# name=HaHa [root@linux ~]# echo $name HaHa [root@linux ~]# myname=\u0026quot;$name is wow\u0026quot; [root@linux ~]# echo $myname HaHa is wow [root@linux ~]# myname='$name is wow' [root@linux ~]# echo $myname $name is wow  从上面例子可以看出,使用了单引号的时候，那么$name只是普通字符,直接输出而已！\n逐行读取文件  使用for循环来读取文件\nfor line in `cat file.txt` do echo $line done  注意:由于使用for来读入文件里的行时，会自动把空格和换行符作为一样分隔符，如果行里有空格的时候，输出的结果会很乱，所以只适用于行连续不能有空格或者换行符的文件\n  使用while循环读取文件\ncat file.txt |while read line do echo $line done  或者：\nwhile read line do echo $line done \u0026lt; file.txt  注意:由于使用while来读入文件里的行时，会整行读入，不会关注行的内容(空格..)，所以比for读文件有更好的适用性，推荐使用while循环读取文件\n   bash shell 脚本中常用隐含变量    隐含变量 含义     $0 当前执行的脚本或者命令名称   $1-$9 代表参数的位置. 举例 $1 代表第一个参数.   $# 脚本调用的参数的个数   $@ 所有参数的内容   $* 所有参数的内容   $$ 当前运行脚本的进程号   $? 命令执行后返回的状态   $! 后台运行的最后一个进程号    注意:\n$? 用于检查上一个命令执行是否正确(在Linux中，命令退出状态为0表示该命令正确执行，任何非0值表示命令出错)\n$$ 变量最常见的用途是用做暂存文件的名字以保证暂存文件不会重复\n$* 和 $@ 结果输出是一样的，但是在使用for循环，在使用 双引号(\u0026ldquo;\u0026rdquo;)引用时 \u0026rdquo;$*\u0026rdquo; 会输出成一个元素 而 \u0026rdquo;$@\u0026rdquo; 会按照每个参数是一个元素方式输出\n 请看测试例子\n#cat test.sh #!/bin/sh echo '\u0026quot;$@\u0026quot; output.....' for i in \u0026quot;$@\u0026quot; do echo $i done echo '\u0026quot;$*\u0026quot; output ....' for i in \u0026quot;$*\u0026quot; do echo $i done  输出结果\n#sh test.sh a b c d \u0026quot;$@\u0026quot; output..... a b c d \u0026quot;$*\u0026quot; output .... a b c d   从输出结果可以看出 \u0026ldquo;$*\u0026rdquo; 输出是一行 而 \u0026ldquo;$@\u0026rdquo; 输出则是四行\n 变量内容的删除与替换 我们在一些情况下，需要对变量中的字符窜进行查找删除或者替换，就需要使用下表列出的方法\n   变量设定方式 说明     ${变量#关键字} 若变量内容从头开始的资料符合‘关键字’，则将符合的最短资料删除   ${变量##关键字} 若变量内容从头开始的资料符合‘关键字’，则将符合的最长资料删除   ${变量%关键字} 若变量内容从尾向前的资料符合‘关键字’，则将符合的最短资料删除   ${变量%%关键字} 若变量内容从尾向前的资料符合‘关键字’，则将符合的最长资料删除   ${变量/旧字串/新字串} 若变量内容符合‘旧字串’则‘第一个旧字串会被新字串取代   ${变量//旧字串/新字串} 若变量内容符合‘旧字串’则‘全部的旧字串会被新字串取代    举例如下(删除字符窜中的某个字符):\n[root@linux ~]# export test_str=\u0026quot;/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin\u0026quot; [root@linux ~]# echo ${test_str#/*kerberos/bin:} /usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin  变量条件测试赋值 在某些时刻我们需要\u0026rsquo;判断\u0026rsquo;某个变量是否存在，若变量存在则将此变量值赋值给新的变量，若变量不存在则将其他值赋值给新的变量.\n   变量设定方式 str 未定义 str 为空字串 str 已赋值为非空字串     var=${str-expr} var=expr var= var=$str   var=${str:-expr} var=expr var=expr var=$str   var=${str+expr} var= var=expr var=expr   var=${str:+expr} var= var= var=expr   var=${str?expr} expr 输出至 stderr var= var=$str   var=${str:?expr} expr 输出至 stderr expr 输出至 stderr var=$str   var=${str=expr} var=expr var= var=$str   var=${str:=expr} var=expr var=expr var=$str    举例如下:\n[root@linux ~]# test_name=\u0026quot;\u0026quot; [root@linux ~]# test_name=${test_name-root} [root@linux ~]# echo $test_name \u0026lt;== 因为 test_name 被设定为空字符窜！所以当然还是保留为空字符窜！ [root@linux ~]# test_name=${test_name:-root} [root@linux ~]# echo $test_name root \u0026lt;== 加上‘:’后若变量内容为空或者是未设定，都能够以后面的内容替换！   基本上这种变量的测试也能够透过 shell script 内的 if...then... 来处理,不过通过上述提及的简单的方法来测试变量，是程序看起来更精简一些！\n shell 中分隔符 : 变量IFS 使用 shell脚本中，如果使用for循环一个字符窜的话，默认使用空格来分割字符窜. 还有前面所提到的 使用for循环逐行读取文件内容时候,文件行中如果有空格的话输出的结果也会变乱. 这个时候 使用 IFS 变量来设置特定的字符窜分割符来，达到输出正确的目的. 默认情况下 IFS 是使用 空格 \\t \\n 来作为默认的分割符的.\n我们将前面使用for逐行读取文件的例子 改进下就可以输出正确了,请看下面\n#!/bin/bash IFS_old=$IFS #将原IFS值保存，以便用完后恢复 IFS=$’\\n’ #更改IFS值为$’\\n’ for line in `cat file.txt` do echo $line done  file.txt 文件内容如下\n[root@linux]$ cat file.txt sdfsdfsdfsdf ssssss ssssss ssssss sssss sdfsdfsdfsdfsdf  执行测试程序 输出结果如下(正确输出)\n[root@linux]$ sh test.sh sdfsdfsdfsdf ssssss ssssss ssssss sssss sdfsdfsdfsdfsdf  如果未设置IFS变量,使用默认的IFS变量值 ,输出结果如下\n[root@linux]$ sh test.sh sdfsdfsdfsdf ssssss ssssss ssssss sssss sdfsdfsdfsdfsdf  从以上测试程序输出结果,可以根据自己的需求来设定 IFS变量,在举一个例子如下:\nwhile IFS=: read userName passWord userID groupID geCos homeDir userShell do echo \u0026quot;$userName -\u0026gt; $homeDir\u0026quot; done \u0026lt; /etc/passwd  shell 数组的使用 数组赋值方式:\n(1) array=(var1 var2 var3 ... varN) (2) array=([0]=var1 [1]=var2 [2]=var3 ... [n]=varN) (3) array[0]=var1 arrya[1]=var2 ... array[n]=varN  计算数组元素个数或者长度:\n(1) ${#array[@]} (2) ${#array[*]}  了解了数组基础语法，举例说明，请看:\n#!/bin/bash NAMESERVERS=(\u0026quot;ns1.www.net.\u0026quot; \u0026quot;ns2.www.net.\u0026quot; \u0026quot;ns3.www.net.\u0026quot;) # 得到数组长度 tLen=${#NAMESERVERS[@]} # 循环数组 for (( i=0; i\u0026lt;${tLen}; i++ )); do echo ${NAMESERVERS[$i]} done  在看一个复杂一点的例子,将文件内容读取到数组中:\n#!/bin/bash # 设置IFS将分割符 设置为 换行符(\\n) OLDIFS=$IFS IFS=$'\\n' # 读取文件内容到数组 fileArray=($(cat file.txt)) # restore it IFS=$OLDIFS tLen=${#fileArray[@]} # 循环显示文件内容 for (( i=0; i\u0026lt;${tLen}; i++ )); do echo \u0026quot;${fileArray[$i]}\u0026quot; done  逻辑判断 条件测试 文件属性的判断    操作符 测试结果     -e filename 文件存在返回1， 否则返回0   -r filename 文件可读返回1,否则返回0   -w filename 文件可写返回1,否则返回0   -x filename 文件可执行返回1,否则返回0   -o filename 文件属于用户本人返回1, 否则返回0   -z filename 文件长度为0返回1, 否则返回0   -f filename 文件为普通文件返回1, 否则返回0   -d filename 文件为目录文件时返回1, 否则返回0    举例如下,测试文件是否存在:\n#!/bin/bash echo \u0026quot;checks the existence of the messages file.\u0026quot; echo -n \u0026quot;Checking...\u0026quot; if [ -f /var/log/messages ];then echo \u0026quot;/var/log/messages exists.\u0026quot; fi echo echo \u0026quot;...done.\u0026quot;  字符串比较    操作符 比较结果     str1 = str2 当两个字串相等时为真   str1 != str2 当两个字串不等时为真   -n str1 当字符串的长度大于0时为真   -z str1 当字符串的长度为0时为真   str 当字符串为非空时为真    举例如下,比较字符串来测试用户ID :\nif [ \u0026quot;$(whoami)\u0026quot; != 'root' ]; then echo \u0026quot;You have no permission to run $0 as non-root user.\u0026quot; exit 1; fi  数值比较(整数)    操作符 比较结果     num1 -eq num2 两数相等为真   num1 -ne num2 两数不等为真   num1 -gt num2 num1大于num2为真   num1 -ge num2 num1大于等于num2为真   num1 -lt num2 num1小于num2为真   num1 -le num2 num1小于等于num2为真    举例如下:\nnum=`wc -l work.txt` if [ $num -gt 150 ];then echo \u0026quot;you've worked hard enough for today.\u0026quot; echo fi  如果要查看详细的测试操作,可以查看man手册 man test\n"
},
{
	"uri": "/docs/devops/git_useage_qa/",
	"title": "Git使用常见问题",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.1 2018-01-02 创建 HunterFu 初始版本    使用git pull文件时和本地文件冲突怎么办？ 同事在使用git pull代码时，经常会碰到有冲突的情况，提示如下信息：\nerror: Your local changes to 'c/environ.c' would be overwritten by merge. Aborting. Please, commit your changes or stash them before you can merge.  这个意思是说更新下来的内容和本地修改的内容有冲突，先提交你的改变或者先将本地修改暂时存储起来。\n处理的方式非常简单，主要是使用git stash命令进行处理，分成以下几个步骤进行处理。\n 先将本地修改存储起来 $ git stash 这样本地的所有修改就都被暂时存储起来\n 用git stash list可以看到保存的信息： git stash list 结果如下 其中 stash@{0} 就是刚才保存的标记。\n 同步仓库内容(pull) 暂存了本地修改之后，就可以pull了。 $ git pull\n 还原暂存的内容 $ git stash pop stash@{0} 系统提示如下类似的信息：\nAuto-merging c/environ.c CONFLICT (content): Merge conflict in c/environ.c  意思就是系统自动合并修改的内容，但是其中有冲突，需要解决其中的冲突。\n 解决文件中冲突的的部分 打开冲突的文件，会看到类似如下的内容： 其中:\nUpdated upstream 和 =====之间的内容就是pull下来的内容\n==== 和 stashed changes 之间的内容就是本地修改的内容\n碰到这种情况，git也不知道哪行内容是需要的，所以要自行确定需要的内容。 解决完成之后，就可以正常的提交了。\n  已经在git仓库中的文件，清除跟踪状态 执行git rm --cached logs/xx.log命令即可\n需要注意的，git rm \u0026ndash;cached 删除的是追踪状态，而不是物理文件；如果你真的是彻底不想要了，你也可以直接 rm＋忽略＋提交\n修正 .gitignore 文件 忽略不需要跟踪的文件或者目录\n"
},
{
	"uri": "/docs/security/inotify/",
	"title": "Inotify监控文件系统事件",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.1 2016-03-28 创建 HunterFu 初始版本    Inotify 机制概述 Inotify 介绍 在日常的运维过程中，经常需要备份某些文件，或者对系统的某些文件进行监控，比如重要的配置文件等。如果需要作到实时同步或者监控，就需要使用内核的inotify机制\nInotify 是基于inode级别的文件系统监控技术,是一种强大的、细粒度的、异步的机制，它满足各种各样的文件监控需要，不仅限于安全和性能\n Inotify 不需要对被监视的目标打开文件描述符，而且如果被监视目标在可移动介质上，那么在 umount 该介质上的文件系统后，被监视目标对应的 watch 将被自动删除，并且会产生一个 umount 事件。 Inotify 既可以监视文件，也可以监视目录。 Inotify 使用系统调用而非 SIGIO 来通知文件系统事件。 Inotify 使用文件描述符作为接口，因而可以使用通常的文件 I/O 操作select 和 poll 来监视文件系统的变化。  Inotify 可监视的文件系统事件  IN_ACCESS : 即文件被访问 IN_MODIFY : 文件被 write IN_ATTRIB : 文件属性被修改，如 chmod、chown、touch 等 IN_CLOSE_WRITE : 可写文件被 close IN_CLOSE_NOWRITE : 不可写文件被 close IN_OPEN : 文件被open IN_MOVED_FROM : 文件被移走,如 mv IN_MOVED_TO : 文件被移来，如 mv、cp IN_CREATE : 创建新文件 IN_DELETE : 文件被删除，如 rm IN_DELETE_SELF : 自删除，即一个可执行文件在执行时删除自己 IN_MOVE_SELF : 自移动，即一个可执行文件在执行时移动自己 IN_UNMOUNT : 宿主文件系统被 umount IN_CLOSE : 文件被关闭，等同于(IN_CLOSE_WRITE | IN_CLOSE_NOWRITE) IN_MOVE : 文件被移动，等同于(IN_MOVED_FROM | IN_MOVED_TO)  注：上面所说的文件也包括目录\nInotify内核版本支持  从kernel 2.6.13开始,Inotify正式并入内核，RHEL5已经支持. 看看是否有 /proc/sys/fs/inotify/目录，以确定内核是否支持inotify  [root@RHEL5 Rsync]# ls -l /proc/sys/fs/inotify/ total 0 -rw-r--r-- 1 root root 0 Oct 9 09:36 max_queued_events -rw-r--r-- 1 root root 0 Oct 9 09:36 max_user_instances -rw-r--r-- 1 root root 0 Oct 9 09:36 max_user_watches  inotify 的默认内核参数  /proc/sys/fs/inotify/max_queued_events\n默认值: 16384 该文件中的值为调用inotify_init时分配给inotify instance中可排队的event的数目的最大值，超出这个值得事件被丢弃，但会触发IN_Q_OVERFLOW事件\n /proc/sys/fs/inotify/max_user_instances\n默认值: 128 指定了每一个real user ID可创建的inotify instatnces的数量上限\n /proc/sys/fs/inotify/max_user_watches\n默认值: 8192 指定了每个inotify instance相关联的watches的上限\n  注意: max_queued_events 是 Inotify 管理的队列的最大长度，文件系统变化越频繁，这个值就应该越大 如果你在日志中看到Event Queue Overflow，说明max_queued_events太小需要调整参数后再次使用.\nInotify 在系统中使用 linux shell 下使用inotify  下载安装 [inotify-tools源码 http://inotify-tools.sourceforge.net/]\n inotifywait 仅执行阻塞，等待 inotify 事件。您可以监控任何一组文件和目录，或监控整个目录树（目录、子目录、子目录的子目录等等）\n在 shell 脚本中使用 inotifywait。 inotifywatch 收集关于被监视的文件系统的统计数据，包括每个 inotify 事件发生多少次。  shell脚本示例\n[root@localhost ]# cat /tmp/test.sh #!/bin/bash inotifywait -mrq --timefmt '%d/%m/%y %H:%M' --format '%T %w%f %e' --event modify,delete,create,attrib /home/admin | while read date time file event do case $event in MODIFY|CREATE|MOVE|MODIFY,ISDIR|CREATE,ISDIR|MODIFY,ISDIR) echo $event'-'$file ;; MOVED_FROM|MOVED_FROM,ISDIR|DELETE|DELETE,ISDIR) echo $event'-'$file ;; esac done  执行脚本，结果输出(这里测试删除了一个目录 rm -fr cronolog-1.6.2.bak)\n[root@localhost]# /tmp/test.sh DELETE-/home/admin/cronolog-1.6.2.bak/COPYING DELETE-/home/admin/cronolog-1.6.2.bak/doc/cronolog.info DELETE-/home/admin/cronolog-1.6.2.bak/doc/cronolog.texi DELETE-/home/admin/cronolog-1.6.2.bak/doc/Makefile.am DELETE-/home/admin/cronolog-1.6.2.bak/doc/Makefile.in DELETE-/home/admin/cronolog-1.6.2.bak/doc/texinfo.tex DELETE-/home/admin/cronolog-1.6.2.bak/doc/cronosplit.1m DELETE-/home/admin/cronolog-1.6.2.bak/doc/Makefile DELETE-/home/admin/cronolog-1.6.2.bak/doc/cronolog.1m DELETE,ISDIR-/home/admin/cronolog-1.6.2.bak/doc DELETE-/home/admin/cronolog-1.6.2.bak/TODO DELETE-/home/admin/cronolog-1.6.2.bak/src/cronotest.c DELETE-/home/admin/cronolog-1.6.2.bak/src/cronolog.c DELETE-/home/admin/cronolog-1.6.2.bak/src/cronoutils.h DELETE-/home/admin/cronolog-1.6.2.bak/src/cronoutils.c DELETE-/home/admin/cronolog-1.6.2.bak/src/Makefile.am DELETE-/home/admin/cronolog-1.6.2.bak/src/Makefile.in DELETE-/home/admin/cronolog-1.6.2.bak/src/cronosplit.in DELETE-/home/admin/cronolog-1.6.2.bak/src/Makefile DELETE-/home/admin/cronolog-1.6.2.bak/src/cronosplit DELETE-/home/admin/cronolog-1.6.2.bak/src/config.h DELETE,ISDIR-/home/admin/cronolog-1.6.2.bak/src DELETE-/home/admin/cronolog-1.6.2.bak/lib/getopt1.c DELETE-/home/admin/cronolog-1.6.2.bak/lib/getopt.h DELETE-/home/admin/cronolog-1.6.2.bak/lib/Makefile.am DELETE-/home/admin/cronolog-1.6.2.bak/lib/Makefile.in DELETE-/home/admin/cronolog-1.6.2.bak/lib/localtime_r.c DELETE-/home/admin/cronolog-1.6.2.bak/lib/getopt.c DELETE-/home/admin/cronolog-1.6.2.bak/lib/Makefile DELETE-/home/admin/cronolog-1.6.2.bak/lib/strptime.c DELETE,ISDIR-/home/admin/cronolog-1.6.2.bak/lib DELETE-/home/admin/cronolog-1.6.2.bak/config.cache DELETE-/home/admin/cronolog-1.6.2.bak/install-sh DELETE-/home/admin/cronolog-1.6.2.bak/Makefile.am DELETE-/home/admin/cronolog-1.6.2.bak/README DELETE-/home/admin/cronolog-1.6.2.bak/AUTHORS DELETE-/home/admin/cronolog-1.6.2.bak/Makefile.in DELETE-/home/admin/cronolog-1.6.2.bak/testsuite/Makefile.am DELETE-/home/admin/cronolog-1.6.2.bak/testsuite/README DELETE-/home/admin/cronolog-1.6.2.bak/testsuite/Makefile.in DELETE-/home/admin/cronolog-1.6.2.bak/testsuite/Makefile DELETE,ISDIR-/home/admin/cronolog-1.6.2.bak/testsuite DELETE-/home/admin/cronolog-1.6.2.bak/cronolog.spec DELETE-/home/admin/cronolog-1.6.2.bak/NEWS DELETE-/home/admin/cronolog-1.6.2.bak/configure DELETE-/home/admin/cronolog-1.6.2.bak/ChangeLog DELETE-/home/admin/cronolog-1.6.2.bak/missing DELETE-/home/admin/cronolog-1.6.2.bak/config.log DELETE-/home/admin/cronolog-1.6.2.bak/aclocal.m4 DELETE-/home/admin/cronolog-1.6.2.bak/Makefile DELETE-/home/admin/cronolog-1.6.2.bak/INSTALL DELETE-/home/admin/cronolog-1.6.2.bak/config.status DELETE-/home/admin/cronolog-1.6.2.bak/configure.in DELETE-/home/admin/cronolog-1.6.2.bak/mkinstalldirs DELETE,ISDIR-/home/admin/cronolog-1.6.2.bak   详细请参考 man inotify , man inotifywait\n使用incron实现重要配置文件监控 Incron是inotify的cron系统，与os本身的cron一样，包含一个后台守护进程（incrond）和一个事件编辑器（incrontab\n与os本身的cron不同的仅仅是触发时间的是os对某个文件（夹）的操作而不是时间,由系统事件触发的机制，对于应用系统来说，几乎可以做到实时性。\n 安装Incron [Centos/RHEL都以pm包方式安装]\n[root@localhost]# yum install Incron  查看 incron 支持的事件类型 incrontab -t ,编辑配置文件使用 incrontab -e\n 配置文件格式说明(默认配置在/var/spool/incron/ 目录下)\n\u0026lt;path\u0026gt; \u0026lt;mask\u0026gt; \u0026lt;command\u0026gt;  选项说明:\n\u0026lt;path\u0026gt;：欲监控的文件或者目录\n\u0026lt;mask\u0026gt;：os对监控对象发生的事件\n\u0026lt;command\u0026gt;：command可以是系统命令，也可以是脚本，不能是用系统的重定向，除非重定向写在脚本中。\n \u0026lt;Command\u0026gt;中还可以使用下面的这些变量：\n$@：代表，即监控对象\n$#：发生系统事件的对象（例如监控了某个文件夹，其下的某个文件发生了变化，那么$#就代表了该文件名）\n$%：代表，即发生的事件\n 配置举例：\n /home/admin/a.txt IN_MODIFY echo \u0026ldquo;$@ $#\u0026rdquo; 表示文件a.txt一旦被修改，就执行 echo \u0026ldquo;$@ $#\u0026rdquo;\n /home/admin/ IN_ALL_EVENTS echo \u0026ldquo;$@ $# $%\u0026rdquo; 表示目录下的文件任何事件触发,就执行 echo \u0026ldquo;$@ $#\u0026rdquo;\n  启动incrond (/etc/init.d/incrond start),然后在 /home/admin目录删除 ssss 文件，查看日志 tail /var/log/cron ,有如下输出\n  Mar 23 14:05:19 localhost incrond[6857]: (root) CMD (echo \u0026quot;/home/admin = = IN_OPEN,IN_ISDIR\u0026quot;) Mar 23 14:05:19 localhost incrond[6857]: (root) CMD (echo \u0026quot;/home/admin = = IN_CLOSE_NOWRITE,IN_ISDIR\u0026quot;) Mar 23 14:05:20 localhost incrond[6857]: (root) CMD (echo \u0026quot;/home/admin = = IN_OPEN,IN_ISDIR\u0026quot;) Mar 23 14:05:20 localhost incrond[6857]: (root) CMD (echo \u0026quot;/home/admin = = IN_CLOSE_NOWRITE,IN_ISDIR\u0026quot;) Mar 23 14:05:20 localhost incrond[6857]: (root) CMD (echo \u0026quot;/home/admin = ssss = IN_DELETE\u0026quot;)  总体来说，在文件和目录实时监控还是很有效的，可以结合其他工具来作统一化的解决方案,比如使用syslog-ng作统一化收集，当然最重要还是要有场景.\n"
},
{
	"uri": "/docs/monitor/",
	"title": "监控系统",
	"tags": [],
	"description": "",
	"content": " 监控系统 Zabbix Zabbix 本分类主要讲述 Zabbix 监控系统相关配置等实践\n"
},
{
	"uri": "/docs/security/iptable/",
	"title": "linux iptable 使用指南",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2016-03-28 创建 HunterFu 初始版本    linux系统中,防火墙(Firewall),网址转换(NAT),数据包(package)记录,流量统计,这些功能是由Netfilter子系统所提供的，而iptables是控制Netfilter的工具.\niptables将许多复杂的规则组织成成容易控制的方式，以便管理员可以进行分组测试，或关闭、启动某组规则。\niptable能够为Unix、Linux和BSD个人工作站创建一个防火墙，也可以为一个子网创建防火墙以保护其它的系统平台。\niptable只读取数据包头，不会给信息流增加负担，也无需进行验证。\n术语解释  DNAT Destination Network Address Translation 目标网络地址转换。 DNAT是一种改变数据包目的ip地址的技术，经常和SNAT联用，以使多台服务器能共享一个ip地址连入Internet，并且继续服务。通过对同一个ip地址分配不同的端口，来决定数据的流向。\n SNAT Source Network Address Translation源网络地址转换。这是一种改变数据包源ip地址的技术， 经常用来使多台计算机分享一个Internet地址。这只在IPv4中使用，因为IPv4的地址已快用完了，IPv6将解 决这个问题。\n  iptable 概述 iptable的链和表结构 如上图可以看出，iptable总体结构.\n5个链(chain)   PREROUTING 在数据包进入防火墙之后、路由判断之前对数据包进行修改\n INPUT 在数据包被路由到本地之后，但在用户空间程序看到它之前对数据包进行修改\n OUTPUT 用户空间程序处理数据包后，由本地发出，再次被路由之前更改数据包\n FORWARD 在最初的路由判断之后、最后一次更改包的源地址之前对数据包进行修改\n POSTROUTING 在所有路由判断之后,对数据包进行修改\n  注意: 链 是每个数据包流需要经过的不同环节，你可以在不同的环节根据需要设置不同的过滤策略,每个链的默认策略都是Accept\n4个表(table)  Mangle表 这个表主要用来mangle包，你可以使用mangle匹配来改变包的一些属性，比如 TOS（TYPE OF SERVICE),TTL (TIME TO LIVE),MARK(后续流量控制TC等)\n Nat表 此表仅用于NAT，也就是转换包的源或目标地址。注意，就象我们前面说过的，只有流的第一个 包会被这个链匹配，其后的包会自动被做相同的处理(DNAT,SNAT,MASQUERADE)\n Filter表 此表用来过滤数据包，我们可以在任何时候匹配包并过滤它们。 我们就是在这里根据包的内容对包做DROP或ACCEPT的.\niptalbe中,要用 -t 参数指定要操作哪个表,如果没有 -t 参数，就默认对filter表操作.\n Raw表 优先级最高，设置raw时一般是为了不再让iptables做数据包的链接跟踪处理，提高性能\n  注意: 表 是规则的集合组,每个表中的规则条目是按顺序匹配的,你可以在数据包经过的不同环节设置规则,表的处理优先级：raw \u0026gt; mangle \u0026gt; nat \u0026gt; filter\n详细的数据包流程 从上图可以看出，数据包流环节和表的配合使用方法\niptable应用场景 上图是应用场景的简单拓扑描述,下面的应用场景举例，都以上图为参考.\n系统启动的时候所有的默认策略都是ACCEPT,在下面的场景举例中，我们都是在这种前提下设定iptable的\n下面每个场景举例都是独立的，没有相关联性的\n网关服务器安全策略 目标 : 网关服务器系统自生安全策略，只对内网用户开放22端口(sshd服务)\n#清空 filter table [root@localhost]# iptables -F -t filter [root@localhost]# iptables -X -t filter [root@localhost]# iptables -Z -t filter #清空 nat table [root@localhost]# iptables -F -t nat [root@localhost]# iptables -X -t nat [root@localhost]# iptables -Z -t nat #设置默认策略(INPUT链默认为DROP) [root@localhost]# iptables -t filter -P INPUT DROP [root@localhost]# iptables -t filter -P OUTPUT ACCEPT [root@localhost]# iptables -t filter -P FORWARD ACCEPT #回环接口(lo),默认accept [root@localhost]# iptables -A INPUT -p ALL -i lo -j ACCEPT #只对内网用户开放sshd服务 [root@localhost]# iptables -A INPUT -p tcp -s 192.168.138.0/24 --dport 22 -j ACCEPT  说明: 防火墙的策略顺序一般都是 从 非信任 ==\u0026gt; 信任,默认关闭所有访问权限,然后按照需要逐条开放访问权限.\n共享上网(nat) 目标：使局域网的用户都可以访问外网的服务器\n[root@localhost]# echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward [root@localhost]# iptables -t nat -A POSTROUTING -o ppp0 -j MASQUERADE  说明: SNAT 和 MASQUERADE 区别\nSNAT : 不管是几个地址，必须明确的指定要SNAT的ip，适合网关服务器有固定地址或者是固定地址范围. MASQUERADE : 是针对ADSL动态拨号这种场景而设计,从服务器的网络接口上,自动获取当前ip地址来做NAT,这样就实现了动态SNAT地址转换\n内网的服务器对外服务(端口映射) 目标：使外网用户可以访问到局域网192.168.138.21这台HTTP服务\n[root@localhost]# echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward [root@localhost]# iptables -t nat -A PREROUTING -p tcp -m tcp --dport 80 -j DNAT --to-destination 192.168.138.21 [root@localhost]# iptables -t nat -A POSTROUTING -o ppp0 -j MASQUERADE  在网关服务器进行透明代理 目标: 使局域网用户,访问外网web服务时，自动使用squid作web透明代理服务器。\n[root@localhost]# echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward [root@localhost]# iptables -t nat -A PREROUTING -s 192.168.138.0/24 -p tcp --dport 80 -i eth0 -j DNAT --to 192.168.138.1 [root@localhost]# iptables -t nat -A PREROUTING -s 192.168.138.0/24 -p tcp --dport 80 -i eth0 -j REDIRECT --to 3128 [root@localhost]# iptables -t nat -A POSTROUTING -o ppp0 -j MASQUERADE  "
},
{
	"uri": "/docs/devops/cdn/",
	"title": "网站加速之CDN技术原理",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2016-03-28 创建 HunterFu 初始版本    在不同地域的用户访问网站的响应速度存在差异,为了提高用户访问的响应速度、优化现有Internet中信息的流动,需要在用户和服务器间加入中间层CDN. 使用户能以最快的速度，从最接近用户的地方获得所需的信息，彻底解决网络拥塞，提高响应速度，是目前大型网站使用的流行的应用方案.\nCDN 概述  CDN的全称是Content Delivery Network，即内容分发网络。其目的是通过在现有的Internet中增加一层新的CACHE(缓存)层，将网站的内容发布到最接近用户的网络\u0026ldquo;边缘\u0026rdquo;的节点，使用户可以就近取得所需的内容，提高用户访问网站的响应速度。从技术上全面解决由于网络带宽小、用户访问量大、网点分布不均等原因，提高用户访问网站的响应速度。   Cache层的技术，消除数据峰值访问造成的结点设备阻塞。Cache服务器具有缓存功能，所以大部分网页对象（Web page object）,如html, htm, php等页面文件，gif,tif,png,bmp等图片文件，以及其他格式的文件，在有效期（TTL）内，对于重复的访问，不必从原始网站重新传送文件实体, 只需通过简单的认证（Freshness Validation）- 传送几十字节的Header，即可将本地的副本直接传送给访问者。由于缓存服务器通常部署在靠近用户端，所以能获得近似局域网的响应速度，并有效减少广域带宽的消耗。不仅能提高响应速度，节约带宽，对于加速Web服务器，有效减轻源服务器的负载是非常有效的。\n 根据加速对象不同，分为 客户端加速 和 服务器加速\n 客户端加速 : Cache部署在网络出口处，把常访问的内容缓存在本地，提高响应速度和节约带宽； 服务器加速 : Cache部署在服务器前端，作为Web服务器的代理缓存机，提高Web服务器的性能，加速访问速度, 如果多台Cache加速服务器且分布在不同地域，需要通过有效地机制管理Cache网络，引导用户就近访问(比如通过DNS引导用户)，全局负载均衡流量，这是CDN内容传输网络的基本思想.  CDN对网络的优化作用主要体现在如下几个方面\n 解决服务器端的\u0026rdquo;第一公里\u0026rdquo;问题 缓解甚至消除了不同运营商之间互联的瓶颈造成的影响 减轻了各省的出口带宽压力 缓解了骨干网的压力 优化了网上热点内容的分布   CDN 的工作原理 传统访问过程(未加速缓存服务) 我们先看传统的未加缓存服务的访问过程，以便了解CDN缓存访问方式与未加缓存访问方式的差别：\n由上图可见，用户访问未使用CDN缓存网站的过程为:\n 用户输入访问的域名,操作系统向 LocalDns 查询域名的ip地址. LocalDns向 ROOT DNS 查询域名的授权服务器(这里假设LocalDns缓存过期) ROOT DNS将域名授权dns记录回应给 LocalDns LocalDns得到域名的授权dns记录后,继续向域名授权dns查询域名的ip地址 域名授权dns 查询域名记录后，回应给 LocalDns LocalDns 将得到的域名ip地址，回应给 用户端 用户得到域名ip地址后，访问站点服务器 站点服务器应答请求，将内容返回给客户端.  CDN访问过程(使用缓存服务) CDN网络是在用户和服务器之间增加Cache层，主要是通过接管DNS实现,将用户的请求引导到Cache上获得源服务器的数据\n下面让我们看看访问使用CDN缓存后的网站的过程：\n通过上图，我们可以了解到，使用了CDN缓存后的网站的访问过程变为：\n 用户输入访问的域名,操作系统向 LocalDns 查询域名的ip地址. LocalDns向 ROOT DNS 查询域名的授权服务器(这里假设LocalDns缓存过期) ROOT DNS将域名授权dns记录回应给 LocalDns LocalDns得到域名的授权dns记录后,继续向域名授权dns查询域名的ip地址 域名授权dns 查询域名记录后(一般是CNAME)，回应给 LocalDns LocalDns 得到域名记录后,向智能调度DNS查询域名的ip地址 智能调度DNS 根据一定的算法和策略(比如静态拓扑，容量等),将最适合的CDN节点ip地址回应给 LocalDns LocalDns 将得到的域名ip地址，回应给 用户端 用户得到域名ip地址后，访问站点服务器 CDN节点服务器应答请求，将内容返回给客户端.(缓存服务器一方面在本地进行保存，以备以后使用，二方面把获取的数据返回给客户端，完成数据服务过程)  通过以上的分析我们可以得到，为了实现对普通用户透明(使用缓存后用户客户端无需进行任何设置)访问，需要使用DNS(域名解析)来引导用户来访问Cache服务器，以实现透明的加速服务. 由于用户访问网站的第一步就是 域名解析 ,所以通过修改dns来引导用户访问是最简单有效的方式.\nCDN网络的组成要素 对于普通的Internet用户，每个CDN节点就相当于一个放置在它周围的网站服务器. 通过对dns的接管，用户的请求被透明地指向离他最近的节点，节点中CDN服务器会像网站的原始服务器一样，响应用户的请求. 由于它离用户更近，因而响应时间必然更快.\n从上面图中 虚线圈起来的那块，就是CDN层,这层是位于 用户端 和 站点服务器之间.\n 智能调度DNS(比如f5的3DNS)  智能调度DNS是CDN服务中的关键系统.当用户访问加入CDN服务的网站时，域名解析请求将最终由 智能调度DNS 负责处理.\n它通过一组预先定义好的策略，将当时最接近用户的节点地址提供给用户，使用户可以得到快速的服务.\n同时它需要与分布在各地的CDN节点保持通信，跟踪各节点的健康状态,容量等，确保将用户的请求分配到就近可用的节点上.\n 缓存功能服务  负载均衡设备(如lvs,F5的BIG/IP) 内容Cache服务器(如squid） 共享存储(根据缓存数据量多少决定是否需要)   CDN 智能调度Dns 实例分析  分析img.alibaba.com域名  在系统中，执行dig命令,输出如下:\n#dig img.alibaba.com ; 部分省略 ;; QUESTION SECTION: ;img.alibaba.com. IN A ;; ANSWER SECTION: img.alibaba.com. 600 IN CNAME img.alibaba.com.edgesuite.net. img.alibaba.com.edgesuite.net. 7191 IN CNAME img.alibaba.com.georedirector.akadns.net. img.alibaba.com.georedirector.akadns.net. 3592 IN CNAME a1366.g.akamai.net. a1366.g.akamai.net. 12 IN A 204.203.18.145 a1366.g.akamai.net. 12 IN A 204.203.18.160 ; 部分省略  从上面查询结果可以看出 img.alibaba.com. CNAME img.alibaba.com.edgesuite.net. 后面的CNAME是由 Akamai(CDN服务商) 去跳转到 智能调度器上的.\n 分析www.discovery.com域名  在系统中，继续执行dig命令,输出如下:\n#dig www.discovery.com ; 部分省略 ;; QUESTION SECTION: ;www.discovery.com. IN A ;; ANSWER SECTION: www.discovery.com. 1077 IN CNAME www.discovery.com.edgesuite.net. www.discovery.com.edgesuite.net. 21477 IN CNAME a212.g.akamai.net. a212.g.akamai.net. 20 IN A 204.203.18.154 a212.g.akamai.net. 20 IN A 204.203.18.147 ; 部分省略  从上面查询结果可以看出 www.discovery.com. IN CNAME www.discovery.com.edgesuite.net. 后面的CNAME是由 Akamai(CDN服务商) 去跳转到 智能调度器上的.\n总结:一般来说，网站需要使用到CDN服务时，一般都是将需要加速访问的域名 CNAME到 CDN服务商的域名上.\n缓存服务和调度功能都是由服务商来完成.\nCDN的 智能调度Dns 简化实现 调度策略说明 在用户请求解析域名的时候，智能DNS判断用户的LocalDns的IP，然后跟DNS服务器内部的IP表范围匹配一下，看看用户是电信还是网通用户，然后给用户返回对应的IP地址 这里使用的是静态拓扑的方法,只是判断LocalDns的IP.要想使用更复杂的调度算法可以考虑商业产品,如F5的3DNS.\n假设CDN节点规划 在这里我们将使用 BIND 的View功能来实现运营商的区分,假设我们在每个运营商的机房都放有一个CDN节点,列表如下:\n   域名 运营商（view） 服务地址     www.cdntest.com 网通(CNC) 192.168.0.1   www.cdntest.com 电信(TELECOM) 192.168.0.2   www.cdntest.com 教育网(EDU) 192.168.0.3   www.cdntest.com 默认(ANY) 192.168.0.4    bind view 配置 以下是named.conf配置文件的部分截取，只是涉及到 View 的部分,其他细节可参考互联网. acl \u0026quot;cnc_iprange\u0026quot;{ //定义ip范围(网通) 192.168.1.0/24; 192.168.2.0/24; //此处只是示例,其他省略 }; acl \u0026quot;tel_iprange\u0026quot;{ //定义ip范围(电信) 192.168.3.0/24; 192.168.4.0/24; //其他省略 }; acl \u0026quot;edu_iprange\u0026quot;{ //定义ip范围(教育网) 192.168.5.0/24; 192.168.6.0/24; //其他省略 }; acl \u0026quot;default_iprange\u0026quot;{ //定义ip范围(默认) 192.168.7.0/24; 192.168.8.0/24; //其他省略 }; view \u0026quot;CNC\u0026quot; { Match-clients{cnc_iprange}; zone \u0026quot;.\u0026quot; IN { type hint; file \u0026quot;named.root\u0026quot;; }; zone \u0026quot;localhost\u0026quot; IN { type master; file \u0026quot;localhost.zone\u0026quot;; allow-update { none; }; }; zone \u0026quot;cdntest.com\u0026quot; IN { type master; file \u0026quot;cnc_cdntest.zone\u0026quot;; }; }; view \u0026quot;TEL\u0026quot; { Match-clients{tel_iprange}; zone \u0026quot;.\u0026quot; IN { type hint; file \u0026quot;named.root\u0026quot;; }; zone \u0026quot;localhost\u0026quot; IN { type master; file \u0026quot;localhost.zone\u0026quot;; allow-update { none; }; }; zone \u0026quot;cdntest.com\u0026quot; IN { type master; file \u0026quot;tel_cdntest.zone\u0026quot;; }; }; view \u0026quot;EDU\u0026quot; { Match-clients{edu_iprange}; zone \u0026quot;.\u0026quot; IN { type hint; file \u0026quot;named.root\u0026quot;; }; zone \u0026quot;localhost\u0026quot; IN { type master; file \u0026quot;localhost.zone\u0026quot;; allow-update { none; }; }; zone \u0026quot;cdntest.com\u0026quot; IN { type master; file \u0026quot;edu_cdntest.zone\u0026quot;; }; }; view \u0026quot;DEFAULT\u0026quot; { Match-clients{default_iprange}; zone \u0026quot;.\u0026quot; IN { type hint; file \u0026quot;named.root\u0026quot;; }; zone \u0026quot;localhost\u0026quot; IN { type master; file \u0026quot;localhost.zone\u0026quot;; allow-update { none; }; }; zone \u0026quot;cdntest.com\u0026quot; IN { type master; file \u0026quot;default_cdntest.zone\u0026quot;; }; };  zone文件的配置说明 这4个zone配置文件(cnc_cdntest.zone,tel_cdntest.zone,edu_cdntest.zone,default_cdntest.zone)中，只有www.cndtest.com的A记录不一样，其他的都是一样.\n   域名 zone配置文件 A记录地址     www.cdntest.com cnc_cdntest.zone 192.168.0.1   www.cdntest.com tel_cdntest.zone 192.168.0.2   www.cdntest.com edu_cdntest.zone 192.168.0.3   www.cdntest.com default_cdntest.zone 192.168.0.4    以上只列出了 www.cdntest.com 的A记录地址,其他关于zone的语法 请参考互联网.\n 域名解析流程简要说明\n 用户向 LocalDns 查询域名 www.cdntest.com\n LocalDns 向 授权DNS 查询www.cdntest.com\n 授权DNS 判断用户使用的 LocalDns的ip地址,匹配上述设置的ip范围,如果范围在网通，就将网通对应的ip地址(192.168.0.1),回应给LocalDns(其他依此类推)\n LocalDns 将得到的域名ip地址，回应给 用户端 (域名解析完成)\n  说明:再此过程中，我们简化了主DNS 到 智能DNS 之间的CNAME过程(为了简要说明问题). 这里使用的是静态拓扑(根据ip范围)的方法,也称为地域化方法,只是判断LocalDns的IP.\n 此简化方案中的存在的问题\n 如果用户设置错误的dns，可能会导致用户访问比原来慢(比如网通用户设置了电信的DNS）\n 不能判断CDN节点服务器的健康状态和容量状态，可能会把用户定向到不可用的CDN节点\n 由于静态拓扑方法,可能存在用户访问的CDN节点不是最优化和最快的\n \u0026hellip;..可能还有其他想不到的\u0026hellip;.\n  总结(Summary) 在建立CDN网路时，最关键的就是 智能调度DNS，这个是CND网络总协调,通过高效的调度算法，可以使用户得到最佳的访问体验.\n其次就是 CND节点的管理,比如涉及到 内容的同步机制，配置文件的更新等等，都需要有一套机制来保证.\n当然在大型网站中，也要考建设CDN体系的成本和回报率.\n"
},
{
	"uri": "/docs/bigdata/",
	"title": "大数据",
	"tags": [],
	"description": "",
	"content": " 大数据Spark BigData 本分类主要讲述大数据相关配置等实践\n"
},
{
	"uri": "/docs/security/tcp_wrapper/",
	"title": "Tcp Wrapper 应用安全指南",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2016-03-28 创建 HunterFu 初始版本    TCP Wrapper 简介 一般在操作系统层面上安装防火墙来处理网络连接,然而虽然防火墙有非常广泛的用途，但他却不是万能的,例如它无法处理类似的向连接发起者发送一些文本这样的任务。\nTCP Wrappers扩展了inetd为受其控制的服务程序实施控制的能力,通过使用这种方法，它能够提供日志支持、返回消息给联入的连接、使得服务程序只接受内部连接，等等。\n然而，由TCP Wrappers提供的一些额外的安全功能，不应被视为好的防火墙的替代品,TCP Wrappers 应结合防火墙或其他安全加强设施一并使用，为系统多提供一层安全防护。\nTCP Wrapper 系统概述 从上图可以看出tcp wrapper 在系统中的层级结构和应用程序的关系.\n注意 : Netfilter(iptable) 是工作在网络层的 而 tcp wrapper 是工作在应用层\n从上图可以清楚的看出 /etc/hosts.allow 和 /etc/hosts.deny 配置文件规则关系\nTCP Wrapper 规则使用 规则语法说明 hosts.allow 和 hosts.deny 语法格式如下:\ndaemon : client [:option1:option2:\u0026hellip;]\ndaemon 可以是各种服务程序，比如 sshd. 服务程序在编译的时候将libwrap.so.0动态库进来就可以使用TCP Wrappers.\n可以使用ldd命令查看程序是否支持libwrap,例如:\n[root@localhost]# ldd /usr/sbin/sshd | grep libwrap libwrap.so.0 =\u0026gt; /lib/libwrap.so.0 (0x0034f000)  client 是来源用户列表以逗号分隔，可以是主机名，或者ip地址等.\noptions 附加选项 是在规则匹配后，可以选择发邮件给管理员或者记录日志等，每个动作之间使用冒号分隔\n规则使用举例 本地网络是 192.168.54.0/24, 可以访问本机服务的列表如下:\n[root@localhost]# cat /etc/hosts.allow: popd : 192.168.54.137 imapd : 192.168.54.0/255.255.255.0 sendmail : 192.168.54.0/255.255.255.0 sshd : 192.168.54.2 172.16.234.4 [root@localhost]# cat /etc/hosts.deny: ALL : ALL  进阶参考 总体来说,以下是使用TCP Wrapper的小技巧:\n 默认策略是拒绝访问(ALL : ALL in the hosts.deny file). 尽可能是用ip地址 而不是主机名 可以参考 man 5 hosts_access 了解更多  "
},
{
	"uri": "/docs/security/",
	"title": "信息安全",
	"tags": [],
	"description": "",
	"content": " 信息安全 Security 本分类主要讲述信息安全配置等实践\n"
},
{
	"uri": "/docs/security/openssl/",
	"title": "openssl加密DSA,RSA介绍",
	"tags": [],
	"description": "",
	"content": "    版本 日期 状态 修订人 摘要     V1.0 2016-03-28 创建 HunterFu 初始版本    在日常系统管理工作中，需要作一些加解密的工作，通过openssl工具包就能完成我们很多需求！\nopenssl RSA 加解密 RSA是基于数论中大素数的乘积难分解理论上的非对称加密法,使用公私钥的方法进行加解密\n公钥 用于加密，它是向所有人公开的 ; 私钥用于解密，只有密文的接收者持有\n如图所示，甲乙之间使用非对称加密的方式完成了重要信息的安全传输。\n 乙方生成一对密钥（公钥和私钥）并将公钥向其它方公开。 得到公钥的甲方使用该密钥对机密信息进行加密后再发送给乙方。 乙方再用自己保存的另一把专用密钥（私钥）对加密后的信息进行解密。乙方只能用其专用密钥（私钥）解密由对应的公钥加密后的信息。 在传输过程中，即使攻击者截获了传输的密文，并得到了乙的公钥，也无法破解密文，因为只有乙的私钥才能解密密文 同样，如果乙要回复加密信息给甲，那么需要甲先公布甲的公钥给乙用于加密，甲自己保存甲的私钥用于解密。   生成一个私钥\n[root@hunterfu ~]# openssl genrsa -out private.key 1024  注意: 需要注意的是这个文件包含了公钥和密钥两部分，也就是说这个文件即可用来加密也可以用来解密,后面的1024是生成密钥的长度.\n 通过密钥文件private.key 提取公钥\n[root@hunterfu ~]# openssl rsa -in private.key -pubout -out pub.key  使用公钥加密信息\n[root@hunterfu ~]# echo -n \u0026quot;123456\u0026quot; | openssl rsautl -encrypt -inkey pub.key -pubin \u0026gt;encode.result  使用私钥解密信息\n[root@hunterfu ~]#cat encode.result | openssl rsautl -decrypt -inkey private.key 123456   至此，一次RSA加密解密的过程已经完成！\nopenssl DSA签名与验证 和RSA加密解密过程相反，在DSA数字签名和认证中，发送者使用自己的私钥对文件或消息进行签名，接受者收到消息后使用发送者的公钥来验证签名的真实性\nDSA只是一种算法，和RSA不同之处在于它不能用作加密和解密，也不能进行密钥交换，只用于签名,它比RSA要快很多.\n 生成一个密钥(私钥)\n[root@hunterfu ~]# openssl dsaparam -out dsaparam.pem 1024 [root@hunterfu ~]# openssl gendsa -out privkey.pem dsaparam.pem  生成公钥\n[root@hunterfu ~]# openssl dsa -in privkey.pem -out pubkey.pem -pubout [root@hunterfu ~]# rm -fr dsaparam.pem  使用私钥签名\n[root@hunterfu ~]# echo -n \u0026quot;123456\u0026quot; | openssl dgst -dss1 -sign privkey.pem \u0026gt; sign.result  使用公钥验证\n[root@hunterfu ~]# echo -n \u0026quot;123456\u0026quot; | openssl dgst -dss1 -verify pubkey.pem -signature sign.result Verified OK   至此，一次DSA签名与验证过程完成！\n总结及注意事项  注意: 由于信息经过加密或者签名后，都变成不可读模式,为了方便终端查看和传输使用(url提交数据,需要作urlencode操作)，可以使用base64进行编码\n openssl enc -base64 -A ：将加密后的信息使用base64编码  openssl enc -d -base64 -A ： 将信息使用base64反编码  java中此私钥需要转换下格式才能使用:\n[root@hunterfu ~]# openssl pkcs8 -topk8 -nocrypt -in private.key -outform PEM -out java_private.key   当然openssl 是一个很实用的加密工具包，还有很多东西值得学习和总结，以后有空再说！\n"
},
{
	"uri": "/docs/devops/",
	"title": "开发运维",
	"tags": [],
	"description": "",
	"content": " 开发运维 DevOps 本分类主要讲述DevOps相关实践\n"
},
{
	"uri": "/docs/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/docs/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/docs/",
	"title": "技术文档",
	"tags": [],
	"description": "",
	"content": " 关于技术文档 这里文档都是日常运维中的点滴积累，有些已经是比较完善的文档，方便大家互相参考学\n主要内容分类  私有云的建设和应用,这里以CLOUDSTACK为基础IAAS平台 容器云的建设和应用,这里以Openshift Origin平台为基础 监控系统的建设和应用,主要以Zabbix为核心监控及二次开发相关 大数据相关系统, 如Hadoop、Spark、Kafka、Storm等,涉及到一些BI(商业智能)等 信息安全涉及到一些方法论和开源工具的使用等 开发运维(DevOps) 相关的技术点滴和CI/CD相关实践  问题反馈 在看文档过程中，有任何问题，请到这里留言反馈，谢谢。\n我们会在第一时间响应回复\n 开源方案应用 如果在真实的生产工作中，您有应用比较好的方案，可以推荐给我们，包括解决的问题，使用的场景等等，我们会推广给更多的用户，让更多的用户受益于开源方案 请到这里推荐反馈\n推荐的开源方案，最好是目前正在使用的，解决了那些问题，也希望给出优点和劣势\n "
}]